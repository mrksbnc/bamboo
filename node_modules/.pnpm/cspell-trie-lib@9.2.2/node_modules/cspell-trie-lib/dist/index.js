import { opAppend, opCombine, opConcatMap, opFilter, opFlatten, opMap, opUnique, pipe, pipeSync, reduce } from "@cspell/cspell-pipe/sync";
import assert from "node:assert";
import { endianness } from "node:os";
import { genSequence } from "gensequence";

//#region src/lib/utils/memorizeLastCall.ts
const SymEmpty = Symbol("memorizeLastCall");
function memorizeLastCall(fn) {
	let lastP = void 0;
	let lastR = SymEmpty;
	function calc(p) {
		if (lastR !== SymEmpty && lastP === p) return lastR;
		lastP = p;
		lastR = fn(p);
		return lastR;
	}
	return calc;
}

//#endregion
//#region src/lib/ITrieNode/find.ts
const defaultLegacyMinCompoundLength$2 = 3;
const _defaultFindOptions$1 = {
	matchCase: false,
	compoundMode: "compound",
	legacyMinCompoundLength: defaultLegacyMinCompoundLength$2
};
Object.freeze(_defaultFindOptions$1);
const knownCompoundModes$1 = new Map([
	"none",
	"compound",
	"legacy"
].map((a) => [a, a]));
const notFound = {
	found: false,
	compoundUsed: false,
	caseMatched: false,
	forbidden: void 0
};
Object.freeze(notFound);
/**
*
* @param root Trie root node. root.c contains the compound root and forbidden root.
* @param word A pre normalized word use `normalizeWord` or `normalizeWordToLowercase`
* @param options
*/
function findWordNode$1(root, word, options) {
	return _findWordNode$1(root, word, options);
}
/**
*
* @param root Trie root node. root.c contains the compound root and forbidden root.
* @param word A pre normalized word use `normalizeWord` or `normalizeWordToLowercase`
* @param options
*/
function findWord$1(root, word, options) {
	if (root.find) {
		const found$1 = root.find(word, options?.matchCase || false);
		if (found$1) {
			if (options?.checkForbidden && found$1.forbidden === void 0) found$1.forbidden = isForbiddenWord$1(root, word, root.forbidPrefix);
			return found$1;
		}
		if (!root.hasCompoundWords) return notFound;
	}
	const { found, compoundUsed, caseMatched, forbidden } = _findWordNode$1(root, word, options);
	const result = {
		found,
		compoundUsed,
		caseMatched,
		forbidden
	};
	if (options?.checkForbidden && forbidden === void 0) result.forbidden = isForbiddenWord$1(root, word, root.forbidPrefix);
	return result;
}
/**
*
* @param root Trie root node. root.c contains the compound root and forbidden root.
* @param word A pre normalized word use `normalizeWord` or `normalizeWordToLowercase`
* @param options
*/
function _findWordNode$1(root, word, options) {
	const trieInfo = root.info;
	const matchCase = options?.matchCase || false;
	const compoundMode = knownCompoundModes$1.get(options?.compoundMode) || _defaultFindOptions$1.compoundMode;
	const compoundPrefix = compoundMode === "compound" ? trieInfo.compoundCharacter ?? root.compoundFix : "";
	const ignoreCasePrefix = matchCase ? "" : trieInfo.stripCaseAndAccentsPrefix ?? root.caseInsensitivePrefix;
	const mustCheckForbidden = options?.checkForbidden === true;
	const checkForbidden = options?.checkForbidden ?? true;
	function __findCompound() {
		const f = findCompoundWord$1(root, word, compoundPrefix, ignoreCasePrefix);
		if (f.found !== false && (mustCheckForbidden || f.compoundUsed && checkForbidden)) f.forbidden = isForbiddenWord$1(!f.caseMatched ? walk$2(root, root.caseInsensitivePrefix) : root, word, root.forbidPrefix);
		return f;
	}
	function __findExact() {
		const n = root.getNode ? root.getNode(word) : walk$2(root, word);
		return {
			found: isEndOfWordNode$1(n) && word,
			compoundUsed: false,
			forbidden: checkForbidden ? isForbiddenWord$1(root, word, root.forbidPrefix) : void 0,
			node: n,
			caseMatched: true
		};
	}
	switch (compoundMode) {
		case "none": return matchCase ? __findExact() : __findCompound();
		case "compound": return __findCompound();
		case "legacy": return findLegacyCompound$1(root, word, options);
	}
}
function findLegacyCompound$1(root, word, options) {
	const roots = [root];
	if (!options?.matchCase) roots.push(walk$2(root, root.caseInsensitivePrefix));
	return findLegacyCompoundNode$1(roots, word, options?.legacyMinCompoundLength || defaultLegacyMinCompoundLength$2);
}
function findCompoundNode$1(root, word, compoundCharacter, ignoreCasePrefix) {
	const stack = [{
		n: root,
		compoundPrefix: ignoreCasePrefix,
		cr: void 0,
		caseMatched: true
	}];
	const compoundPrefix = compoundCharacter || ignoreCasePrefix;
	const possibleCompoundPrefix = ignoreCasePrefix && compoundCharacter ? ignoreCasePrefix + compoundCharacter : "";
	const nw = word.normalize();
	const w = [...nw];
	function determineRoot(s) {
		const prefix = s.compoundPrefix;
		let r = root;
		let i$1;
		for (i$1 = 0; i$1 < prefix.length && r; ++i$1) r = r.get(prefix[i$1]);
		const caseMatched$1 = s.caseMatched && prefix[0] !== ignoreCasePrefix;
		return {
			n: s.n,
			compoundPrefix: prefix === compoundPrefix ? possibleCompoundPrefix : "",
			cr: r,
			caseMatched: caseMatched$1
		};
	}
	let compoundUsed = false;
	let caseMatched = true;
	let i = 0;
	let node;
	while (true) {
		const s = stack[i];
		const h = w[i++];
		const n = s.cr || s.n;
		const c = h && n?.get(h) || void 0;
		if (c && i < word.length) {
			caseMatched = s.caseMatched;
			stack[i] = {
				n: c,
				compoundPrefix,
				cr: void 0,
				caseMatched
			};
		} else if (!c || !c.eow) {
			node = node || c;
			while (--i > 0) {
				const s$1 = stack[i];
				if (!s$1.compoundPrefix || !s$1.n?.hasChildren()) continue;
				if (s$1.n.get(compoundCharacter)) break;
			}
			if (i >= 0 && stack[i].compoundPrefix) {
				compoundUsed = i > 0;
				const r = determineRoot(stack[i]);
				stack[i] = r;
				if (!r.cr) break;
				if (!i && !r.caseMatched && nw !== nw.toLowerCase()) break;
			} else break;
		} else {
			node = c;
			caseMatched = s.caseMatched;
			break;
		}
	}
	return {
		found: i === word.length && word || false,
		compoundUsed,
		node,
		forbidden: void 0,
		caseMatched
	};
}
function findCompoundWord$1(root, word, compoundCharacter, ignoreCasePrefix) {
	const { found, compoundUsed, node, caseMatched } = findCompoundNode$1(root, word, compoundCharacter, ignoreCasePrefix);
	if (!node || !node.eow) return {
		found: false,
		compoundUsed,
		node,
		forbidden: void 0,
		caseMatched
	};
	return {
		found,
		compoundUsed,
		node,
		forbidden: void 0,
		caseMatched
	};
}
function findWordExact$1(root, word) {
	const r = root;
	if (r?.findExact) return r.findExact(word);
	return isEndOfWordNode$1(walk$2(root, word));
}
function isEndOfWordNode$1(n) {
	return !!n?.eow;
}
function walk$2(root, word) {
	const w = [...word];
	let n = root;
	let i = 0;
	while (n && i < w.length) {
		const h = w[i++];
		n = n.get(h);
	}
	return n;
}
function findLegacyCompoundNode$1(roots, word, minCompoundLength) {
	const root = roots[0];
	const numRoots = roots.length;
	const stack = [{
		n: root,
		usedRoots: 1,
		subLength: 0,
		isCompound: false,
		cr: void 0,
		caseMatched: true
	}];
	const w = word;
	const wLen = w.length;
	let compoundUsed = false;
	let caseMatched = true;
	let i = 0;
	let node;
	while (true) {
		const s = stack[i];
		const h = w[i++];
		const c = (s.cr || s.n)?.get(h);
		if (c && i < wLen) stack[i] = {
			n: c,
			usedRoots: 0,
			subLength: s.subLength + 1,
			isCompound: s.isCompound,
			cr: void 0,
			caseMatched: s.caseMatched
		};
		else if (!c || !c.eow || c.eow && s.subLength < minCompoundLength - 1) {
			while (--i > 0) {
				const s$1 = stack[i];
				if (s$1.usedRoots < numRoots && s$1.n?.eow && (s$1.subLength >= minCompoundLength || !s$1.subLength) && wLen - i >= minCompoundLength) break;
			}
			if (i > 0 || stack[i].usedRoots < numRoots) {
				compoundUsed = i > 0;
				const s$1 = stack[i];
				s$1.cr = roots[s$1.usedRoots++];
				s$1.subLength = 0;
				s$1.isCompound = compoundUsed;
				s$1.caseMatched = s$1.caseMatched && s$1.usedRoots <= 1;
			} else break;
		} else {
			node = c;
			caseMatched = s.caseMatched;
			break;
		}
	}
	function extractWord() {
		if (!word || i < word.length) return false;
		const letters = [];
		let subLen = 0;
		for (let j = 0; j < i; ++j) {
			const { subLength } = stack[j];
			if (subLength < subLen) letters.push("+");
			letters.push(word[j]);
			subLen = subLength;
		}
		return letters.join("");
	}
	return {
		found: extractWord(),
		compoundUsed,
		node,
		forbidden: void 0,
		caseMatched
	};
}
function isForbiddenWord$1(root, word, forbiddenPrefix) {
	const r = root;
	if (r?.isForbidden) return r.isForbidden(word);
	return findWordExact$1(root?.get(forbiddenPrefix), word);
}
const createFindOptions$1 = memorizeLastCall(_createFindOptions$1);
function _createFindOptions$1(options) {
	if (!options) return _defaultFindOptions$1;
	const d = _defaultFindOptions$1;
	return {
		matchCase: options.matchCase ?? d.matchCase,
		compoundMode: options.compoundMode ?? d.compoundMode,
		legacyMinCompoundLength: options.legacyMinCompoundLength ?? d.legacyMinCompoundLength,
		checkForbidden: options.checkForbidden ?? d.checkForbidden
	};
}

//#endregion
//#region src/lib/walker/walkerTypes.ts
const JOIN_SEPARATOR = "+";
const WORD_SEPARATOR = " ";
let CompoundWordsMethod = /* @__PURE__ */ function(CompoundWordsMethod$1) {
	/**
	* Do not compound words.
	*/
	CompoundWordsMethod$1[CompoundWordsMethod$1["NONE"] = 0] = "NONE";
	/**
	* Create word compounds separated by spaces.
	*/
	CompoundWordsMethod$1[CompoundWordsMethod$1["SEPARATE_WORDS"] = 1] = "SEPARATE_WORDS";
	/**
	* Create word compounds without separation.
	*/
	CompoundWordsMethod$1[CompoundWordsMethod$1["JOIN_WORDS"] = 2] = "JOIN_WORDS";
	return CompoundWordsMethod$1;
}({});

//#endregion
//#region src/lib/ITrieNode/walker/walker.ts
/**
* Walks the Trie and yields a value at each node.
* next(goDeeper: boolean):
*/
function* compoundWalker$1(root, compoundingMethod) {
	const empty = Object.freeze([]);
	const roots = {
		[CompoundWordsMethod.NONE]: empty,
		[CompoundWordsMethod.JOIN_WORDS]: [[JOIN_SEPARATOR, root]],
		[CompoundWordsMethod.SEPARATE_WORDS]: [[WORD_SEPARATOR, root]]
	};
	const rc = roots[compoundingMethod].length ? roots[compoundingMethod] : void 0;
	function children(n) {
		if (n.hasChildren()) {
			const entries = n.entries();
			const c = Array.isArray(entries) ? entries : [...entries];
			return n.eow && rc ? [...c, ...rc] : c;
		}
		if (n.eow) return roots[compoundingMethod];
		return empty;
	}
	let depth = 0;
	const stack = [];
	stack[depth] = {
		t: "",
		c: children(root),
		ci: 0
	};
	while (depth >= 0) {
		let s = stack[depth];
		let baseText = s.t;
		while (s.ci < s.c.length) {
			const [char, node] = s.c[s.ci++];
			const text = baseText + char;
			if ((yield {
				text,
				node,
				depth
			}) ?? true) {
				depth++;
				baseText = text;
				stack[depth] = {
					t: text,
					c: children(node),
					ci: 0
				};
			}
			s = stack[depth];
		}
		depth -= 1;
	}
}
/**
* Walks the Trie and yields a value at each node.
* next(goDeeper: boolean):
*/
function* nodeWalker$1(root) {
	let depth = 0;
	const stack = [];
	const entries = root.entries();
	stack[depth] = {
		t: "",
		n: root,
		c: Array.isArray(entries) ? entries : [...entries],
		ci: 0
	};
	while (depth >= 0) {
		let s = stack[depth];
		let baseText = s.t;
		while (s.ci < s.c.length && s.n) {
			const idx$1 = s.ci++;
			const [char, node] = s.c[idx$1];
			const text = baseText + char;
			if ((yield {
				text,
				node,
				depth
			}) !== false) {
				depth++;
				baseText = text;
				const s$1 = stack[depth];
				const entries$1 = node.entries();
				const c = Array.isArray(entries$1) ? entries$1 : [...entries$1];
				if (s$1) {
					s$1.t = text;
					s$1.n = node;
					s$1.c = c;
					s$1.ci = 0;
				} else stack[depth] = {
					t: text,
					n: node,
					c,
					ci: 0
				};
			}
			s = stack[depth];
		}
		depth -= 1;
	}
}
function walker$1(root, compoundingMethod = CompoundWordsMethod.NONE) {
	return compoundingMethod === CompoundWordsMethod.NONE ? nodeWalker$1(root) : compoundWalker$1(root, compoundingMethod);
}
function walkerWords$1(root) {
	return walkerWordsITrie(root);
}
/**
* Walks the Trie and yields each word.
*/
function* walkerWordsITrie(root) {
	let depth = 0;
	const stack = [];
	const entries = root.entries();
	stack[depth] = {
		t: "",
		n: root,
		c: Array.isArray(entries) ? entries : [...entries],
		ci: 0
	};
	while (depth >= 0) {
		let s = stack[depth];
		let baseText = s.t;
		while (s.ci < s.c.length && s.n) {
			const [char, node] = s.c[s.ci++];
			if (!node) continue;
			const text = baseText + char;
			if (node.eow) yield text;
			depth++;
			baseText = text;
			const entries$1 = node.entries();
			const c = Array.isArray(entries$1) ? entries$1 : [...entries$1];
			if (stack[depth]) {
				s = stack[depth];
				s.t = text;
				s.n = node;
				s.c = c;
				s.ci = 0;
			} else stack[depth] = {
				t: text,
				n: node,
				c,
				ci: 0
			};
			s = stack[depth];
		}
		depth -= 1;
	}
}

//#endregion
//#region src/lib/ITrieNode/trie-util.ts
/**
* Generate a Iterator that can walk a Trie and yield the words.
*/
function iteratorTrieWords$1(node) {
	return walkerWords$1(node);
}
function findNode$1(node, word) {
	for (let i = 0; i < word.length; ++i) {
		const n = node.get(word[i]);
		if (!n) return void 0;
		node = n;
	}
	return node;
}
function countWords$1(root) {
	const visited = /* @__PURE__ */ new Map();
	function walk$3(n) {
		const nestedCount = visited.get(n.id);
		if (nestedCount !== void 0) return nestedCount;
		let cnt = n.eow ? 1 : 0;
		visited.set(n, cnt);
		for (const c of n.values()) cnt += walk$3(c);
		visited.set(n, cnt);
		return cnt;
	}
	return walk$3(root);
}

//#endregion
//#region src/lib/utils/isDefined.ts
function isDefined(t) {
	return t !== void 0;
}

//#endregion
//#region src/lib/walker/hintedWalker.ts
function hintedWalker(root, ignoreCase, hint, compoundingMethod, emitWordSeparator) {
	return hintedWalkerNext(root, ignoreCase, hint, compoundingMethod, emitWordSeparator);
}
/**
* Walks the Trie and yields a value at each node.
* next(goDeeper: boolean):
*/
function* hintedWalkerNext(root, ignoreCase, hint, compoundingMethod, emitWordSeparator = "") {
	const _compoundingMethod = compoundingMethod ?? CompoundWordsMethod.NONE;
	const compoundCharacter = root.compoundCharacter;
	const noCaseCharacter = root.stripCaseAndAccentsPrefix;
	const rawRoots = [root, ignoreCase ? root.c[noCaseCharacter] : void 0].filter(isDefined);
	const specialRootsPrefix = existMap([
		compoundCharacter,
		noCaseCharacter,
		root.forbiddenWordPrefix
	]);
	function filterRoot(root$1) {
		const c = (root$1.c && Object.entries(root$1.c))?.filter(([v]) => !(v in specialRootsPrefix));
		return { c: c && Object.fromEntries(c) };
	}
	const roots = rawRoots.map(filterRoot);
	const compoundRoots = rawRoots.map((r) => r.c?.[compoundCharacter]).filter(isDefined);
	const setOfCompoundRoots = new Set(compoundRoots);
	const rootsForCompoundMethods = [...roots, ...compoundRoots];
	const compoundMethodRoots = {
		[CompoundWordsMethod.NONE]: [],
		[CompoundWordsMethod.JOIN_WORDS]: rootsForCompoundMethods.map((r) => [JOIN_SEPARATOR, r]),
		[CompoundWordsMethod.SEPARATE_WORDS]: rootsForCompoundMethods.map((r) => [WORD_SEPARATOR, r])
	};
	function* children(n, hintOffset) {
		if (n.c) {
			const h = hint.slice(hintOffset, hintOffset + 3) + hint.slice(Math.max(0, hintOffset - 2), hintOffset);
			const hints = new Set(h);
			const c = n.c;
			yield* [...hints].filter((a) => a in c).map((letter) => ({
				letter,
				node: c[letter],
				hintOffset: hintOffset + 1
			}));
			hints.add(compoundCharacter);
			yield* Object.entries(c).filter((a) => !hints.has(a[0])).map(([letter, node]) => ({
				letter,
				node,
				hintOffset: hintOffset + 1
			}));
			if (compoundCharacter in c && !setOfCompoundRoots.has(n)) for (const compoundRoot of compoundRoots) for (const child of children(compoundRoot, hintOffset)) {
				const { letter, node, hintOffset: hintOffset$1 } = child;
				yield {
					letter: emitWordSeparator + letter,
					node,
					hintOffset: hintOffset$1
				};
			}
		}
		if (n.f) yield* [...compoundMethodRoots[_compoundingMethod]].map(([letter, node]) => ({
			letter,
			node,
			hintOffset
		}));
	}
	for (const root$1 of roots) {
		let depth = 0;
		const stack = [];
		const stackText = [""];
		stack[depth] = children(root$1, depth);
		let ir;
		while (depth >= 0) {
			while (!(ir = stack[depth].next()).done) {
				const { letter: char, node, hintOffset } = ir.value;
				const text = stackText[depth] + char;
				const hinting = yield {
					text,
					node,
					depth
				};
				if (hinting && hinting.goDeeper) {
					depth++;
					stackText[depth] = text;
					stack[depth] = children(node, hintOffset);
				}
			}
			depth -= 1;
		}
	}
}
function existMap(values) {
	const m = Object.create(null);
	for (const v of values) m[v] = true;
	return m;
}

//#endregion
//#region src/lib/TrieNode/trie.ts
function trieRootToITrieRoot(root) {
	return ImplITrieRoot.toITrieNode(root);
}
const EmptyKeys$2 = Object.freeze([]);
const EmptyValues = Object.freeze([]);
const EmptyEntries$2 = Object.freeze([]);
var ImplITrieNode = class ImplITrieNode {
	id;
	_keys;
	constructor(node) {
		this.node = node;
		this.id = node;
	}
	/** flag End of Word */
	get eow() {
		return !!this.node.f;
	}
	/** number of children */
	get size() {
		if (!this.node.c) return 0;
		return this.keys().length;
	}
	/** get keys to children */
	keys() {
		if (this._keys) return this._keys;
		const keys = this.node.c ? Object.keys(this.node.c) : EmptyKeys$2;
		this._keys = keys;
		return keys;
	}
	/** get the child nodes */
	values() {
		return !this.node.c ? EmptyValues : Object.values(this.node.c).map((n) => ImplITrieNode.toITrieNode(n));
	}
	entries() {
		return !this.node.c ? EmptyEntries$2 : Object.entries(this.node.c).map(([k, n]) => [k, ImplITrieNode.toITrieNode(n)]);
	}
	/** get child ITrieNode */
	get(char) {
		const n = this.node.c?.[char];
		if (!n) return void 0;
		return ImplITrieNode.toITrieNode(n);
	}
	getNode(chars) {
		return this.findNode(chars);
	}
	has(char) {
		const c = this.node.c;
		return c && char in c || false;
	}
	child(keyIdx) {
		const char = this.keys()[keyIdx];
		const n = char && this.get(char);
		if (!n) throw new Error("Index out of range.");
		return n;
	}
	hasChildren() {
		return !!this.node.c;
	}
	#findTrieNode(word) {
		let node = this.node;
		for (const char of word) {
			if (!node) return void 0;
			node = node.c?.[char];
		}
		return node;
	}
	findNode(word) {
		const node = this.#findTrieNode(word);
		return node && ImplITrieNode.toITrieNode(node);
	}
	findExact(word) {
		const node = this.#findTrieNode(word);
		return !!node && !!node.f;
	}
	static toITrieNode(node) {
		return new this(node);
	}
};
var ImplITrieRoot = class extends ImplITrieNode {
	info;
	hasForbiddenWords;
	hasCompoundWords;
	hasNonStrictWords;
	constructor(root) {
		super(root);
		this.root = root;
		const { stripCaseAndAccentsPrefix, compoundCharacter, forbiddenWordPrefix, isCaseAware } = root;
		this.info = {
			stripCaseAndAccentsPrefix,
			compoundCharacter,
			forbiddenWordPrefix,
			isCaseAware
		};
		this.hasForbiddenWords = !!root.c[forbiddenWordPrefix];
		this.hasCompoundWords = !!root.c[compoundCharacter];
		this.hasNonStrictWords = !!root.c[stripCaseAndAccentsPrefix];
	}
	get eow() {
		return false;
	}
	resolveId(id) {
		return new ImplITrieNode(id);
	}
	get forbidPrefix() {
		return this.root.forbiddenWordPrefix;
	}
	get compoundFix() {
		return this.root.compoundCharacter;
	}
	get caseInsensitivePrefix() {
		return this.root.stripCaseAndAccentsPrefix;
	}
	static toITrieNode(node) {
		return new this(node);
	}
};

//#endregion
//#region src/lib/walker/walker.ts
/**
* Walks the Trie and yields a value at each node.
* next(goDeeper: boolean):
*/
function* compoundWalker(root, compoundingMethod) {
	const roots = {
		[CompoundWordsMethod.NONE]: [],
		[CompoundWordsMethod.JOIN_WORDS]: [[JOIN_SEPARATOR, root]],
		[CompoundWordsMethod.SEPARATE_WORDS]: [[WORD_SEPARATOR, root]]
	};
	const rc = roots[compoundingMethod].length ? roots[compoundingMethod] : void 0;
	const empty = [];
	function children(n) {
		if (n.c && n.f && rc) return [...Object.entries(n.c), ...rc];
		if (n.c) return Object.entries(n.c);
		if (n.f && rc) return rc;
		return empty;
	}
	let depth = 0;
	const stack = [];
	stack[depth] = {
		t: "",
		c: children(root),
		ci: 0
	};
	while (depth >= 0) {
		let s = stack[depth];
		let baseText = s.t;
		while (s.ci < s.c.length) {
			const [char, node] = s.c[s.ci++];
			const text = baseText + char;
			if ((yield {
				text,
				node,
				depth
			}) ?? true) {
				depth++;
				baseText = text;
				stack[depth] = {
					t: text,
					c: children(node),
					ci: 0
				};
			}
			s = stack[depth];
		}
		depth -= 1;
	}
}
/**
* Walks the Trie and yields a value at each node.
* next(goDeeper: boolean):
*/
function* nodeWalker(root) {
	const empty = [];
	function children(n) {
		if (n.c) return Object.keys(n.c);
		return empty;
	}
	let depth = 0;
	const stack = [];
	stack[depth] = {
		t: "",
		n: root.c,
		c: children(root),
		ci: 0
	};
	while (depth >= 0) {
		let s = stack[depth];
		let baseText = s.t;
		while (s.ci < s.c.length && s.n) {
			const char = s.c[s.ci++];
			const node = s.n[char];
			const text = baseText + char;
			if ((yield {
				text,
				node,
				depth
			}) !== false) {
				depth++;
				baseText = text;
				const s$1 = stack[depth];
				const c = children(node);
				if (s$1) {
					s$1.t = text;
					s$1.n = node.c;
					s$1.c = c;
					s$1.ci = 0;
				} else stack[depth] = {
					t: text,
					n: node.c,
					c,
					ci: 0
				};
			}
			s = stack[depth];
		}
		depth -= 1;
	}
}
const walkerWords = _walkerWords;
/**
* Walks the Trie and yields each word.
*/
function* _walkerWords(root) {
	const empty = [];
	function children(n) {
		if (n.c) return Object.keys(n.c);
		return empty;
	}
	let depth = 0;
	const stack = [];
	stack[depth] = {
		t: "",
		n: root.c,
		c: children(root),
		ci: 0
	};
	while (depth >= 0) {
		let s = stack[depth];
		let baseText = s.t;
		while (s.ci < s.c.length && s.n) {
			const char = s.c[s.ci++];
			const node = s.n[char];
			const text = baseText + char;
			if (node.f) yield text;
			depth++;
			baseText = text;
			const c = children(node);
			if (stack[depth]) {
				s = stack[depth];
				s.t = text;
				s.n = node.c;
				s.c = c;
				s.ci = 0;
			} else stack[depth] = {
				t: text,
				n: node.c,
				c,
				ci: 0
			};
			s = stack[depth];
		}
		depth -= 1;
	}
}
function walker(root, compoundingMethod = CompoundWordsMethod.NONE) {
	return compoundingMethod === CompoundWordsMethod.NONE ? nodeWalker(root) : compoundWalker(root, compoundingMethod);
}

//#endregion
//#region src/lib/suggestions/genSuggestionsOptions.ts
const defaultGenSuggestionOptions = {
	compoundMethod: CompoundWordsMethod.NONE,
	ignoreCase: true,
	changeLimit: 5
};
const defaultSuggestionOptions = {
	...defaultGenSuggestionOptions,
	numSuggestions: 8,
	includeTies: true,
	timeout: 5e3
};
const keyMapOfSuggestionOptionsStrict = {
	changeLimit: "changeLimit",
	compoundMethod: "compoundMethod",
	ignoreCase: "ignoreCase",
	compoundSeparator: "compoundSeparator",
	filter: "filter",
	includeTies: "includeTies",
	numSuggestions: "numSuggestions",
	timeout: "timeout",
	weightMap: "weightMap"
};
/**
* Create suggestion options using composition.
* @param opts - partial options.
* @returns Options - with defaults.
*/
function createSuggestionOptions(...opts) {
	const options = { ...defaultSuggestionOptions };
	const keys = Object.keys(keyMapOfSuggestionOptionsStrict);
	for (const opt of opts) for (const key of keys) assign(options, opt, key);
	return options;
}
function assign(dest, src, k) {
	dest[k] = src[k] ?? dest[k];
}

//#endregion
//#region src/lib/utils/PairingHeap.ts
var PairingHeap = class {
	_heap;
	_size = 0;
	constructor(compare$3) {
		this.compare = compare$3;
	}
	/** Add an item to the heap. */
	add(v) {
		this._heap = insert$1(this.compare, this._heap, v);
		++this._size;
		return this;
	}
	/** take an item from the heap. */
	dequeue() {
		const n = this.next();
		if (n.done) return void 0;
		return n.value;
	}
	/** Add items to the heap */
	append(i) {
		for (const v of i) this.add(v);
		return this;
	}
	/** get the next value */
	next() {
		if (!this._heap) return {
			value: void 0,
			done: true
		};
		const value = this._heap.v;
		--this._size;
		this._heap = removeHead(this.compare, this._heap);
		return { value };
	}
	/** peek at the next value without removing it. */
	peek() {
		return this._heap?.v;
	}
	[Symbol.iterator]() {
		return this;
	}
	/** alias of `size` */
	get length() {
		return this._size;
	}
	/** number of entries in the heap. */
	get size() {
		return this._size;
	}
};
function removeHead(compare$3, heap) {
	if (!heap || !heap.c) return void 0;
	return mergeSiblings(compare$3, heap.c);
}
function insert$1(compare$3, heap, v) {
	const n = {
		v,
		s: void 0,
		c: void 0
	};
	if (!heap || compare$3(v, heap.v) <= 0) {
		n.c = heap;
		return n;
	}
	n.s = heap.c;
	heap.c = n;
	return heap;
}
function merge(compare$3, a, b) {
	if (compare$3(a.v, b.v) <= 0) {
		a.s = void 0;
		b.s = a.c;
		a.c = b;
		return a;
	}
	b.s = void 0;
	a.s = b.c;
	b.c = a;
	return b;
}
function mergeSiblings(compare$3, n) {
	if (!n.s) return n;
	const s = n.s;
	const ss = s.s;
	const m = merge(compare$3, n, s);
	return ss ? merge(compare$3, m, mergeSiblings(compare$3, ss)) : m;
}

//#endregion
//#region src/lib/suggestions/constants.ts
const DEFAULT_COMPOUNDED_WORD_SEPARATOR = "∙";
const opCosts = {
	baseCost: 100,
	swapCost: 75,
	duplicateLetterCost: 80,
	compound: 1,
	visuallySimilar: 1,
	firstLetterBias: 5,
	wordBreak: 99,
	wordLengthCostFactor: .5
};

//#endregion
//#region src/lib/suggestions/orthography.ts
const intl = new Intl.Collator("en", { sensitivity: "base" });
const compare$2 = intl.compare;
/**
* This a set of letters that look like each other.
* There can be a maximum of 30 groups.
* It is possible for a letter to appear in more than 1 group, but not encouraged.
*/
const visualLetterGroups = [
	forms("ǎàåÄÀAãâáǟặắấĀāăąaäæɐɑαаᾳ") + "ᾳ",
	forms("Bbḃвъь"),
	forms("ċČčcĉçCÇćĊСсς"),
	forms("ḎḋḏḑďđḍDd"),
	forms("ēëÈÊËềéèếệĕeEĒėęěêəɛёЁеʒ"),
	forms("fḟFﬀ"),
	forms("ġĠĞǧĝģGgɣ"),
	forms("ħĦĥḥHhḤȟн"),
	forms("IįïİÎÍīiÌìíîıɪɨїΊΙ"),
	forms("jJĵ"),
	forms("ķKkκкќ"),
	forms("ḷłľļLlĺḶίι"),
	forms("Mṃṁm"),
	forms("nņÑNṇňŇñńŋѝий"),
	forms("ÒOøȭŌōőỏoÖòȱȯóôõöơɔόδо"),
	forms("PṗpрРρ"),
	forms("Qq"),
	forms("řRṛrŕŗѓгя"),
	forms("ṣšȘṢsSŠṡŞŝśșʃΣ"),
	forms("tțȚťTṭṬṫ"),
	forms("ÜüûŪưůūűúÛŭÙùuųU"),
	forms("Vvν"),
	forms("ŵwWẃẅẁωш"),
	forms("xXх"),
	forms("ÿýYŷyÝỳУўу"),
	forms("ZẓžŽżŻźz")
];
function forms(letters) {
	const n = letters.normalize("NFC").replaceAll(/\p{M}/gu, "");
	const na = n.normalize("NFD").replaceAll(/\p{M}/gu, "");
	return [...new Set(n + n.toLowerCase() + n.toUpperCase() + na + na.toLowerCase() + na.toUpperCase())].join("");
}
/**
* This is a map of letters to groups mask values.
* If two letters are part of the same group then `visualLetterMaskMap[a] & visualLetterMaskMap[b] !== 0`
*/
const visualLetterMaskMap = calcVisualLetterMasks(visualLetterGroups);
/**
*
* @param groups
* @returns
*/
function calcVisualLetterMasks(groups) {
	const map = Object.create(null);
	for (let i = 0; i < groups.length; ++i) {
		const m = 1 << i;
		const g = groups[i];
		for (const c of g) map[c] = (map[c] || 0) | m;
	}
	return map;
}

//#endregion
//#region src/lib/distance/weightedMaps.ts
const matchPossibleWordSeparators = /[+∙•・●]/g;
function createWeightMap(...defs) {
	const map = _createWeightMap();
	addDefsToWeightMap(map, defs);
	return map;
}
function addDefToWeightMap(map, ...defs) {
	return addDefsToWeightMap(map, defs);
}
function addAdjustment(map, ...adjustments) {
	for (const adj of adjustments) map.adjustments.set(adj.id, adj);
	return map;
}
function addDefsToWeightMap(map, defs) {
	function addSet(set, def) {
		addSetToTrieCost(map.insDel, set, def.insDel, def.penalty);
		addSetToTrieTrieCost(map.replace, set, def.replace, def.penalty);
		addSetToTrieTrieCost(map.swap, set, def.swap, def.penalty);
	}
	for (const _def of defs) {
		const def = normalizeDef(_def);
		splitMap$1(def).forEach((s) => addSet(s, def));
	}
	return map;
}
function _createWeightMap() {
	return {
		insDel: {},
		replace: {},
		swap: {},
		adjustments: /* @__PURE__ */ new Map()
	};
}
function lowest(a, b) {
	if (a === void 0) return b;
	if (b === void 0) return a;
	return a <= b ? a : b;
}
function highest(a, b) {
	if (a === void 0) return b;
	if (b === void 0) return a;
	return a >= b ? a : b;
}
function normalize(s) {
	const f = new Set([s]);
	f.add(s.normalize("NFC"));
	f.add(s.normalize("NFD"));
	return f;
}
function* splitMapSubstringsIterable(map) {
	let seq = "";
	let mode = 0;
	for (const char of map) {
		if (mode && char === ")") {
			yield* normalize(seq);
			mode = 0;
			continue;
		}
		if (mode) {
			seq += char;
			continue;
		}
		if (char === "(") {
			mode = 1;
			seq = "";
			continue;
		}
		yield* normalize(char);
	}
}
function splitMapSubstrings(map) {
	return [...splitMapSubstringsIterable(map)];
}
/**
* Splits a WeightedMapDef.map
* @param map
*/
function splitMap$1(def) {
	const { map } = def;
	return map.split("|").map(splitMapSubstrings).filter((s) => s.length > 0);
}
function addToTrieCost(trie, str, cost, penalties) {
	if (!str) return;
	let t = trie;
	for (const c of str) {
		const n = t.n = t.n || Object.create(null);
		t = n[c] = n[c] || Object.create(null);
	}
	t.c = lowest(t.c, cost);
	t.p = highest(t.p, penalties);
}
function addToTrieTrieCost(trie, left, right, cost, penalties) {
	let t = trie;
	for (const c of left) {
		const n = t.n = t.n || Object.create(null);
		t = n[c] = n[c] || Object.create(null);
	}
	addToTrieCost(t.t = t.t || Object.create(null), right, cost, penalties);
}
function addSetToTrieCost(trie, set, cost, penalties) {
	if (cost === void 0) return;
	for (const str of set) addToTrieCost(trie, str, cost, penalties);
}
function addSetToTrieTrieCost(trie, set, cost, penalties) {
	if (cost === void 0) return;
	for (const left of set) for (const right of set) {
		if (left === right) continue;
		addToTrieTrieCost(trie, left, right, cost, penalties);
	}
}
function* searchTrieNodes(trie, str, i) {
	const len = str.length;
	for (let n = trie.n; i < len && n;) {
		const t = n[str[i]];
		if (!t) return;
		++i;
		yield {
			i,
			t
		};
		n = t.n;
	}
}
function* findTrieCostPrefixes(trie, str, i) {
	for (const n of searchTrieNodes(trie, str, i)) {
		const { c, p } = n.t;
		if (c !== void 0) yield {
			i: n.i,
			c,
			p: p || 0
		};
	}
}
function* findTrieTrieCostPrefixes(trie, str, i) {
	for (const n of searchTrieNodes(trie, str, i)) {
		const t = n.t.t;
		if (t !== void 0) yield {
			i: n.i,
			t
		};
	}
}
function createWeightCostCalculator(weightMap) {
	return new _WeightCostCalculator(weightMap);
}
var _WeightCostCalculator = class {
	constructor(weightMap) {
		this.weightMap = weightMap;
	}
	*calcInsDelCosts(pos) {
		const { a, ai, b, bi, c, p } = pos;
		for (const del of findTrieCostPrefixes(this.weightMap.insDel, a, ai)) yield {
			a,
			b,
			ai: del.i,
			bi,
			c: c + del.c,
			p: p + del.p
		};
		for (const ins of findTrieCostPrefixes(this.weightMap.insDel, b, bi)) yield {
			a,
			b,
			ai,
			bi: ins.i,
			c: c + ins.c,
			p: p + ins.p
		};
	}
	*calcReplaceCosts(pos) {
		const { a, ai, b, bi, c, p } = pos;
		for (const del of findTrieTrieCostPrefixes(this.weightMap.replace, a, ai)) for (const ins of findTrieCostPrefixes(del.t, b, bi)) yield {
			a,
			b,
			ai: del.i,
			bi: ins.i,
			c: c + ins.c,
			p: p + ins.p
		};
	}
	*calcSwapCosts(pos) {
		const { a, ai, b, bi, c, p } = pos;
		const swap = this.weightMap.swap;
		for (const left of findTrieTrieCostPrefixes(swap, a, ai)) for (const right of findTrieCostPrefixes(left.t, a, left.i)) {
			const sw = a.slice(left.i, right.i) + a.slice(ai, left.i);
			if (b.slice(bi).startsWith(sw)) {
				const len = sw.length;
				yield {
					a,
					b,
					ai: ai + len,
					bi: bi + len,
					c: c + right.c,
					p: p + right.p
				};
			}
		}
	}
	calcAdjustment(word) {
		let penalty = 0;
		for (const adj of this.weightMap.adjustments.values()) if (adj.regexp.global) for (const _m of word.matchAll(adj.regexp)) penalty += adj.penalty;
		else if (adj.regexp.test(word)) penalty += adj.penalty;
		return penalty;
	}
};
function normalizeDef(def) {
	const { map,...rest } = def;
	return {
		...rest,
		map: normalizeMap(map)
	};
}
function normalizeMap(map) {
	return map.replaceAll(matchPossibleWordSeparators, DEFAULT_COMPOUNDED_WORD_SEPARATOR);
}

//#endregion
//#region src/lib/distance/distanceAStarWeighted.ts
/**
* Calculate the edit distance between two words using an A* algorithm.
*
* Using basic weights, this algorithm has the same results as the Damerau-Levenshtein algorithm.
*/
function distanceAStarWeighted(wordA, wordB, map, cost = 100) {
	const calc = createWeightCostCalculator(map);
	const best = _distanceAStarWeightedEx(wordA, wordB, calc, cost);
	const penalty = calc.calcAdjustment(wordB);
	return best.c + best.p + penalty;
}
function _distanceAStarWeightedEx(wordA, wordB, map, cost = 100) {
	const a = "^" + wordA + "$";
	const b = "^" + wordB + "$";
	const aN = a.length;
	const bN = b.length;
	const candidates = new CandidatePool(aN, bN);
	candidates.add({
		ai: 0,
		bi: 0,
		c: 0,
		p: 0,
		f: void 0
	});
	/** Substitute / Replace */
	function opSub(n) {
		const { ai, bi, c, p } = n;
		if (ai < aN && bi < bN) {
			const cc = a[ai] === b[bi] ? c : c + cost;
			candidates.add({
				ai: ai + 1,
				bi: bi + 1,
				c: cc,
				p,
				f: n
			});
		}
	}
	/** Insert */
	function opIns(n) {
		const { ai, bi, c, p } = n;
		if (bi < bN) candidates.add({
			ai,
			bi: bi + 1,
			c: c + cost,
			p,
			f: n
		});
	}
	/** Delete */
	function opDel(n) {
		const { ai, bi, c, p } = n;
		if (ai < aN) candidates.add({
			ai: ai + 1,
			bi,
			c: c + cost,
			p,
			f: n
		});
	}
	/** Swap adjacent letters */
	function opSwap(n) {
		const { ai, bi, c, p } = n;
		if (a[ai] === b[bi + 1] && a[ai + 1] === b[bi]) candidates.add({
			ai: ai + 2,
			bi: bi + 2,
			c: c + cost,
			p,
			f: n
		});
	}
	function opMap$1(n) {
		const { ai, bi, c, p } = n;
		const pos = {
			a,
			b,
			ai,
			bi,
			c,
			p
		};
		[
			map.calcInsDelCosts(pos),
			map.calcSwapCosts(pos),
			map.calcReplaceCosts(pos)
		].forEach((iter) => {
			for (const nn of iter) candidates.add({
				...nn,
				f: n
			});
		});
	}
	let best;
	while (best = candidates.next()) {
		if (best.ai === aN && best.bi === bN) break;
		opSwap(best);
		opIns(best);
		opDel(best);
		opMap$1(best);
		opSub(best);
	}
	assert(best);
	return best;
}
var CandidatePool = class {
	pool = new PairingHeap(compare$1);
	grid = [];
	constructor(aN, bN) {
		this.aN = aN;
		this.bN = bN;
	}
	next() {
		let n;
		while (n = this.pool.dequeue()) if (!n.d) return n;
	}
	add(n) {
		const i = idx(n.ai, n.bi, this.bN);
		const g = this.grid[i];
		if (!g) {
			this.grid[i] = n;
			this.pool.add(n);
			return;
		}
		if (g.c <= n.c) return;
		g.d = true;
		this.grid[i] = n;
		this.pool.add(n);
	}
};
function idx(r, c, cols) {
	return r * cols + c;
}
function compare$1(a, b) {
	return a.c - b.c || b.ai + b.bi - a.ai - a.bi;
}

//#endregion
//#region src/lib/distance/levenshtein.ts
const initialRow = [...".".repeat(50)].map((_, i) => i);
Object.freeze(initialRow);
/**
* Damerau–Levenshtein distance
* [Damerau–Levenshtein distance - Wikipedia](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)
* @param a - first word
* @param b - second word
* @returns Distance value
*/
function levenshteinDistance(a, b) {
	const aa = "  " + a;
	const bb = "  " + b;
	const nA = a.length + 1;
	const nB = b.length + 1;
	const firstRow = initialRow.slice(0, nA + 1);
	for (let i = firstRow.length; i <= nA; ++i) firstRow[i] = i;
	const matrix = [
		firstRow,
		[1, ...firstRow],
		[
			2,
			1,
			...firstRow
		]
	];
	let ppRow = matrix[0];
	let pRow = matrix[1];
	for (let j = 2; j <= nB; ++j) {
		const row = matrix[j % 3];
		row[0] = pRow[0] + 1;
		row[1] = pRow[1] + 1;
		const bp = bb[j - 1];
		const bc = bb[j];
		let ap = aa[0];
		for (let i = 2, i1 = 1; i <= nA; i1 = i, ++i) {
			const ac = aa[i];
			const c = pRow[i1] + (ac == bc ? 0 : 1);
			const ct = ac == bp && ap == bc ? ppRow[i1 - 1] + 1 : c;
			row[i] = Math.min(c, ct, pRow[i] + 1, row[i1] + 1);
			ap = ac;
		}
		ppRow = pRow;
		pRow = row;
	}
	return pRow[nA];
}

//#endregion
//#region src/lib/distance/distance.ts
const defaultCost = 100;
/**
* Calculate the edit distance between any two words.
* Use the Damerau–Levenshtein distance algorithm.
* @param wordA
* @param wordB
* @param editCost - the cost of each edit (defaults to 100)
* @returns the edit distance.
*/
function editDistance(wordA, wordB, editCost = defaultCost) {
	return levenshteinDistance(wordA, wordB) * editCost;
}
/**
* Calculate the weighted edit distance between any two words.
* @param wordA
* @param wordB
* @param weights - the weights to use
* @param editCost - the cost of each edit (defaults to 100)
* @returns the edit distance
*/
function editDistanceWeighted(wordA, wordB, weights, editCost = defaultCost) {
	return distanceAStarWeighted(wordA, wordB, weights, editCost);
}
/**
* Collect Map definitions into a single weighted map.
* @param defs - list of definitions
* @returns A Weighted Map to be used with distance calculations.
*/
function createWeightedMap(defs) {
	return createWeightMap(...defs);
}

//#endregion
//#region src/lib/utils/timer.ts
function startTimer() {
	const start = performance.now();
	return () => performance.now() - start;
}
function createPerfTimer() {
	const timer = startTimer();
	const active = /* @__PURE__ */ new Map();
	const events = [{
		name: "start",
		at: 0
	}];
	function updateEvent(event, atTime = timer()) {
		const elapsed = atTime - event.at;
		event.elapsed = (event.elapsed || 0) + elapsed;
		return elapsed;
	}
	function start(name) {
		const event = createEvent(name || "start");
		events.push(event);
		name && active.set(name, event);
		return () => updateEvent(event);
	}
	function stop(name) {
		const knownEvent = name && active.get(name);
		if (knownEvent) return updateEvent(knownEvent);
		return mark(name || "stop");
	}
	function createEvent(name) {
		return {
			name,
			at: timer()
		};
	}
	function mark(name) {
		const event = createEvent(name);
		events.push(event);
		return event.at;
	}
	function formatReport() {
		const lineElements = [
			{
				name: "Event Name",
				at: "Time",
				elapsed: "Elapsed"
			},
			{
				name: "----------",
				at: "----",
				elapsed: "-------"
			},
			...mapEvents()
		];
		function mapEvents() {
			const stack = [];
			return events.map((e) => {
				for (let s = stack.pop(); s; s = stack.pop()) if (s >= e.at + (e.elapsed || 0)) {
					stack.push(s);
					break;
				}
				const d = stack.length;
				if (e.elapsed) stack.push(e.at + e.elapsed);
				return {
					name: "| ".repeat(d) + (e.name || "").replaceAll("	", "  "),
					at: `${t(e.at)}`,
					elapsed: e.elapsed ? `${t(e.elapsed)}` : "--"
				};
			});
		}
		function t(ms) {
			return ms.toFixed(3) + "ms";
		}
		function m(v, s) {
			return Math.max(v, s.length);
		}
		const lengths = lineElements.reduce((a, b) => ({
			name: m(a.name, b.name),
			at: m(a.at, b.at),
			elapsed: m(a.elapsed, b.elapsed)
		}), {
			name: 0,
			at: 0,
			elapsed: 0
		});
		return lineElements.map((e) => `${e.at.padStart(lengths.at)}  ${e.name.padEnd(lengths.name)}  ${e.elapsed.padStart(lengths.elapsed)}`).join("\n");
	}
	function measureFn(name, fn) {
		const s = start(name);
		const v = fn();
		s();
		return v;
	}
	async function measureAsyncFn(name, fn) {
		const s = start(name);
		const v = await fn();
		s();
		return v;
	}
	function report(reporter = console.log) {
		reporter(formatReport());
	}
	return {
		start,
		stop,
		mark,
		elapsed: timer,
		report,
		formatReport,
		measureFn,
		measureAsyncFn
	};
}
let globalPerfTimer = void 0;
function getGlobalPerfTimer() {
	const timer = globalPerfTimer || createPerfTimer();
	globalPerfTimer = timer;
	return timer;
}

//#endregion
//#region src/lib/utils/util.ts
function isDefined$1(a) {
	return a !== void 0;
}
/**
* Remove any fields with an `undefined` value.
* @param t - object to clean
* @returns t
*/
function cleanCopy(t) {
	return clean$1({ ...t });
}
/**
* Remove any fields with an `undefined` value.
* **MODIFIES THE OBJECT**
* @param t - object to clean
* @returns t
*/
function clean$1(t) {
	for (const prop in t) if (t[prop] === void 0) delete t[prop];
	return t;
}
function unique(a) {
	return [...new Set(a)];
}
/**
*
* @param text verbatim text to be inserted into a regexp
* @returns text that can be used in a regexp.
*/
function regexQuote(text) {
	return text.replaceAll(/([[\]\-+(){},|*.\\])/g, "\\$1");
}
/**
* Factory to create a function that will replace all occurrences of `match` with `withText`
* @param match - string to match
* @param replaceWithText - the text to substitute.
*/
function replaceAllFactory(match, replaceWithText) {
	const r = RegExp(regexQuote(match), "g");
	return (text) => text.replace(r, replaceWithText);
}

//#endregion
//#region src/lib/suggestions/suggestCollector.ts
const defaultMaxNumberSuggestions = 10;
const BASE_COST = 100;
const MAX_NUM_CHANGES = 5;
const MAX_ALLOWED_COST_SCALE = 1.03 * .5;
const collator = new Intl.Collator();
const regexSeparator = new RegExp(`[${regexQuote(WORD_SEPARATOR)}]`, "g");
const wordLengthCost = [
	0,
	50,
	25,
	5,
	0
];
const EXTRA_WORD_COST = 5;
/** time in ms */
const DEFAULT_COLLECTOR_TIMEOUT = 1e3;
const symStopProcessing = Symbol("Collector Stop Processing");
function compSuggestionResults(a, b) {
	return (a.isPreferred && -1 || 0) - (b.isPreferred && -1 || 0) || a.cost - b.cost || a.word.length - b.word.length || collator.compare(a.word, b.word);
}
const defaultSuggestionCollectorOptions = Object.freeze({
	numSuggestions: defaultMaxNumberSuggestions,
	filter: () => true,
	changeLimit: MAX_NUM_CHANGES,
	includeTies: false,
	ignoreCase: true,
	timeout: DEFAULT_COLLECTOR_TIMEOUT,
	weightMap: void 0,
	compoundSeparator: "",
	compoundMethod: void 0
});
function suggestionCollector(wordToMatch, options) {
	const { filter = () => true, changeLimit = MAX_NUM_CHANGES, includeTies = false, ignoreCase = true, timeout = DEFAULT_COLLECTOR_TIMEOUT, weightMap, compoundSeparator = defaultSuggestionCollectorOptions.compoundSeparator } = options;
	const numSuggestions = Math.max(options.numSuggestions, 0) || 0;
	const numSugToHold = weightMap ? numSuggestions * 2 : numSuggestions;
	const sugs = /* @__PURE__ */ new Map();
	let maxCost = BASE_COST * Math.min(wordToMatch.length * MAX_ALLOWED_COST_SCALE, changeLimit);
	const useSeparator = compoundSeparator || (weightMap ? DEFAULT_COMPOUNDED_WORD_SEPARATOR : defaultSuggestionCollectorOptions.compoundSeparator);
	const fnCleanWord = !useSeparator || useSeparator === compoundSeparator ? (w) => w : replaceAllFactory(useSeparator, "");
	if (useSeparator && weightMap) addDefToWeightMap(weightMap, {
		map: useSeparator,
		insDel: 50
	});
	const genSuggestionOptions = clean$1({
		changeLimit,
		ignoreCase,
		compoundMethod: options.compoundMethod,
		compoundSeparator: useSeparator
	});
	let timeRemaining = timeout;
	function dropMax() {
		if (sugs.size < 2 || !numSuggestions) {
			sugs.clear();
			return;
		}
		const sorted = [...sugs.values()].sort(compSuggestionResults);
		let i = numSugToHold - 1;
		maxCost = sorted[i].cost;
		for (; i < sorted.length && sorted[i].cost <= maxCost; ++i);
		for (; i < sorted.length; ++i) sugs.delete(sorted[i].word);
	}
	function adjustCost(sug) {
		if (sug.isPreferred) return sug;
		const words = sug.word.split(regexSeparator);
		const extraCost = words.map((w) => wordLengthCost[w.length] || 0).reduce((a, b) => a + b, 0) + (words.length - 1) * EXTRA_WORD_COST;
		return {
			word: sug.word,
			cost: sug.cost + extraCost
		};
	}
	function collectSuggestion(suggestion) {
		const { word, cost, isPreferred } = adjustCost(suggestion);
		if (cost <= maxCost && filter(suggestion.word, cost)) {
			const known = sugs.get(word);
			if (known) {
				known.cost = Math.min(known.cost, cost);
				known.isPreferred = known.isPreferred || isPreferred;
			} else {
				sugs.set(word, {
					word,
					cost,
					isPreferred
				});
				if (cost < maxCost && sugs.size > numSugToHold) dropMax();
			}
		}
		return maxCost;
	}
	/**
	* Collection suggestions from a SuggestionIterator
	* @param src - the SuggestionIterator used to generate suggestions.
	* @param timeout - the amount of time in milliseconds to allow for suggestions.
	*/
	function collect(src, timeout$1, filter$1) {
		let stop = false;
		timeout$1 = timeout$1 ?? timeRemaining;
		timeout$1 = Math.min(timeout$1, timeRemaining);
		if (timeout$1 < 0) return;
		const timer = startTimer();
		let ir;
		while (!(ir = src.next(stop || maxCost)).done) {
			if (timer() > timeout$1) stop = symStopProcessing;
			const { value } = ir;
			if (!value) continue;
			if (isSuggestionResult(value)) {
				if (!filter$1 || filter$1(value.word, value.cost)) collectSuggestion(value);
				continue;
			}
		}
		timeRemaining -= timer();
	}
	function cleanCompoundResult(sr) {
		const { word, cost } = sr;
		const cWord = fnCleanWord(word);
		if (cWord !== word) return {
			word: cWord,
			cost,
			compoundWord: word,
			isPreferred: void 0
		};
		return { ...sr };
	}
	function suggestions() {
		if (numSuggestions < 1 || !sugs.size) return [];
		const NF = "NFD";
		const nWordToMatch = wordToMatch.normalize(NF);
		const rawValues = [...sugs.values()];
		const sorted = (weightMap ? rawValues.map(({ word, cost, isPreferred }) => ({
			word,
			cost: isPreferred ? cost : editDistanceWeighted(nWordToMatch, word.normalize(NF), weightMap, 110),
			isPreferred
		})) : rawValues).sort(compSuggestionResults).map(cleanCompoundResult);
		let i = Math.min(sorted.length, numSuggestions) - 1;
		const limit = includeTies ? sorted.length : Math.min(sorted.length, numSuggestions);
		const iCost = sorted[i].cost;
		const maxCost$1 = Math.min(iCost, weightMap ? changeLimit * BASE_COST - 1 : iCost);
		for (i = 1; i < limit && sorted[i].cost <= maxCost$1; ++i);
		sorted.length = i;
		return sorted;
	}
	return {
		collect,
		add: function(suggestion) {
			collectSuggestion(suggestion);
			return this;
		},
		get suggestions() {
			return suggestions();
		},
		get maxCost() {
			return maxCost;
		},
		get word() {
			return wordToMatch;
		},
		get maxNumSuggestions() {
			return numSuggestions;
		},
		get changeLimit() {
			return changeLimit;
		},
		includesTies: includeTies,
		ignoreCase,
		symbolStopProcessing: symStopProcessing,
		genSuggestionOptions
	};
}
/**
* Impersonating a Collector, allows searching for multiple variants on the same word.
* The collection is still in the original collector.
* @param collector - collector to impersonate
* @param word - word to present instead of `collector.word`.
* @returns a SuggestionCollector
*/
function impersonateCollector(collector, word) {
	const r = Object.create(collector);
	Object.defineProperty(r, "word", {
		value: word,
		writable: false
	});
	return r;
}
function isSuggestionResult(s) {
	const r = s;
	return !!r && typeof r === "object" && r?.cost !== void 0 && r.word != void 0;
}

//#endregion
//#region src/lib/suggestions/suggestAStar.ts
/**
* Compare Path Nodes.
* Balance the calculation between depth vs cost
*/
function comparePath(a, b) {
	return a.c / (a.i + 1) - b.c / (b.i + 1) + (b.i - a.i);
}
function suggestAStar(trie, word, options = {}) {
	const opts = createSuggestionOptions(options);
	const collector = suggestionCollector(word, opts);
	collector.collect(getSuggestionsAStar(trie, word, opts));
	return collector.suggestions;
}
function* getSuggestionsAStar(trie, srcWord, options = {}) {
	const { compoundMethod, changeLimit, ignoreCase, weightMap } = createSuggestionOptions(options);
	const visMap = visualLetterMaskMap;
	const root = trie.getRoot();
	const rootIgnoreCase = ignoreCase && root.get(root.info.stripCaseAndAccentsPrefix) || void 0;
	const pathHeap = new PairingHeap(comparePath);
	const resultHeap = new PairingHeap(compareSuggestion);
	const rootPNode = {
		n: root,
		i: 0,
		c: 0,
		s: "",
		p: void 0,
		t: createCostTrie()
	};
	const BC = opCosts.baseCost;
	const VC = opCosts.visuallySimilar;
	const DL = opCosts.duplicateLetterCost;
	const wordSeparator = compoundMethod === CompoundWordsMethod.JOIN_WORDS ? JOIN_SEPARATOR : WORD_SEPARATOR;
	const sc = specialChars(trie.info);
	const comp = trie.info.compoundCharacter;
	const compRoot = root.get(comp);
	const compRootIgnoreCase = rootIgnoreCase && rootIgnoreCase.get(comp);
	const emitted = Object.create(null);
	const srcLetters = [...srcWord];
	/** Initial limit is based upon the length of the word. */
	let limit = BC * Math.min(srcLetters.length * opCosts.wordLengthCostFactor, changeLimit);
	pathHeap.add(rootPNode);
	if (rootIgnoreCase) pathHeap.add({
		n: rootIgnoreCase,
		i: 0,
		c: 0,
		s: "",
		p: void 0,
		t: createCostTrie()
	});
	let best = pathHeap.dequeue();
	let maxSize = pathHeap.size;
	let suggestionsGenerated = 0;
	let nodesProcessed = 0;
	let nodesProcessedLimit = 1e3;
	let minGen = 1;
	while (best) {
		if (++nodesProcessed > nodesProcessedLimit) {
			nodesProcessedLimit += 1e3;
			if (suggestionsGenerated < minGen) break;
			minGen += suggestionsGenerated;
		}
		if (best.c > limit) {
			best = pathHeap.dequeue();
			maxSize = Math.max(maxSize, pathHeap.size);
			continue;
		}
		processPath(best);
		for (const sug of resultHeap) {
			++suggestionsGenerated;
			if (sug.cost > limit) continue;
			if (sug.word in emitted && emitted[sug.word] <= sug.cost) continue;
			const action = yield sug;
			emitted[sug.word] = sug.cost;
			if (typeof action === "number") limit = Math.min(action, limit);
			if (typeof action === "symbol") return;
		}
		best = pathHeap.dequeue();
		maxSize = Math.max(maxSize, pathHeap.size);
	}
	return;
	function compareSuggestion(a, b) {
		const pa = a.isPreferred && 1 || 0;
		return (b.isPreferred && 1 || 0) - pa || a.cost - b.cost || Math.abs(a.word.charCodeAt(0) - srcWord.charCodeAt(0)) - Math.abs(b.word.charCodeAt(0) - srcWord.charCodeAt(0));
	}
	function processPath(p) {
		const len = srcLetters.length;
		if (p.n.eow && p.i === len) {
			const result = {
				word: pNodeToWord(p),
				cost: p.c
			};
			resultHeap.add(result);
		}
		calcEdges(p);
	}
	function calcEdges(p) {
		const { n, i, t } = p;
		const s = srcLetters[i];
		const sg = visMap[s] || 0;
		const cost0 = p.c;
		const cost = cost0 + BC + (i ? 0 : opCosts.firstLetterBias);
		const costVis = cost0 + VC;
		const costLegacyCompound = cost0 + opCosts.wordBreak;
		const costCompound = cost0 + opCosts.compound;
		if (s) {
			const m = n.get(s);
			if (m) storePath(t, m, i + 1, cost0, s, p, "=", s);
			if (weightMap) processWeightMapEdges(p, weightMap);
			const ns = srcLetters[i + 1];
			if (s == ns && m) storePath(t, m, i + 2, cost0 + DL, s, p, "dd", s);
			storePath(t, n, i + 1, cost, "", p, "d", "");
			for (const [ss, node] of n.entries()) {
				if (node.id === m?.id || ss in sc) continue;
				const c = sg & (visMap[ss] || 0) ? costVis : cost;
				storePath(t, node, i + 1, c, ss, p, "r", ss);
			}
			if (n.eow && i && compoundMethod) storePath(t, root, i, costLegacyCompound, wordSeparator, p, "L", wordSeparator);
			if (ns) {
				const n2 = n.get(ns)?.get(s);
				if (n2) {
					const ss = ns + s;
					storePath(t, n2, i + 2, cost0 + opCosts.swapCost, ss, p, "s", ss);
				}
			}
		}
		if (compRoot && costCompound <= limit && n.get(comp)) {
			if (compRootIgnoreCase) storePath(t, compRootIgnoreCase, i, costCompound, "", p, "~+", "~+");
			storePath(t, compRoot, i, costCompound, "", p, "+", "+");
		}
		if (cost <= limit) for (const [char, node] of n.entries()) {
			if (char in sc) continue;
			storePath(t, node, i, cost, char, p, "i", char);
		}
	}
	function processWeightMapEdges(p, weightMap$1) {
		delLetters(p, weightMap$1, srcLetters, storePath);
		insLetters(p, weightMap$1, srcLetters, storePath);
		repLetters(p, weightMap$1, srcLetters, storePath);
	}
	/**
	* Apply a cost to the current step.
	* @param t - trie node
	* @param s - letter to apply, empty string means to apply to the current node
	* @param i - index
	* @param c - cost
	* @returns PNode if it was applied, otherwise undefined
	*/
	function storePath(t, n, i, c, s, p, a, ss) {
		const tt = getCostTrie(t, ss);
		if (tt.c[i] <= c || c > limit) return void 0;
		tt.c[i] = c;
		pathHeap.add({
			n,
			i,
			c,
			s,
			p,
			t: tt,
			a
		});
	}
}
function delLetters(pNode, weightMap, letters, storePath) {
	const { t, n } = pNode;
	const trie = weightMap.insDel;
	let ii = pNode.i;
	const cost0 = pNode.c - pNode.i;
	const len = letters.length;
	for (let nn = trie.n; ii < len && nn;) {
		const tt = nn[letters[ii]];
		if (!tt) return;
		++ii;
		if (tt.c !== void 0) storePath(t, n, ii, cost0 + tt.c, "", pNode, "d", "");
		nn = tt.n;
	}
}
function insLetters(p, weightMap, _letters, storePath) {
	const { t, i, c, n } = p;
	const cost0 = c;
	searchTrieCostNodesMatchingTrie2(weightMap.insDel, n, (s, tc, n$1) => {
		if (tc.c !== void 0) storePath(t, n$1, i, cost0 + tc.c, s, p, "i", s);
	});
}
function repLetters(pNode, weightMap, letters, storePath) {
	const node = pNode.n;
	const pt = pNode.t;
	const cost0 = pNode.c;
	const len = letters.length;
	const trie = weightMap.replace;
	let i = pNode.i;
	for (let n = trie.n; i < len && n;) {
		const t = n[letters[i]];
		if (!t) return;
		++i;
		const tInsert = t.t;
		if (tInsert) searchTrieCostNodesMatchingTrie2(tInsert, node, (s, tt, n$1) => {
			const c = tt.c;
			if (c === void 0) return;
			storePath(pt, n$1, i, cost0 + c + (tt.p || 0), s, pNode, "r", s);
		});
		n = t.n;
	}
}
function createCostTrie() {
	return {
		c: [],
		t: Object.create(null)
	};
}
function getCostTrie(t, s) {
	if (s.length == 1) return t.t[s] ??= createCostTrie();
	if (!s) return t;
	let tt = t;
	for (const c of s) tt = tt.t[c] ??= createCostTrie();
	return tt;
}
function pNodeToWord(p) {
	const parts = [];
	let n = p;
	while (n) {
		parts.push(n.s);
		n = n.p;
	}
	parts.reverse();
	return parts.join("");
}
function specialChars(options) {
	const charSet = Object.create(null);
	for (const c of Object.values(options)) if (typeof c === "string") charSet[c] = true;
	return charSet;
}
function searchTrieCostNodesMatchingTrie2(trie, node, emit, s = "") {
	const n = trie.n;
	if (!n) return;
	for (const [key, c] of node.entries()) {
		const t = n[key];
		if (!t) continue;
		const pfx = s + key;
		emit(pfx, t, c);
		if (t.n) searchTrieCostNodesMatchingTrie2(t, c, emit, pfx);
	}
}

//#endregion
//#region src/lib/constants.ts
const COMPOUND_FIX = "+";
const OPTIONAL_COMPOUND_FIX = "*";
const CASE_INSENSITIVE_PREFIX = "~";
const FORBID_PREFIX = "!";
const LINE_COMMENT = "#";
const IDENTITY_PREFIX = "=";
const defaultTrieInfo = Object.freeze({
	compoundCharacter: COMPOUND_FIX,
	forbiddenWordPrefix: FORBID_PREFIX,
	stripCaseAndAccentsPrefix: CASE_INSENSITIVE_PREFIX,
	isCaseAware: true,
	hasForbiddenWords: false,
	hasCompoundWords: false,
	hasNonStrictWords: false
});

//#endregion
//#region src/lib/utils/mergeDefaults.ts
/**
* Creates a new object of type T based upon the field values from `value`.
* n[k] = value[k] ?? default[k] where k must be a field in default.
* Note: it will remove fields not in defaultValue!
* @param value
* @param defaultValue
*/
function mergeDefaults(value, defaultValue) {
	const result = { ...defaultValue };
	if (value) {
		for (const [k, v] of Object.entries(value)) if (k in result) result[k] = v ?? result[k];
	}
	return result;
}

//#endregion
//#region src/lib/utils/mergeOptionalWithDefaults.ts
function mergeOptionalWithDefaults(...options) {
	return options.reduce((acc, opt) => mergeDefaults(opt, acc), defaultTrieInfo);
}

//#endregion
//#region src/lib/utils/text.ts
/**
* Expand a line into a set of characters.
*
* Example:
* - `a-c` -> `<a,b,c>`
* - `ac-` -> `<a,c,->`
* - `-abz` -> `<-,a,b,z>`
* - `\u0300-\u0308` -> `<accents>`
*
* @param line - set of characters
* @param rangeChar - the character to indicate ranges, set to empty to not have ranges.
*/
function expandCharacterSet(line, rangeChar = "-") {
	const charSet = /* @__PURE__ */ new Set();
	let mode = 0;
	let prev = "";
	for (const char of line) {
		if (mode) {
			expandRange(prev, char).forEach((a) => charSet.add(a));
			mode = 0;
		}
		if (char === rangeChar && prev) {
			mode = 1;
			continue;
		}
		charSet.add(char);
		prev = char;
	}
	if (mode) charSet.add(rangeChar);
	return charSet;
}
/**
* Expands a range between two characters.
* - `a <= b` -- `[a, b]`
* - `a > b` -- `[]`
* @param a - staring character
* @param b - ending character
* @returns array of unicode characters.
*/
function expandRange(a, b) {
	const values = [];
	const end = b.codePointAt(0);
	const begin = a.codePointAt(0);
	if (!(begin && end)) return values;
	for (let i = begin; i <= end; ++i) values.push(String.fromCodePoint(i));
	return values;
}
/**
* Tries to find the different cases for a letter.
* It can generate multiple forms:
* - `ß` => `['ß', 'SS', 'ss']`
* - `a` => `['a', 'A']`
* - `A` => `['A', 'z']`
* - `Å` => `['A', 'z']`
* @param letter - the letter to generate upper and lower cases.
* @param locale - the locale to use for changing case.
* @returns the set of found cases.
*/
function caseForms(letter, locale) {
	const forms$1 = new Set([letter]);
	function tryCases(s) {
		forms$1.add(s.toLocaleLowerCase(locale));
		forms$1.add(s.toLocaleUpperCase(locale));
	}
	tryCases(letter);
	[...forms$1].forEach(tryCases);
	return [...forms$1].filter((a) => !!a);
}
/**
* Generate the different normalized forms of the letters.
* @param letter - letter to normalize.
* @returns combined set of possible forms.
*/
function accentForms(letter) {
	return new Set([
		letter,
		letter.normalize("NFC"),
		letter.normalize("NFD")
	]);
}
/**
* Remove all accents.
* @param characters - unicode characters
* @returns characters with accents removed (if it was possible)
*/
function stripAccents(characters) {
	return characters.normalize("NFD").replaceAll(/\p{M}/gu, "");
}
/**
* Remove all non accent characters from a string.
* @param characters - characters with accents.
* @returns - only the accents.
*/
function stripNonAccents(characters) {
	return characters.normalize("NFD").replaceAll(/[^\p{M}]/gu, "");
}
function isValidUtf16Character(char) {
	const len = char.length;
	const code = char.charCodeAt(0) & 64512;
	return len === 1 && (code & 63488) !== 55296 || len === 2 && (code & 64512) === 55296 && (char.charCodeAt(1) & 64512) === 56320;
}
function assertValidUtf16Character(char) {
	if (!isValidUtf16Character(char)) {
		const len = char.length;
		const codes$1 = toCharCodes(char.slice(0, 2)).map((c) => "0x" + ("0000" + c.toString(16)).slice(-4));
		let message;
		if (len == 1) message = `Invalid utf16 character, lone surrogate: ${codes$1[0]}`;
		else if (len == 2) message = `Invalid utf16 character, not a valid surrogate pair: [${codes$1.join(", ")}]`;
		else message = `Invalid utf16 character, must be a single character, found: ${len}`;
		throw new Error(message);
	}
}
function toCharCodes(s) {
	const values = [];
	for (let i = 0; i < s.length; ++i) values.push(s.charCodeAt(i));
	return values;
}

//#endregion
//#region src/lib/TrieBlob/Utf8.ts
/**
* Encode a CodePoint into a Big Endian utf8 value, up to 4 bytes.
* These numbers sort into the correct order for utf8.
*
*            hightest byte           lowest byte   Code Point Range
* - 1 byte:  00000000 00000000 00000000 0xxxxxxx - 0x0000_0000 - 0x0000_007f
* - 2 bytes: 00000000 00000000 110xxxxx 10xxxxxx - 0x0000_0080 - 0x0000_07ff
* - 3 bytes: 00000000 1110xxxx 10xxxxxx 10xxxxxx - 0x0000_0800 - 0x0000_ffff
* - 4 bytes: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx - 0x0001_0000 - 0x001f_ffff
*
* @param code - the code point to encode
* @returns number containing the utf8 value.
*/
function encodeUtf8N_BE(code) {
	if (code < 128) return code;
	if (code < 2048) return 49280 | (code & 1984) << 2 | code & 63;
	if (code < 65536) return 14712960 | (code & 61440) << 4 | (code & 4032) << 2 | code & 63;
	return 4034953344 + ((code & 1835008) << 6 | (code & 258048) << 4 | (code & 4032) << 2 | code & 63);
}
var Utf8Accumulator = class Utf8Accumulator {
	remaining = 0;
	value = 0;
	decode(byte) {
		let remaining = this.remaining;
		if (byte & -256) return this.reset();
		if ((byte & 128) === 0) {
			if (remaining) return this.reset();
			return byte;
		}
		if (remaining) {
			if ((byte & 192) !== 128) return this.reset();
			let value = this.value;
			value = value << 6 | byte & 63;
			this.value = value;
			remaining -= 1;
			this.remaining = remaining;
			return remaining ? void 0 : value;
		}
		if ((byte & 224) === 192) {
			this.value = byte & 31;
			this.remaining = 1;
			return;
		}
		if ((byte & 240) === 224) {
			this.value = byte & 15;
			this.remaining = 2;
			return;
		}
		if ((byte & 248) === 240) {
			this.value = byte & 7;
			this.remaining = 3;
			return;
		}
		return this.reset();
	}
	reset() {
		this.remaining = 0;
		this.value = 0;
		return 65533;
	}
	clone(into = new Utf8Accumulator()) {
		into.remaining = this.remaining;
		into.value = this.value;
		return into;
	}
	static isMultiByte(v) {
		return (v & 128) !== 0;
	}
	static isSingleByte(v) {
		return (v & 128) === 0;
	}
	static create() {
		return new this();
	}
};
function encodeTextToUtf8Into(text, into, offset = 0) {
	let i = offset;
	const len = text.length;
	for (let j = 0; j < len; j++) {
		let code = text.charCodeAt(j);
		code = (code & 63488) === 55296 ? text.codePointAt(j++) || 0 : code;
		if (code < 128) {
			into[i++] = code;
			continue;
		}
		if (code < 2048) {
			const u$1 = 49280 | (code & 1984) << 2 | code & 63;
			into[i++] = u$1 >>> 8;
			into[i++] = u$1 & 255;
			continue;
		}
		if (code < 65536) {
			const u$1 = 14712960 | (code & 61440) << 4 | (code & 4032) << 2 | code & 63;
			into[i++] = u$1 >>> 16;
			into[i++] = u$1 >>> 8 & 255;
			into[i++] = u$1 & 255;
			continue;
		}
		const u = 4034953344 | ((code & 1835008) << 6 | (code & 258048) << 4 | (code & 4032) << 2 | code & 63);
		into[i++] = u >>> 24 & 255;
		into[i++] = u >>> 16 & 255;
		into[i++] = u >>> 8 & 255;
		into[i++] = u & 255;
	}
	return i - offset;
}
function encodeTextToUtf8(text) {
	const array = new Array(text.length);
	const len = encodeTextToUtf8Into(text, array);
	array.length !== len && (array.length = len);
	return array;
}

//#endregion
//#region src/lib/TrieBlob/CharIndex.ts
Object.freeze([0]);
var CharIndex = class {
	#charToUtf8SeqMap;
	#lastWord = "";
	#lastWordSeq = [];
	#multiByteChars;
	constructor(charIndex) {
		this.charIndex = charIndex;
		this.#charToUtf8SeqMap = buildCharIndexSequenceMap(charIndex);
		this.#multiByteChars = [...this.#charToUtf8SeqMap.values()].some((c) => c.length > 1);
	}
	getCharUtf8Seq(c) {
		const found = this.#charToUtf8SeqMap.get(c);
		if (found) return found;
		const s = encodeTextToUtf8(c);
		this.#charToUtf8SeqMap.set(c, s);
		return s;
	}
	wordToUtf8Seq(word) {
		if (this.#lastWord === word) return this.#lastWordSeq;
		const seq = encodeTextToUtf8(word);
		this.#lastWord = word;
		this.#lastWordSeq = seq;
		return seq;
	}
	indexContainsMultiByteChars() {
		return this.#multiByteChars;
	}
	get size() {
		return this.charIndex.length;
	}
	toJSON() {
		return { charIndex: this.charIndex };
	}
};
function buildCharIndexSequenceMap(charIndex) {
	const map = /* @__PURE__ */ new Map();
	for (const key of charIndex) map.set(key, encodeTextToUtf8(key));
	return map;
}
var CharIndexBuilder = class {
	charIndex = [];
	charIndexMap = /* @__PURE__ */ new Map();
	charIndexSeqMap = /* @__PURE__ */ new Map();
	#mapIdxToSeq = /* @__PURE__ */ new Map();
	constructor() {
		this.getUtf8Value("");
	}
	getUtf8Value(c) {
		const found = this.charIndexMap.get(c);
		if (found !== void 0) return found;
		const nc = c.normalize("NFC");
		this.charIndex.push(nc);
		const utf8 = encodeUtf8N_BE(nc.codePointAt(0) || 0);
		this.charIndexMap.set(c, utf8);
		this.charIndexMap.set(nc, utf8);
		this.charIndexMap.set(c.normalize("NFD"), utf8);
		return utf8;
	}
	utf8ValueToUtf8Seq(idx$1) {
		const found = this.#mapIdxToSeq.get(idx$1);
		if (found !== void 0) return found;
		const seq = splitUtf8(idx$1);
		this.#mapIdxToSeq.set(idx$1, seq);
		return seq;
	}
	charToUtf8Seq(c) {
		const idx$1 = this.getUtf8Value(c);
		return this.utf8ValueToUtf8Seq(idx$1);
	}
	wordToUtf8Seq(word) {
		const seq = new Array(word.length);
		let i = 0;
		for (const c of word) {
			const idx$1 = this.getUtf8Value(c);
			const cSep = this.utf8ValueToUtf8Seq(idx$1);
			if (typeof cSep === "number") {
				seq[i++] = cSep;
				continue;
			}
			for (const cIdx of cSep) seq[i++] = cIdx;
		}
		if (seq.length !== i) seq.length = i;
		return seq;
	}
	get size() {
		return this.charIndex.length;
	}
	build() {
		return new CharIndex(this.charIndex);
	}
};
function splitUtf8(utf8) {
	if (utf8 <= 255) return [utf8];
	if (utf8 <= 65535) return [utf8 >> 8 & 255, utf8 & 255];
	if (utf8 <= 16777215) return [
		utf8 >> 16 & 255,
		utf8 >> 8 & 255,
		utf8 & 255
	];
	return [
		utf8 >> 24 & 255,
		utf8 >> 16 & 255,
		utf8 >> 8 & 255,
		utf8 & 255
	].filter((v) => v);
}

//#endregion
//#region src/lib/TrieBlob/FastTrieBlobBitMaskInfo.ts
function extractInfo(info) {
	const { NodeMaskEOW, NodeMaskChildCharIndex, NodeChildRefShift } = info;
	return {
		NodeMaskEOW,
		NodeMaskChildCharIndex,
		NodeChildRefShift
	};
}

//#endregion
//#region src/lib/TrieBlob/FastTrieBlobInternals.ts
var FastTrieBlobInternals = class {
	NodeMaskEOW;
	NodeMaskChildCharIndex;
	NodeChildRefShift;
	isIndexDecoderNeeded;
	info;
	constructor(nodes, charIndex, maskInfo, info) {
		this.nodes = nodes;
		this.charIndex = charIndex;
		const { NodeMaskEOW, NodeMaskChildCharIndex, NodeChildRefShift } = maskInfo;
		this.NodeMaskEOW = NodeMaskEOW;
		this.NodeMaskChildCharIndex = NodeMaskChildCharIndex;
		this.NodeChildRefShift = NodeChildRefShift;
		this.isIndexDecoderNeeded = charIndex.indexContainsMultiByteChars();
		this.info = mergeOptionalWithDefaults(info);
	}
};
var FastTrieBlobInternalsAndMethods = class extends FastTrieBlobInternals {
	nodeFindNode;
	nodeFindExact;
	nodeGetChild;
	isForbidden;
	findExact;
	hasForbiddenWords;
	hasCompoundWords;
	hasNonStrictWords;
	constructor(nodes, charIndex, maskInfo, info, trieMethods) {
		super(nodes, charIndex, maskInfo, info);
		this.nodeFindExact = trieMethods.nodeFindExact;
		this.nodeGetChild = trieMethods.nodeGetChild;
		this.isForbidden = trieMethods.isForbidden;
		this.findExact = trieMethods.findExact;
		this.nodeFindNode = trieMethods.nodeFindNode;
		this.hasForbiddenWords = trieMethods.hasForbiddenWords;
		this.hasCompoundWords = trieMethods.hasCompoundWords;
		this.hasNonStrictWords = trieMethods.hasNonStrictWords;
	}
};
/**
* Sorts the nodes in place if possible.
* @param nodes
* @param mask
* @returns
*/
function sortNodes(nodes, mask) {
	if (Object.isFrozen(nodes)) {
		assertSorted(nodes, mask);
		return nodes;
	}
	for (let i = 0; i < nodes.length; ++i) {
		let node = nodes[i];
		if (node.length > 2) {
			const isFrozen = Object.isFrozen(node);
			node = isFrozen ? Uint32Array.from(node) : node;
			const nodeInfo = node[0];
			node[0] = 0;
			node.sort((a, b) => !a ? -1 : !b ? 1 : (a & mask) - (b & mask));
			node[0] = nodeInfo;
			if (isFrozen) {
				nodes[i] = node;
				Object.freeze(node);
			}
		}
	}
	Object.freeze(nodes);
	return nodes;
}
function assertSorted(nodes, mask) {
	for (let i = 0; i < nodes.length; ++i) {
		const node = nodes[i];
		if (node.length > 2) {
			let last = -1;
			for (let j = 1; j < node.length; ++j) {
				const n = node[j] & mask;
				if (n < last) throw new Error(`Node ${i} is not sorted. ${last} > ${n}`);
				last = n;
			}
		}
	}
}

//#endregion
//#region src/lib/TrieBlob/FastTrieBlobIRoot.ts
const EmptyKeys$1 = Object.freeze([]);
const EmptyNodes$1 = Object.freeze([]);
const EmptyEntries$1 = Object.freeze([]);
var FastTrieBlobINode = class FastTrieBlobINode {
	id;
	node;
	eow;
	_keys;
	_count;
	_size;
	_chained;
	_nodesEntries;
	_entries;
	_values;
	charToIdx;
	constructor(trie, nodeIdx) {
		this.trie = trie;
		this.nodeIdx = nodeIdx;
		const node = trie.nodes[nodeIdx];
		this.node = node;
		this.eow = !!(node[0] & trie.NodeMaskEOW);
		this._count = node.length - 1;
		this.id = nodeIdx;
		this.findExact = (word) => trie.nodeFindExact(nodeIdx, word);
	}
	/** get keys to children */
	keys() {
		if (this._keys) return this._keys;
		if (!this._count) return EmptyKeys$1;
		this._keys = this.getNodesEntries().map(([key]) => key);
		return this._keys;
	}
	values() {
		if (!this._count) return EmptyNodes$1;
		if (this._values) return this._values;
		this._values = this.entries().map(([, value]) => value);
		return this._values;
	}
	entries() {
		if (this._entries) return this._entries;
		if (!this._count) return EmptyEntries$1;
		this._entries = this.getNodesEntries().map(([key, value]) => [key, new FastTrieBlobINode(this.trie, value)]);
		return this._entries;
	}
	/** get child ITrieNode */
	get(char) {
		const idx$1 = this.trie.nodeGetChild(this.id, char);
		if (idx$1 === void 0) return void 0;
		return new FastTrieBlobINode(this.trie, idx$1);
	}
	getNode(chars) {
		const idx$1 = this.trie.nodeFindNode(this.id, chars);
		if (idx$1 === void 0) return void 0;
		return new FastTrieBlobINode(this.trie, idx$1);
	}
	has(char) {
		return this.trie.nodeGetChild(this.id, char) !== void 0;
	}
	hasChildren() {
		return this._count > 0;
	}
	child(keyIdx) {
		if (!this._values && !this.containsChainedIndexes()) {
			const nodeIdx = this.node[keyIdx + 1] >>> this.trie.NodeChildRefShift;
			return new FastTrieBlobINode(this.trie, nodeIdx);
		}
		return this.values()[keyIdx];
	}
	getCharToIdxMap() {
		const m = this.charToIdx;
		if (m) return m;
		const map = Object.create(null);
		const keys = this.keys();
		for (let i = 0; i < keys.length; ++i) map[keys[i]] = i;
		this.charToIdx = map;
		return map;
	}
	findExact(word) {
		return this.trie.nodeFindExact(this.id, word);
	}
	isForbidden(word) {
		const n = this.trie.nodeGetChild(this.id, this.trie.info.forbiddenWordPrefix);
		if (n === void 0) return false;
		return this.trie.nodeFindExact(n, word);
	}
	findCaseInsensitive(word) {
		const n = this.trie.nodeGetChild(this.id, this.trie.info.stripCaseAndAccentsPrefix);
		if (n === void 0) return false;
		return this.trie.nodeFindExact(n, word);
	}
	containsChainedIndexes() {
		if (this._chained !== void 0) return this._chained;
		if (!this._count || !this.trie.isIndexDecoderNeeded) {
			this._chained = false;
			return false;
		}
		let found = false;
		const NodeMaskChildCharIndex = this.trie.NodeMaskChildCharIndex;
		const len = this._count;
		const node = this.node;
		for (let i = 1; i <= len && !found; ++i) {
			const codePoint = node[i] & NodeMaskChildCharIndex;
			found = Utf8Accumulator.isMultiByte(codePoint);
		}
		this._chained = !!found;
		return this._chained;
	}
	getNodesEntries() {
		if (this._nodesEntries) return this._nodesEntries;
		if (!this.containsChainedIndexes()) {
			const entries = Array(this._count);
			const nodes = this.node;
			const NodeMaskChildCharIndex = this.trie.NodeMaskChildCharIndex;
			const RefShift = this.trie.NodeChildRefShift;
			for (let i = 0; i < this._count; ++i) {
				const entry = nodes[i + 1];
				const codePoint = entry & NodeMaskChildCharIndex;
				entries[i] = [String.fromCodePoint(codePoint), entry >>> RefShift];
			}
			this._nodesEntries = entries;
			return entries;
		}
		this._nodesEntries = this.walkChainedIndexes();
		return this._nodesEntries;
	}
	walkChainedIndexes() {
		const NodeMaskChildCharIndex = this.trie.NodeMaskChildCharIndex;
		const NodeChildRefShift = this.trie.NodeChildRefShift;
		const nodes = this.trie.nodes;
		const acc = Utf8Accumulator.create();
		const stack = [{
			n: this.node,
			c: 1,
			acc
		}];
		let depth = 0;
		/** there is at least this._count number of entries, more if there are nested indexes. */
		const entries = Array(this._count);
		let eIdx = 0;
		while (depth >= 0) {
			const s = stack[depth];
			const { n: node, c: off } = s;
			if (off >= node.length) {
				--depth;
				continue;
			}
			++s.c;
			const entry = node[off];
			const charIdx = entry & NodeMaskChildCharIndex;
			const acc$1 = s.acc.clone();
			const codePoint = acc$1.decode(charIdx);
			if (codePoint !== void 0) {
				const char = String.fromCodePoint(codePoint);
				const nodeIdx = entry >>> NodeChildRefShift;
				entries[eIdx++] = [char, nodeIdx];
				continue;
			}
			const idx$1 = entry >>> NodeChildRefShift;
			const ss = stack[++depth];
			if (ss) {
				ss.n = nodes[idx$1];
				ss.c = 1;
				ss.acc = acc$1;
			} else stack[depth] = {
				n: nodes[idx$1],
				c: 1,
				acc: acc$1
			};
		}
		return entries;
	}
	get size() {
		if (this._size === void 0) {
			if (!this.containsChainedIndexes()) {
				this._size = this._count;
				return this._size;
			}
			this._size = this.getNodesEntries().length;
		}
		return this._size;
	}
};
var FastTrieBlobIRoot = class extends FastTrieBlobINode {
	hasForbiddenWords;
	hasCompoundWords;
	hasNonStrictWords;
	constructor(trie, nodeIdx) {
		super(trie, nodeIdx);
		this.hasForbiddenWords = trie.hasForbiddenWords;
		this.hasCompoundWords = trie.hasCompoundWords;
		this.hasNonStrictWords = trie.hasNonStrictWords;
	}
	resolveId(id) {
		return new FastTrieBlobINode(this.trie, id);
	}
	find(word, strict) {
		let found = this.findExact(word);
		if (found) return {
			found: word,
			compoundUsed: false,
			caseMatched: true
		};
		if (strict) return void 0;
		found = this.findCaseInsensitive(word);
		return found ? {
			found: word,
			compoundUsed: false,
			caseMatched: false
		} : void 0;
	}
	get info() {
		return this.trie.info;
	}
	get forbidPrefix() {
		return this.trie.info.forbiddenWordPrefix;
	}
	get compoundFix() {
		return this.trie.info.compoundCharacter;
	}
	get caseInsensitivePrefix() {
		return this.trie.info.stripCaseAndAccentsPrefix;
	}
};

//#endregion
//#region src/lib/TrieBlob/TrieBlobIRoot.ts
var TrieBlobInternals = class {
	NodeMaskEOW;
	NodeMaskNumChildren;
	NodeMaskChildCharIndex;
	NodeChildRefShift;
	isIndexDecoderNeeded;
	nodeFindExact;
	isForbidden;
	findExact;
	nodeGetChild;
	nodeFindNode;
	hasForbiddenWords;
	hasCompoundWords;
	hasNonStrictWords;
	constructor(nodes, charIndex, maskInfo, methods) {
		this.nodes = nodes;
		this.charIndex = charIndex;
		const { NodeMaskEOW, NodeMaskChildCharIndex, NodeMaskNumChildren, NodeChildRefShift } = maskInfo;
		this.NodeMaskEOW = NodeMaskEOW;
		this.NodeMaskNumChildren = NodeMaskNumChildren;
		this.NodeMaskChildCharIndex = NodeMaskChildCharIndex;
		this.NodeChildRefShift = NodeChildRefShift;
		this.isIndexDecoderNeeded = charIndex.indexContainsMultiByteChars();
		this.nodeFindExact = methods.nodeFindExact;
		this.isForbidden = methods.isForbidden;
		this.findExact = methods.findExact;
		this.nodeGetChild = methods.nodeGetChild;
		this.nodeFindNode = methods.nodeFindNode;
		this.hasForbiddenWords = methods.hasForbiddenWords;
		this.hasCompoundWords = methods.hasCompoundWords;
		this.hasNonStrictWords = methods.hasNonStrictWords;
	}
};
const EmptyKeys = Object.freeze([]);
const EmptyNodes = Object.freeze([]);
const EmptyEntries = Object.freeze([]);
var TrieBlobINode = class TrieBlobINode {
	id;
	node;
	eow;
	_keys;
	_count;
	_size;
	_chained;
	_nodesEntries;
	_entries;
	_values;
	charToIdx;
	constructor(trie, nodeIdx) {
		this.trie = trie;
		this.nodeIdx = nodeIdx;
		const node = trie.nodes[nodeIdx];
		this.node = node;
		this.eow = !!(node & trie.NodeMaskEOW);
		this._count = node & trie.NodeMaskNumChildren;
		this.id = nodeIdx;
	}
	/** get keys to children */
	keys() {
		if (this._keys) return this._keys;
		if (!this._count) return EmptyKeys;
		this._keys = this.getNodesEntries().map(([key]) => key);
		return this._keys;
	}
	values() {
		if (!this._count) return EmptyNodes;
		if (this._values) return this._values;
		this._values = this.entries().map(([, value]) => value);
		return this._values;
	}
	entries() {
		if (this._entries) return this._entries;
		if (!this._count) return EmptyEntries;
		this._entries = this.getNodesEntries().map(([key, value]) => [key, new TrieBlobINode(this.trie, value)]);
		return this._entries;
	}
	/** get child ITrieNode */
	get(char) {
		return this.#getChildNode(char);
	}
	has(char) {
		return this.trie.nodeGetChild(this.nodeIdx, char) !== void 0;
	}
	hasChildren() {
		return this._count > 0;
	}
	child(keyIdx) {
		if (!this._values && !this.containsChainedIndexes()) {
			const nodeIdx = this.trie.nodes[this.nodeIdx + keyIdx + 1] >>> this.trie.NodeChildRefShift;
			return new TrieBlobINode(this.trie, nodeIdx);
		}
		return this.values()[keyIdx];
	}
	#getChildNodeIdx(char) {
		return this.trie.nodeGetChild(this.nodeIdx, char);
	}
	#getChildNode(char) {
		if (this.charToIdx) {
			const keyIdx = this.charToIdx[char];
			if (keyIdx === void 0) return void 0;
			return this.child(keyIdx);
		}
		const idx$1 = this.#getChildNodeIdx(char);
		if (idx$1 === void 0) return void 0;
		return new TrieBlobINode(this.trie, idx$1);
	}
	getCharToIdxMap() {
		const m = this.charToIdx;
		if (m) return m;
		const map = Object.create(null);
		const keys = this.keys();
		for (let i = 0; i < keys.length; ++i) map[keys[i]] = i;
		this.charToIdx = map;
		return map;
	}
	getNode(word) {
		const n = this.trie.nodeFindNode(this.nodeIdx, word);
		return n === void 0 ? void 0 : new TrieBlobINode(this.trie, n);
	}
	findExact(word) {
		return this.trie.nodeFindExact(this.nodeIdx, word);
	}
	containsChainedIndexes() {
		if (this._chained !== void 0) return this._chained;
		if (!this._count || !this.trie.isIndexDecoderNeeded) {
			this._chained = false;
			return false;
		}
		let found = false;
		const NodeMaskChildCharIndex = this.trie.NodeMaskChildCharIndex;
		const offset = this.nodeIdx + 1;
		const nodes = this.trie.nodes;
		const len = this._count;
		for (let i = 0; i < len && !found; ++i) {
			const charIdx = nodes[i + offset] & NodeMaskChildCharIndex;
			found = Utf8Accumulator.isMultiByte(charIdx);
		}
		this._chained = !!found;
		return this._chained;
	}
	getNodesEntries() {
		if (this._nodesEntries) return this._nodesEntries;
		if (!this.containsChainedIndexes()) {
			const entries = Array(this._count);
			const nodes = this.trie.nodes;
			const offset = this.nodeIdx + 1;
			const NodeMaskChildCharIndex = this.trie.NodeMaskChildCharIndex;
			const RefShift = this.trie.NodeChildRefShift;
			for (let i = 0; i < this._count; ++i) {
				const entry = nodes[offset + i];
				const codePoint = entry & NodeMaskChildCharIndex;
				entries[i] = [String.fromCodePoint(codePoint), entry >>> RefShift];
			}
			this._nodesEntries = entries;
			return entries;
		}
		this._nodesEntries = this.walkChainedIndexes();
		return this._nodesEntries;
	}
	walkChainedIndexes() {
		const NodeMaskChildCharIndex = this.trie.NodeMaskChildCharIndex;
		const NodeChildRefShift = this.trie.NodeChildRefShift;
		const NodeMaskNumChildren = this.trie.NodeMaskNumChildren;
		const nodes = this.trie.nodes;
		const acc = Utf8Accumulator.create();
		const stack = [{
			nodeIdx: this.nodeIdx + 1,
			lastIdx: this.nodeIdx + this._count,
			acc
		}];
		let depth = 0;
		const entries = Array(this._count);
		let eIdx = 0;
		while (depth >= 0) {
			const s = stack[depth];
			const { nodeIdx, lastIdx } = s;
			if (nodeIdx > lastIdx) {
				--depth;
				continue;
			}
			++s.nodeIdx;
			const entry = nodes[nodeIdx];
			const charIdx = entry & NodeMaskChildCharIndex;
			const acc$1 = s.acc.clone();
			const codePoint = acc$1.decode(charIdx);
			if (codePoint !== void 0) {
				const char = String.fromCodePoint(codePoint);
				const nodeIdx$1 = entry >>> NodeChildRefShift;
				entries[eIdx++] = [char, nodeIdx$1];
				continue;
			}
			const idx$1 = entry >>> NodeChildRefShift;
			const lIdx = idx$1 + (nodes[idx$1] & NodeMaskNumChildren);
			const ss = stack[++depth];
			if (ss) {
				ss.nodeIdx = idx$1 + 1;
				ss.lastIdx = lIdx;
				ss.acc = acc$1;
			} else stack[depth] = {
				nodeIdx: idx$1 + 1,
				lastIdx: lIdx,
				acc: acc$1
			};
		}
		return entries;
	}
	get size() {
		if (this._size === void 0) {
			if (!this.containsChainedIndexes()) {
				this._size = this._count;
				return this._size;
			}
			this._size = this.getNodesEntries().length;
		}
		return this._size;
	}
};
var TrieBlobIRoot = class extends TrieBlobINode {
	find;
	isForbidden;
	hasForbiddenWords;
	hasCompoundWords;
	hasNonStrictWords;
	constructor(trie, nodeIdx, info, methods) {
		super(trie, nodeIdx);
		this.info = info;
		this.find = methods.find;
		this.isForbidden = trie.isForbidden;
		this.hasForbiddenWords = trie.hasForbiddenWords;
		this.hasCompoundWords = trie.hasCompoundWords;
		this.hasNonStrictWords = trie.hasNonStrictWords;
	}
	resolveId(id) {
		return new TrieBlobINode(this.trie, id);
	}
	get forbidPrefix() {
		return this.info.forbiddenWordPrefix;
	}
	get compoundFix() {
		return this.info.compoundCharacter;
	}
	get caseInsensitivePrefix() {
		return this.info.stripCaseAndAccentsPrefix;
	}
};

//#endregion
//#region src/lib/TrieBlob/TrieBlob.ts
const NodeHeaderNumChildrenBits = 8;
const NodeHeaderNumChildrenShift = 0;
const HEADER_SIZE = 32;
const HEADER_OFFSET = 0;
const HEADER_OFFSET_SIG = HEADER_OFFSET;
const HEADER_OFFSET_ENDIAN = HEADER_OFFSET_SIG + 8;
const HEADER_OFFSET_VERSION = HEADER_OFFSET_ENDIAN + 4;
const HEADER_OFFSET_NODES = HEADER_OFFSET_VERSION + 4;
const HEADER_OFFSET_NODES_LEN = HEADER_OFFSET_NODES + 4;
const HEADER_OFFSET_CHAR_INDEX = HEADER_OFFSET_NODES_LEN + 4;
const HEADER = {
	header: HEADER_OFFSET,
	sig: HEADER_OFFSET_SIG,
	version: HEADER_OFFSET_VERSION,
	endian: HEADER_OFFSET_ENDIAN,
	nodes: HEADER_OFFSET_NODES,
	nodesLen: HEADER_OFFSET_NODES_LEN,
	charIndex: HEADER_OFFSET_CHAR_INDEX,
	charIndexLen: HEADER_OFFSET_CHAR_INDEX + 4
};
const headerSig = "TrieBlob";
const version = "00.01.00";
const endianSig = 67305985;
var TrieBlob = class TrieBlob {
	info;
	#forbidIdx;
	#compoundIdx;
	#nonStrictIdx;
	#size;
	#iTrieRoot;
	/** the nodes data in 8 bits */
	#nodes8;
	#beAdj = endianness() === "BE" ? 3 : 0;
	wordToCharacters = (word) => [...word];
	hasForbiddenWords;
	hasCompoundWords;
	hasNonStrictWords;
	constructor(nodes, charIndex, info) {
		this.nodes = nodes;
		this.charIndex = charIndex;
		trieBlobSort(nodes);
		this.info = mergeOptionalWithDefaults(info);
		this.#nodes8 = new Uint8Array(nodes.buffer, nodes.byteOffset + this.#beAdj);
		this.#forbidIdx = this._lookupNode(0, this.info.forbiddenWordPrefix);
		this.#compoundIdx = this._lookupNode(0, this.info.compoundCharacter);
		this.#nonStrictIdx = this._lookupNode(0, this.info.stripCaseAndAccentsPrefix);
		this.hasForbiddenWords = !!this.#forbidIdx;
		this.hasCompoundWords = !!this.#compoundIdx;
		this.hasNonStrictWords = !!this.#nonStrictIdx;
	}
	wordToUtf8Seq(word) {
		return this.charIndex.wordToUtf8Seq(word);
	}
	letterToNodeCharIndexSequence(letter) {
		return this.charIndex.getCharUtf8Seq(letter);
	}
	has(word) {
		return this.#hasWord(0, word);
	}
	isForbiddenWord(word) {
		return !!this.#forbidIdx && this.#hasWord(this.#forbidIdx, word);
	}
	/**
	* Try to find the word in the trie. The word must be normalized.
	* If `strict` is `true` the case and accents must match.
	* Compound words are supported assuming that the compound character is in the trie.
	*
	* @param word - the word to find (normalized)
	* @param strict - if `true` the case and accents must match.
	*/
	find(word, strict) {
		if (!this.hasCompoundWords) {
			if (this.#hasWord(0, word)) return {
				found: word,
				compoundUsed: false,
				caseMatched: true
			};
			if (strict || !this.#nonStrictIdx) return {
				found: false,
				compoundUsed: false,
				caseMatched: false
			};
			return {
				found: this.#hasWord(this.#nonStrictIdx, word) && word,
				compoundUsed: false,
				caseMatched: false
			};
		}
	}
	getRoot() {
		return this.#iTrieRoot ??= this._getRoot();
	}
	_getRoot() {
		return new TrieBlobIRoot(new TrieBlobInternals(this.nodes, this.charIndex, {
			NodeMaskEOW: TrieBlob.NodeMaskEOW,
			NodeMaskNumChildren: TrieBlob.NodeMaskNumChildren,
			NodeMaskChildCharIndex: TrieBlob.NodeMaskChildCharIndex,
			NodeChildRefShift: TrieBlob.NodeChildRefShift
		}, {
			nodeFindExact: (idx$1, word) => this.#hasWord(idx$1, word),
			nodeGetChild: (idx$1, letter) => this._lookupNode(idx$1, letter),
			nodeFindNode: (idx$1, word) => this.#findNode(idx$1, word),
			isForbidden: (word) => this.isForbiddenWord(word),
			findExact: (word) => this.has(word),
			hasCompoundWords: this.hasCompoundWords,
			hasForbiddenWords: this.hasForbiddenWords,
			hasNonStrictWords: this.hasNonStrictWords
		}), 0, this.info, { find: (word, strict) => this.find(word, strict) });
	}
	getNode(prefix) {
		return findNode$1(this.getRoot(), prefix);
	}
	/**
	* Check if the word is in the trie starting at the given node index.
	*/
	#hasWord(nodeIdx, word) {
		const nodeIdxFound = this.#findNode(nodeIdx, word);
		if (!nodeIdxFound) return false;
		const node = this.nodes[nodeIdxFound];
		const m = TrieBlob.NodeMaskEOW;
		return (node & m) === m;
	}
	#findNode(nodeIdx, word) {
		const wordIndexes = this.wordToUtf8Seq(word);
		return this.#lookupNode(nodeIdx, wordIndexes);
	}
	/**
	* Find the node index for the given Utf8 character sequence.
	* @param nodeIdx - node index to start the search
	* @param seq - the byte sequence of the character to look for
	* @returns
	*/
	#lookupNode(nodeIdx, seq) {
		const NodeMaskNumChildren = TrieBlob.NodeMaskNumChildren;
		const NodeChildRefShift = TrieBlob.NodeChildRefShift;
		const nodes = this.nodes;
		const nodes8 = this.#nodes8;
		const wordIndexes = seq;
		const len = wordIndexes.length;
		let node = nodes[nodeIdx];
		for (let p = 0; p < len; ++p, node = nodes[nodeIdx]) {
			const letterIdx = wordIndexes[p];
			const count = node & NodeMaskNumChildren;
			const idx4 = nodeIdx << 2;
			if (count > 15) {
				const pEnd = idx4 + (count << 2);
				let i$1 = idx4 + 4;
				let j = pEnd;
				while (j - i$1 >= 4) {
					const m = i$1 + j >> 1 & -4;
					if (nodes8[m] < letterIdx) i$1 = m + 4;
					else j = m;
				}
				if (i$1 > pEnd || nodes8[i$1] !== letterIdx) return void 0;
				nodeIdx = nodes[i$1 >> 2] >>> NodeChildRefShift;
				continue;
			}
			let i = idx4 + count * 4;
			for (; i > idx4; i -= 4) if (nodes8[i] === letterIdx) break;
			if (i <= idx4) return void 0;
			nodeIdx = nodes[i >> 2] >>> NodeChildRefShift;
		}
		return nodeIdx;
	}
	/**
	* Find the node index for the given character.
	* @param nodeIdx - node index to start the search
	* @param char - character to look for
	* @returns
	*/
	_lookupNode(nodeIdx, char) {
		const indexSeq = this.letterToNodeCharIndexSequence(char);
		return this.#lookupNode(nodeIdx, indexSeq);
	}
	*words() {
		const NodeMaskNumChildren = TrieBlob.NodeMaskNumChildren;
		const NodeMaskEOW = TrieBlob.NodeMaskEOW;
		const NodeMaskChildCharIndex = TrieBlob.NodeMaskChildCharIndex;
		const NodeChildRefShift = TrieBlob.NodeChildRefShift;
		const nodes = this.nodes;
		const stack = [{
			nodeIdx: 0,
			pos: 0,
			word: "",
			acc: Utf8Accumulator.create()
		}];
		let depth = 0;
		while (depth >= 0) {
			const { nodeIdx, pos, word, acc } = stack[depth];
			const node = nodes[nodeIdx];
			if (!pos && node & NodeMaskEOW) yield word;
			if (pos >= (node & NodeMaskNumChildren)) {
				--depth;
				continue;
			}
			const entry = nodes[nodeIdx + ++stack[depth].pos];
			const nAcc = acc.clone();
			const codePoint = nAcc.decode(entry & NodeMaskChildCharIndex);
			const letter = codePoint && String.fromCodePoint(codePoint) || "";
			++depth;
			stack[depth] = {
				nodeIdx: entry >>> NodeChildRefShift,
				pos: 0,
				word: word + letter,
				acc: nAcc
			};
		}
	}
	get size() {
		if (this.#size) return this.#size;
		const NodeMaskNumChildren = TrieBlob.NodeMaskNumChildren;
		const nodes = this.nodes;
		let p = 0;
		let count = 0;
		while (p < nodes.length) {
			++count;
			p += (nodes[p] & NodeMaskNumChildren) + 1;
		}
		this.#size = count;
		return count;
	}
	toJSON() {
		return {
			options: this.info,
			nodes: nodesToJson(this.nodes),
			charIndex: this.charIndex
		};
	}
	encodeBin() {
		const charIndex = Buffer.from(this.charIndex.charIndex.join("\n"));
		const nodeOffset = HEADER_SIZE + (charIndex.byteLength + 3 & -4);
		const size = nodeOffset + this.nodes.length * 4;
		const useLittle = isLittleEndian();
		const buffer$1 = Buffer.alloc(size);
		const header = new DataView(buffer$1.buffer);
		const nodeData = new Uint8Array(this.nodes.buffer);
		buffer$1.write(headerSig, HEADER.sig, "utf8");
		buffer$1.write(version, HEADER.version, "utf8");
		header.setUint32(HEADER.endian, endianSig, useLittle);
		header.setUint32(HEADER.nodes, nodeOffset, useLittle);
		header.setUint32(HEADER.nodesLen, this.nodes.length, useLittle);
		header.setUint32(HEADER.charIndex, HEADER_SIZE, useLittle);
		header.setUint32(HEADER.charIndexLen, charIndex.length, useLittle);
		buffer$1.set(charIndex, HEADER_SIZE);
		buffer$1.set(nodeData, nodeOffset);
		return buffer$1;
	}
	static decodeBin(blob) {
		if (!checkSig(blob)) throw new ErrorDecodeTrieBlob("Invalid TrieBlob Header");
		const header = new DataView(blob.buffer);
		const useLittle = isLittleEndian();
		if (header.getUint32(HEADER.endian, useLittle) !== endianSig) throw new ErrorDecodeTrieBlob("Invalid TrieBlob Header");
		const offsetNodes = header.getUint32(HEADER.nodes, useLittle);
		const lenNodes = header.getUint32(HEADER.nodesLen, useLittle);
		const offsetCharIndex = header.getUint32(HEADER.charIndex, useLittle);
		const lenCharIndex = header.getUint32(HEADER.charIndexLen, useLittle);
		const charIndex = Buffer.from(blob.subarray(offsetCharIndex, offsetCharIndex + lenCharIndex)).toString("utf8").split("\n");
		return new TrieBlob(new Uint32Array(blob.buffer, offsetNodes, lenNodes), new CharIndex(charIndex), defaultTrieInfo);
	}
	static NodeMaskEOW = 256;
	static NodeMaskNumChildren = (1 << NodeHeaderNumChildrenBits) - 1 & 65535;
	static NodeMaskNumChildrenShift = NodeHeaderNumChildrenShift;
	static NodeChildRefShift = 8;
	/**
	* Only 8 bits are reserved for the character index.
	* The max index is {@link TrieBlob.SpecialCharIndexMask} - 1.
	* Node chaining is used to reference higher character indexes.
	* - @see {@link TrieBlob.SpecialCharIndexMask}
	* - @see {@link TrieBlob.MaxCharIndex}
	*/
	static NodeMaskChildCharIndex = 255;
	static nodesView(trie) {
		return new Uint32Array(trie.nodes);
	}
};
function isLittleEndian() {
	const buf = new Uint8Array([
		1,
		2,
		3,
		4
	]);
	return new DataView(buf.buffer).getUint32(0, true) === 67305985;
}
function checkSig(blob) {
	if (blob.length < HEADER_SIZE) return false;
	if (Buffer.from(blob, 0, 8).toString("utf8", 0, 8) !== headerSig) return false;
	return true;
}
var ErrorDecodeTrieBlob = class extends Error {
	constructor(message) {
		super(message);
	}
};
function nodesToJson(nodes) {
	function nodeElement(offset$1) {
		const node = nodes[offset$1];
		const numChildren = node & TrieBlob.NodeMaskNumChildren;
		const eow = !!(node & TrieBlob.NodeMaskEOW);
		const children = [];
		for (let i = 1; i <= numChildren; ++i) children.push({
			c: ("00" + (nodes[offset$1 + i] & TrieBlob.NodeMaskChildCharIndex).toString(16)).slice(-2),
			o: nodes[offset$1 + i] >>> TrieBlob.NodeChildRefShift
		});
		return {
			id: offset$1,
			eow,
			n: offset$1 + numChildren + 1,
			c: children
		};
	}
	const elements = [];
	let offset = 0;
	while (offset < nodes.length) {
		const e = nodeElement(offset);
		elements.push(e);
		offset = e.n;
	}
	return elements;
}
/**
* Sorts the child nodes in the trie to ensure binary lookup works.
* @param data
*/
function trieBlobSort(data) {
	const MaskNumChildren = TrieBlob.NodeMaskNumChildren;
	const MaskChildCharIndex = TrieBlob.NodeMaskChildCharIndex;
	const limit = data.length;
	let idx$1 = 0;
	let node = data[0];
	let nc = node & MaskNumChildren;
	for (; idx$1 < limit; idx$1 += nc + 1, node = data[idx$1], nc = node & MaskNumChildren) {
		if (!nc) continue;
		const start = idx$1 + 1;
		const end = start + nc;
		let last = 0;
		let i = start;
		for (; i < end; ++i) {
			const cIdx = data[i] & MaskChildCharIndex;
			if (last >= cIdx) break;
			last = cIdx;
		}
		if (i === end) continue;
		data.slice(start, end).sort((a, b) => (a & MaskChildCharIndex) - (b & MaskChildCharIndex)).forEach((v, i$1) => data[start + i$1] = v);
	}
}

//#endregion
//#region src/lib/TrieBlob/FastTrieBlob.ts
var FastTrieBlob = class FastTrieBlob {
	_readonly = false;
	#forbidIdx;
	#compoundIdx;
	#nonStrictIdx;
	_iTrieRoot;
	wordToCharacters;
	hasForbiddenWords;
	hasCompoundWords;
	hasNonStrictWords;
	constructor(nodes, _charIndex, bitMasksInfo, info) {
		this.nodes = nodes;
		this._charIndex = _charIndex;
		this.bitMasksInfo = bitMasksInfo;
		this.info = info;
		this.wordToCharacters = (word) => [...word];
		this.#forbidIdx = this.#searchNodeForChar(0, this.info.forbiddenWordPrefix) || 0;
		this.#compoundIdx = this.#searchNodeForChar(0, this.info.compoundCharacter) || 0;
		this.#nonStrictIdx = this.#searchNodeForChar(0, this.info.stripCaseAndAccentsPrefix) || 0;
		this.hasForbiddenWords = !!this.#forbidIdx;
		this.hasCompoundWords = !!this.#compoundIdx;
		this.hasNonStrictWords = !!this.#nonStrictIdx;
	}
	wordToUtf8Seq(word) {
		return this._charIndex.wordToUtf8Seq(word);
	}
	letterToUtf8Seq(letter) {
		return this._charIndex.getCharUtf8Seq(letter);
	}
	has(word) {
		return this.#has(0, word);
	}
	hasCaseInsensitive(word) {
		if (!this.#nonStrictIdx) return false;
		return this.#has(this.#nonStrictIdx, word);
	}
	#has(nodeIdx, word) {
		return this.#hasSorted(nodeIdx, word);
	}
	#hasSorted(nodeIdx, word) {
		const charIndexes = this.wordToUtf8Seq(word);
		const found = this.#lookupNode(nodeIdx, charIndexes);
		if (found === void 0) return false;
		return !!(this.nodes[found][0] & this.bitMasksInfo.NodeMaskEOW);
	}
	/**
	* Find the node index for the given Utf8 character sequence.
	* @param nodeIdx - node index to start the search
	* @param seq - the byte sequence of the character to look for
	* @returns
	*/
	#lookupNode(nodeIdx, seq) {
		const NodeMaskChildCharIndex = this.bitMasksInfo.NodeMaskChildCharIndex;
		const NodeChildRefShift = this.bitMasksInfo.NodeChildRefShift;
		const nodes = this.nodes;
		const len = seq.length;
		let node = nodes[nodeIdx];
		for (let p = 0; p < len; ++p, node = nodes[nodeIdx]) {
			const letterIdx = seq[p];
			const count = node.length;
			if (count < 2) return void 0;
			let i = 1;
			let j = count - 1;
			let c = -1;
			while (i < j) {
				const m = i + j >> 1;
				c = node[m] & NodeMaskChildCharIndex;
				if (c < letterIdx) i = m + 1;
				else j = m;
			}
			if (i >= count || (node[i] & NodeMaskChildCharIndex) !== letterIdx) return void 0;
			nodeIdx = node[i] >>> NodeChildRefShift;
			if (!nodeIdx) return void 0;
		}
		return nodeIdx;
	}
	*words() {
		const NodeMaskChildCharIndex = this.bitMasksInfo.NodeMaskChildCharIndex;
		const NodeChildRefShift = this.bitMasksInfo.NodeChildRefShift;
		const NodeMaskEOW = this.bitMasksInfo.NodeMaskEOW;
		const nodes = this.nodes;
		const stack = [{
			nodeIdx: 0,
			pos: 0,
			word: "",
			accumulator: Utf8Accumulator.create()
		}];
		let depth = 0;
		while (depth >= 0) {
			const { nodeIdx, pos, word, accumulator } = stack[depth];
			const node = nodes[nodeIdx];
			if (!pos && node[0] & NodeMaskEOW) yield word;
			if (pos >= node.length - 1) {
				--depth;
				continue;
			}
			const entry = node[++stack[depth].pos];
			const charIdx = entry & NodeMaskChildCharIndex;
			const acc = accumulator.clone();
			const codePoint = acc.decode(charIdx);
			const letter = codePoint && String.fromCodePoint(codePoint) || "";
			++depth;
			stack[depth] = {
				nodeIdx: entry >>> NodeChildRefShift,
				pos: 0,
				word: word + letter,
				accumulator: acc
			};
		}
	}
	toTrieBlob() {
		const NodeMaskChildCharIndex = this.bitMasksInfo.NodeMaskChildCharIndex;
		const NodeChildRefShift = this.bitMasksInfo.NodeChildRefShift;
		const nodes = this.nodes;
		function calcNodeToIndex(nodes$1) {
			let offset$1 = 0;
			const idx$1 = Array(nodes$1.length + 1);
			for (let i = 0; i < nodes$1.length; ++i) {
				idx$1[i] = offset$1;
				offset$1 += nodes$1[i].length;
			}
			idx$1[nodes$1.length] = offset$1;
			return idx$1;
		}
		const nodeToIndex = calcNodeToIndex(nodes);
		const nodeElementCount = nodeToIndex[nodeToIndex.length - 1];
		const binNodes = new Uint32Array(nodeElementCount);
		const lenShift = TrieBlob.NodeMaskNumChildrenShift;
		const refShift = TrieBlob.NodeChildRefShift;
		let offset = 0;
		for (let i = 0; i < nodes.length; ++i) {
			const node = nodes[i];
			binNodes[offset++] = node.length - 1 << lenShift | node[0];
			for (let j = 1; j < node.length; ++j) {
				const v = node[j];
				const nodeRef = v >>> NodeChildRefShift;
				const charIndex = v & NodeMaskChildCharIndex;
				binNodes[offset++] = nodeToIndex[nodeRef] << refShift | charIndex;
			}
		}
		return new TrieBlob(binNodes, this._charIndex, this.info);
	}
	isReadonly() {
		return this._readonly;
	}
	freeze() {
		this._readonly = true;
		return this;
	}
	toJSON() {
		return {
			info: this.info,
			nodes: nodesToJSON(this.nodes)
		};
	}
	static create(data) {
		return new FastTrieBlob(data.nodes, data.charIndex, extractInfo(data), data.info);
	}
	static toITrieNodeRoot(trie) {
		return new FastTrieBlobIRoot(new FastTrieBlobInternalsAndMethods(trie.nodes, trie._charIndex, trie.bitMasksInfo, trie.info, {
			nodeFindNode: (idx$1, word) => trie.#lookupNode(idx$1, trie.wordToUtf8Seq(word)),
			nodeFindExact: (idx$1, word) => trie.#has(idx$1, word),
			nodeGetChild: (idx$1, letter) => trie.#searchNodeForChar(idx$1, letter),
			isForbidden: (word) => trie.isForbiddenWord(word),
			findExact: (word) => trie.has(word),
			hasForbiddenWords: trie.hasForbiddenWords,
			hasCompoundWords: trie.hasCompoundWords,
			hasNonStrictWords: trie.hasNonStrictWords
		}), 0);
	}
	static NodeMaskEOW = TrieBlob.NodeMaskEOW;
	static NodeChildRefShift = TrieBlob.NodeChildRefShift;
	static NodeMaskChildCharIndex = TrieBlob.NodeMaskChildCharIndex;
	static DefaultBitMaskInfo = {
		NodeMaskEOW: FastTrieBlob.NodeMaskEOW,
		NodeMaskChildCharIndex: FastTrieBlob.NodeMaskChildCharIndex,
		NodeChildRefShift: FastTrieBlob.NodeChildRefShift
	};
	get iTrieRoot() {
		return this._iTrieRoot ??= FastTrieBlob.toITrieNodeRoot(this);
	}
	getRoot() {
		return this.iTrieRoot;
	}
	getNode(prefix) {
		return findNode$1(this.getRoot(), prefix);
	}
	isForbiddenWord(word) {
		return !!this.#forbidIdx && this.#has(this.#forbidIdx, word);
	}
	nodeInfo(nodeIndex, accumulator) {
		const acc = accumulator ?? Utf8Accumulator.create();
		const n = this.nodes[nodeIndex];
		const eow = !!(n[0] & this.bitMasksInfo.NodeMaskEOW);
		const children = [];
		children.length = n.length - 1;
		for (let p = 1; p < n.length; ++p) {
			const v = n[p];
			const cIdx = v & this.bitMasksInfo.NodeMaskChildCharIndex;
			const codePoint = acc.clone().decode(cIdx);
			children[p] = {
				c: codePoint !== void 0 ? String.fromCodePoint(codePoint) : "∎",
				i: v >>> this.bitMasksInfo.NodeChildRefShift,
				cIdx
			};
		}
		return {
			eow,
			children
		};
	}
	/** number of nodes */
	get size() {
		return this.nodes.length;
	}
	/** Search from nodeIdx for the node index representing the character. */
	#searchNodeForChar(nodeIdx, char) {
		const charIndexes = this.letterToUtf8Seq(char);
		return this.#lookupNode(nodeIdx, charIndexes);
	}
	get charIndex() {
		return [...this._charIndex.charIndex];
	}
	static fromTrieBlob(trie) {
		const bitMasksInfo = {
			NodeMaskEOW: TrieBlob.NodeMaskEOW,
			NodeMaskChildCharIndex: TrieBlob.NodeMaskChildCharIndex,
			NodeChildRefShift: TrieBlob.NodeChildRefShift
		};
		const trieNodesBin = TrieBlob.nodesView(trie);
		const nodeOffsets = [];
		for (let offset = 0; offset < trieNodesBin.length; offset += (trieNodesBin[offset] & TrieBlob.NodeMaskNumChildren) + 1) nodeOffsets.push(offset);
		const offsetToNodeIndex = new Map(nodeOffsets.map((offset, i) => [offset, i]));
		const nodes = Array.from({ length: nodeOffsets.length });
		for (let i = 0; i < nodes.length; ++i) {
			const offset = nodeOffsets[i];
			const n = trieNodesBin[offset];
			const eow = n & TrieBlob.NodeMaskEOW;
			const count = n & TrieBlob.NodeMaskNumChildren;
			const node = new Uint32Array(count + 1);
			node[0] = eow;
			nodes[i] = node;
			for (let j = 1; j <= count; ++j) {
				const n$1 = trieNodesBin[offset + j];
				const charIndex = n$1 & TrieBlob.NodeMaskChildCharIndex;
				const nodeIndex = n$1 >>> TrieBlob.NodeChildRefShift;
				const idx$1 = offsetToNodeIndex.get(nodeIndex);
				if (idx$1 === void 0) throw new Error(`Invalid node index ${nodeIndex}`);
				node[j] = idx$1 << TrieBlob.NodeChildRefShift | charIndex;
			}
		}
		return new FastTrieBlob(sortNodes(nodes, TrieBlob.NodeMaskChildCharIndex), trie.charIndex, bitMasksInfo, trie.info);
	}
	static isFastTrieBlob(obj) {
		return obj instanceof FastTrieBlob;
	}
};
function nodesToJSON(nodes) {
	const mapNodeToAcc = /* @__PURE__ */ new Map();
	function mapNode(node, i) {
		if (node.length === 1) return {
			i,
			w: !!(node[0] & TrieBlob.NodeMaskEOW) && 1 || 0
		};
		const acc = mapNodeToAcc.get(node) || Utf8Accumulator.create();
		function mapChild(n) {
			const index = n >>> TrieBlob.NodeChildRefShift;
			const seq = n & TrieBlob.NodeMaskChildCharIndex;
			const cAcc = acc.clone();
			const codePoint = cAcc.decode(seq);
			if (codePoint === void 0) mapNodeToAcc.set(nodes[index], cAcc);
			return {
				i: index,
				c: codePoint && String.fromCodePoint(codePoint) || void 0,
				s: seq.toString(16).padStart(2, "0")
			};
		}
		return {
			i,
			w: !!(node[0] & TrieBlob.NodeMaskEOW) && 1 || 0,
			c: [...node.slice(1)].map(mapChild)
		};
	}
	return nodes.map((n, i) => mapNode(n, i));
}

//#endregion
//#region src/lib/TrieBlob/resolveMap.ts
function resolveMap(map, key, resolve) {
	const r = map.get(key);
	if (r !== void 0) return r;
	const v = resolve(key);
	map.set(key, v);
	return v;
}

//#endregion
//#region src/lib/TrieBlob/FastTrieBlobBuilder.ts
var FastTrieBlobBuilder = class FastTrieBlobBuilder {
	charIndex = new CharIndexBuilder();
	nodes;
	_readonly = false;
	IdxEOW;
	_cursor;
	_options;
	wordToCharacters = (word) => [...word];
	bitMasksInfo;
	constructor(options, bitMasksInfo = FastTrieBlobBuilder.DefaultBitMaskInfo) {
		this._options = mergeOptionalWithDefaults(options);
		this.bitMasksInfo = bitMasksInfo;
		this.nodes = [[0], Object.freeze([FastTrieBlobBuilder.NodeMaskEOW])];
		this.IdxEOW = 1;
	}
	setOptions(options) {
		this._options = mergeOptionalWithDefaults(this.options, options);
		return this.options;
	}
	get options() {
		return this._options;
	}
	wordToUtf8Seq(word) {
		return this.charIndex.wordToUtf8Seq(word);
	}
	letterToUtf8Seq(letter) {
		return this.charIndex.charToUtf8Seq(letter);
	}
	insert(word) {
		this.#assertNotReadonly();
		if (typeof word === "string") return this._insert(word);
		const words = word;
		for (const w of words) this._insert(w);
		return this;
	}
	getCursor() {
		this.#assertNotReadonly();
		this._cursor ??= this.createCursor();
		return this._cursor;
	}
	createCursor() {
		const NodeChildRefShift = this.bitMasksInfo.NodeChildRefShift;
		const NodeMaskEOW = this.bitMasksInfo.NodeMaskEOW;
		const LetterMask = this.bitMasksInfo.NodeMaskChildCharIndex;
		const refNodes = [0, 1];
		function childPos(node, letterIdx) {
			for (let i = 1; i < node.length; ++i) if ((node[i] & LetterMask) === letterIdx) return i;
			return 0;
		}
		assert(this.nodes.length === 2);
		const eow = 1;
		const eowShifted = eow << NodeChildRefShift;
		const nodes = this.nodes;
		const stack = [{
			nodeIdx: 0,
			pos: 0,
			pDepth: -1
		}];
		let nodeIdx = 0;
		let depth = 0;
		const insertChar = (char) => {
			if (!nodes[nodeIdx]) refNodes.push(nodeIdx);
			const pDepth = depth;
			const utf8Seq = this.letterToUtf8Seq(char);
			for (let i = 0; i < utf8Seq.length; ++i) insertCharIndexes(utf8Seq[i], pDepth);
		};
		/**
		* A single character can result in multiple nodes being created
		* because it takes multiple bytes to represent a character.
		* @param seq - partial character index.
		*/
		const insertCharIndexes = (seq, pDepth) => {
			if (nodes[nodeIdx] && Object.isFrozen(nodes[nodeIdx])) {
				nodeIdx = nodes.push([...nodes[nodeIdx]]) - 1;
				const { pos: pos$1, nodeIdx: pNodeIdx } = stack[depth];
				const pNode = nodes[pNodeIdx];
				pNode[pos$1] = pNode[pos$1] & LetterMask | nodeIdx << NodeChildRefShift;
			}
			const node = nodes[nodeIdx] || [0];
			nodes[nodeIdx] = node;
			const hasIdx = childPos(node, seq);
			const childIdx = hasIdx ? node[hasIdx] >>> NodeChildRefShift : nodes.length;
			const pos = hasIdx || node.push(childIdx << NodeChildRefShift | seq) - 1;
			++depth;
			const s = stack[depth];
			if (s) {
				s.nodeIdx = nodeIdx;
				s.pos = pos;
				s.pDepth = pDepth;
			} else stack[depth] = {
				nodeIdx,
				pos,
				pDepth
			};
			nodeIdx = childIdx;
		};
		const markEOW = () => {
			if (nodeIdx === eow) return;
			const node = nodes[nodeIdx];
			if (!node) {
				const { pos, nodeIdx: pNodeIdx } = stack[depth];
				const pNode = nodes[pNodeIdx];
				pNode[pos] = pNode[pos] & LetterMask | eowShifted;
			} else {
				nodes[nodeIdx] = node;
				node[0] |= NodeMaskEOW;
			}
			nodeIdx = eow;
		};
		const reference = (refId) => {
			const refNodeIdx = refNodes[refId];
			assert(refNodeIdx !== void 0);
			assert(nodes[nodeIdx] === void 0);
			assert(nodes[refNodeIdx]);
			Object.freeze(nodes[refNodeIdx]);
			const s = stack[depth];
			nodeIdx = s.nodeIdx;
			const pos = s.pos;
			const node = nodes[nodeIdx];
			node[pos] = refNodeIdx << NodeChildRefShift | node[pos] & LetterMask;
		};
		const backStep = (num) => {
			if (!num) return;
			assert(num <= depth && num > 0);
			for (let n = num; n > 0; --n) depth = stack[depth].pDepth;
			nodeIdx = stack[depth + 1].nodeIdx;
		};
		return {
			insertChar,
			markEOW,
			reference,
			backStep
		};
	}
	_insert(word) {
		word = word.trim();
		if (!word) return this;
		const NodeMaskChildCharIndex = this.bitMasksInfo.NodeMaskChildCharIndex;
		const NodeChildRefShift = this.bitMasksInfo.NodeChildRefShift;
		const NodeMaskEOW = this.bitMasksInfo.NodeMaskEOW;
		const IdxEOW = this.IdxEOW;
		const nodes = this.nodes;
		const utf8Seq = this.wordToUtf8Seq(word);
		const len = utf8Seq.length;
		let nodeIdx = 0;
		for (let p = 0; p < len; ++p) {
			const seq = utf8Seq[p];
			const node = nodes[nodeIdx];
			let i = node.length - 1;
			for (; i > 0; --i) if ((node[i] & NodeMaskChildCharIndex) === seq) break;
			if (i > 0) {
				nodeIdx = node[i] >>> NodeChildRefShift;
				if (nodeIdx === 1 && p < len - 1) {
					nodeIdx = this.nodes.push([NodeMaskEOW]) - 1;
					node[i] = nodeIdx << NodeChildRefShift | seq;
				}
				continue;
			}
			nodeIdx = p < len - 1 ? this.nodes.push([0]) - 1 : IdxEOW;
			node.push(nodeIdx << NodeChildRefShift | seq);
		}
		if (nodeIdx > 1) {
			const node = nodes[nodeIdx];
			node[0] |= NodeMaskEOW;
		}
		return this;
	}
	has(word) {
		const NodeMaskChildCharIndex = this.bitMasksInfo.NodeMaskChildCharIndex;
		const NodeChildRefShift = this.bitMasksInfo.NodeChildRefShift;
		const NodeMaskEOW = this.bitMasksInfo.NodeMaskEOW;
		const nodes = this.nodes;
		const charIndexes = this.wordToUtf8Seq(word);
		const len = charIndexes.length;
		let nodeIdx = 0;
		let node = nodes[nodeIdx];
		for (let p = 0; p < len; ++p, node = nodes[nodeIdx]) {
			const letterIdx = charIndexes[p];
			let i = node.length - 1;
			for (; i > 0; --i) if ((node[i] & NodeMaskChildCharIndex) === letterIdx) break;
			if (i < 1) return false;
			nodeIdx = node[i] >>> NodeChildRefShift;
		}
		return !!(node[0] & NodeMaskEOW);
	}
	isReadonly() {
		return this._readonly;
	}
	freeze() {
		this._readonly = true;
		return this;
	}
	build() {
		this._cursor = void 0;
		this._readonly = true;
		this.freeze();
		return FastTrieBlob.create(new FastTrieBlobInternals(sortNodes(this.nodes.map((n) => Uint32Array.from(n)), this.bitMasksInfo.NodeMaskChildCharIndex), this.charIndex.build(), this.bitMasksInfo, this.options));
	}
	toJSON() {
		return {
			options: this.options,
			nodes: nodesToJSON(this.nodes.map((n) => Uint32Array.from(n)))
		};
	}
	#assertNotReadonly() {
		assert(!this.isReadonly(), "FastTrieBlobBuilder is readonly");
	}
	static fromWordList(words, options) {
		return new FastTrieBlobBuilder(options).insert(words).build();
	}
	static fromTrieRoot(root) {
		const bitMasksInfo = FastTrieBlobBuilder.DefaultBitMaskInfo;
		const NodeChildRefShift = bitMasksInfo.NodeChildRefShift;
		const NodeCharIndexMask = bitMasksInfo.NodeMaskChildCharIndex;
		const NodeMaskEOW = bitMasksInfo.NodeMaskEOW;
		const tf = new FastTrieBlobBuilder(void 0, bitMasksInfo);
		const IdxEOW = tf.IdxEOW;
		const known = new Map([[root, 0]]);
		function resolveNode(n) {
			if (n.f && !n.c) return IdxEOW;
			const node = [n.f ? NodeMaskEOW : 0];
			return tf.nodes.push(node) - 1;
		}
		function walk$3(n) {
			const found = known.get(n);
			if (found) return found;
			const nodeIdx = resolveMap(known, n, resolveNode);
			const node = tf.nodes[nodeIdx];
			if (!n.c) return nodeIdx;
			const children = Object.entries(n.c);
			for (let p = 0; p < children.length; ++p) {
				const [char, childNode] = children[p];
				addCharToNode(node, char, childNode);
			}
			return nodeIdx;
		}
		function resolveChild(node, charIndex) {
			let i = 1;
			for (i = 1; i < node.length && (node[i] & NodeCharIndexMask) !== charIndex; ++i);
			return i;
		}
		function addCharToNode(node, char, n) {
			const indexSeq = tf.letterToUtf8Seq(char);
			assertValidUtf16Character(char);
			for (const idx$1 of indexSeq.slice(0, -1)) {
				const pos = resolveChild(node, idx$1);
				if (pos < node.length) node = tf.nodes[node[pos] >>> NodeChildRefShift];
				else {
					const next = [0];
					node[pos] = tf.nodes.push(next) - 1 << NodeChildRefShift | idx$1;
					node = next;
				}
			}
			const letterIdx = indexSeq[indexSeq.length - 1];
			const i = node.push(letterIdx) - 1;
			node[i] = walk$3(n) << NodeChildRefShift | letterIdx;
		}
		walk$3(root);
		return tf.build();
	}
	static NodeMaskEOW = TrieBlob.NodeMaskEOW;
	static NodeChildRefShift = TrieBlob.NodeChildRefShift;
	static NodeMaskChildCharIndex = TrieBlob.NodeMaskChildCharIndex;
	static DefaultBitMaskInfo = {
		NodeMaskEOW: FastTrieBlobBuilder.NodeMaskEOW,
		NodeMaskChildCharIndex: FastTrieBlobBuilder.NodeMaskChildCharIndex,
		NodeChildRefShift: FastTrieBlobBuilder.NodeChildRefShift
	};
};

//#endregion
//#region src/lib/utils/clean.ts
function clean(t) {
	const copy = { ...t };
	for (const key of Object.keys(copy)) if (copy[key] === void 0) delete copy[key];
	return copy;
}

//#endregion
//#region src/lib/ITrie.ts
const defaultLegacyMinCompoundLength$1 = 3;
var ITrieImpl = class ITrieImpl {
	_info;
	root;
	count;
	weightMap;
	#optionsCompound = this.createFindOptions({ compoundMode: "compound" });
	hasForbiddenWords;
	hasCompoundWords;
	hasNonStrictWords;
	constructor(data, numNodes) {
		this.data = data;
		this.numNodes = numNodes;
		this.root = data.getRoot();
		this._info = mergeOptionalWithDefaults(data.info);
		this.hasForbiddenWords = data.hasForbiddenWords;
		this.hasCompoundWords = data.hasCompoundWords;
		this.hasNonStrictWords = data.hasNonStrictWords;
	}
	/**
	* Number of words in the Trie, the first call to this method might be expensive.
	* Use `size` to get the number of nodes.
	*/
	numWords() {
		this.count ??= countWords$1(this.root);
		return this.count;
	}
	isNumWordsKnown() {
		return this.count !== void 0;
	}
	get size() {
		return this.data.size;
	}
	get info() {
		return this._info;
	}
	get isCaseAware() {
		return this.info.isCaseAware ?? true;
	}
	/**
	* @param text - text to find in the Trie
	*/
	find(text) {
		return findWordNode$1(this.data.getRoot(), text, this.#optionsCompound).node;
	}
	has(word, minLegacyCompoundLength) {
		if (this.hasWord(word, false)) return true;
		if (minLegacyCompoundLength) return !!this.findWord(word, { useLegacyWordCompounds: minLegacyCompoundLength }).found;
		return false;
	}
	/**
	* Determine if a word is in the dictionary.
	* @param word - the exact word to search for - must be normalized.
	* @param caseSensitive - false means also searching a dictionary where the words were normalized to lower case and accents removed.
	* @returns true if the word was found and is not forbidden.
	*/
	hasWord(word, caseSensitive) {
		return !!this.findWord(word, {
			caseSensitive,
			checkForbidden: false
		}).found;
	}
	findWord(word, options) {
		if (options?.useLegacyWordCompounds) {
			const len = options.useLegacyWordCompounds !== true ? options.useLegacyWordCompounds : defaultLegacyMinCompoundLength$1;
			const findOptions = this.createFindOptions({
				legacyMinCompoundLength: len,
				matchCase: options.caseSensitive || false
			});
			return findLegacyCompound$1(this.root, word, findOptions);
		}
		return findWord$1(this.root, word, {
			matchCase: options?.caseSensitive,
			checkForbidden: options?.checkForbidden
		});
	}
	/**
	* Determine if a word is in the forbidden word list.
	* @param word the word to lookup.
	*/
	isForbiddenWord(word) {
		return this.hasForbiddenWords && isForbiddenWord$1(this.root, word, this.info.forbiddenWordPrefix);
	}
	/**
	* Provides an ordered sequence of words with the prefix of text.
	*/
	completeWord(text) {
		const n = this.find(text);
		const compoundChar = this.info.compoundCharacter;
		const subNodes = pipe(n ? iteratorTrieWords$1(n) : [], opFilter((w) => w[w.length - 1] !== compoundChar), opMap((suffix) => text + suffix));
		return pipe(n && n.eow ? [text] : [], opAppend(subNodes));
	}
	/**
	* Suggest spellings for `text`.  The results are sorted by edit distance with changes near the beginning of a word having a greater impact.
	* @param text - the text to search for
	* @param maxNumSuggestions - the maximum number of suggestions to return.
	* @param compoundMethod - Use to control splitting words.
	* @param numChanges - the maximum number of changes allowed to text. This is an approximate value, since some changes cost less than others.
	*                      the lower the value, the faster results are returned. Values less than 4 are best.
	*/
	suggest(text, options) {
		return this.suggestWithCost(text, options).map((a) => a.word);
	}
	/**
	* Suggest spellings for `text`.  The results are sorted by edit distance with changes near the beginning of a word having a greater impact.
	* The results include the word and adjusted edit cost.  This is useful for merging results from multiple tries.
	*/
	suggestWithCost(text, options) {
		const sep = options.compoundSeparator;
		const weightMap = options.weightMap || this.weightMap;
		const adjWord = sep ? replaceAllFactory(sep, "") : (a) => a;
		const optFilter = options.filter;
		const filter = optFilter ? (word, cost) => {
			const w = adjWord(word);
			return !this.isForbiddenWord(w) && optFilter(w, cost);
		} : (word) => !this.isForbiddenWord(adjWord(word));
		const opts = {
			...options,
			filter,
			weightMap
		};
		return suggestAStar(this.data, text, opts);
	}
	/**
	* genSuggestions will generate suggestions and send them to `collector`. `collector` is responsible for returning the max acceptable cost.
	* Costs are measured in weighted changes. A cost of 100 is the same as 1 edit. Some edits are considered cheaper.
	* Returning a MaxCost < 0 will effectively cause the search for suggestions to stop.
	*/
	genSuggestions(collector, compoundMethod) {
		const filter = (word) => !this.isForbiddenWord(word);
		const options = createSuggestionOptions(clean({
			compoundMethod,
			...collector.genSuggestionOptions
		}));
		const suggestions = getSuggestionsAStar(this.data, collector.word, options);
		collector.collect(suggestions, void 0, filter);
	}
	/**
	* Returns an iterator that can be used to get all words in the trie. For some dictionaries, this can result in millions of words.
	*/
	words() {
		return iteratorTrieWords$1(this.root);
	}
	/**
	* Allows iteration over the entire tree.
	* On the returned Iterator, calling .next(goDeeper: boolean), allows for controlling the depth.
	*/
	iterate() {
		return walker$1(this.root);
	}
	static create(words, info) {
		const builder = new FastTrieBlobBuilder(info);
		builder.insert(words);
		return new ITrieImpl(builder.build(), void 0);
	}
	createFindOptions(options) {
		return createFindOptions$1(options);
	}
};

//#endregion
//#region src/lib/buildITrie.ts
function buildITrieFromWords(words, info = {}) {
	const builder = new FastTrieBlobBuilder(info);
	builder.insert(words);
	const ft = builder.build();
	return new ITrieImpl(ft.size > 1e3 ? ft.toTrieBlob() : ft);
}

//#endregion
//#region src/lib/utils/isValidChar.ts
function isValidChar(char) {
	return isValidUtf16Character(char);
}
function assertIsValidChar(char, message) {
	if (!isValidChar(char)) assert(false, `${message} "${char}" ${formatCharCodes(char)}`);
}
function formatCharCodes(char) {
	return char.split("").map((c) => "0x" + c.charCodeAt(0).toString(16).padStart(4, "0").toUpperCase()).join(":");
}

//#endregion
//#region src/lib/TrieNode/TrieNode.ts
const FLAG_WORD = 1;

//#endregion
//#region src/lib/TrieNode/trie-util.ts
function insert(word, root = {}) {
	const text = [...word];
	let node = root;
	for (let i = 0; i < text.length; ++i) {
		const head = text[i];
		const c = node.c || Object.create(null);
		node.c = c;
		node = c[head] || {};
		c[head] = node;
	}
	node.f = (node.f || 0) | FLAG_WORD;
	return root;
}
function isWordTerminationNode(node) {
	return ((node.f || 0) & FLAG_WORD) === FLAG_WORD;
}
/**
* Sorts the nodes in a trie in place.
*/
function orderTrie(node) {
	if (!node.c) return;
	const nodes = Object.entries(node.c).sort(([a], [b]) => a < b ? -1 : 1);
	node.c = Object.fromEntries(nodes);
	for (const n of nodes) orderTrie(n[1]);
}
/**
* Generator an iterator that will walk the Trie parent then children in a depth first fashion that preserves sorted order.
*/
function walk(node) {
	return walker(node);
}
const iterateTrie = walk;
/**
* Generate a Iterator that can walk a Trie and yield the words.
*/
function iteratorTrieWords(node) {
	return walkerWords(node);
}
function createTrieRoot(options) {
	return {
		...mergeOptionalWithDefaults(options),
		c: Object.create(null)
	};
}
function createTrieRootFromList(words, options) {
	const root = createTrieRoot(options);
	for (const word of words) if (word.length) insert(word, root);
	return root;
}
function has(node, word) {
	let h = word.slice(0, 1);
	let t = word.slice(1);
	while (node.c && h in node.c) {
		node = node.c[h];
		h = t.slice(0, 1);
		t = t.slice(1);
	}
	return !h.length && !!((node.f || 0) & FLAG_WORD);
}
function findNode(node, word) {
	for (let i = 0; i < word.length; ++i) {
		const n = node.c?.[word[i]];
		if (!n) return void 0;
		node = n;
	}
	return node;
}
function countNodes(root) {
	const seen = /* @__PURE__ */ new Set();
	function walk$3(n) {
		if (seen.has(n)) return;
		seen.add(n);
		if (n.c) Object.values(n.c).forEach((n$1) => walk$3(n$1));
	}
	walk$3(root);
	return seen.size;
}
function countWords(root) {
	const visited = /* @__PURE__ */ new Map();
	function walk$3(n) {
		if (visited.has(n)) return visited.get(n);
		let cnt = n.f ? 1 : 0;
		visited.set(n, cnt);
		if (!n.c) return cnt;
		for (const c of Object.values(n.c)) cnt += walk$3(c);
		visited.set(n, cnt);
		return cnt;
	}
	return walk$3(root);
}
function checkCircular(root) {
	const seen = /* @__PURE__ */ new Set();
	const inStack = /* @__PURE__ */ new Set();
	function walk$3(n) {
		if (seen.has(n)) return {
			isCircular: false,
			allSeen: true
		};
		if (inStack.has(n)) {
			const stack = [...inStack, n];
			return {
				isCircular: true,
				allSeen: false,
				ref: {
					stack,
					word: trieStackToWord(stack),
					pos: stack.indexOf(n)
				}
			};
		}
		inStack.add(n);
		let r = {
			isCircular: false,
			allSeen: true
		};
		if (n.c) r = Object.values(n.c).reduce((acc, n$1) => {
			if (acc.isCircular) return acc;
			const r$1 = walk$3(n$1);
			r$1.allSeen = r$1.allSeen && acc.allSeen;
			return r$1;
		}, r);
		if (r.allSeen) seen.add(n);
		inStack.delete(n);
		return r;
	}
	return walk$3(root);
}
function reverseMapTrieNode(node) {
	return node.c && new Map(Object.entries(node.c).map(([c, n]) => [n, c]));
}
function trieStackToWord(stack) {
	let word = "";
	let lastMap = reverseMapTrieNode(stack[0]);
	for (let i = 1; i < stack.length; ++i) {
		const n = stack[i];
		const char = lastMap?.get(n);
		if (char) word += char;
		lastMap = reverseMapTrieNode(n);
	}
	return word;
}
function isCircular(root) {
	return checkCircular(root).isCircular;
}
function trieNodeToRoot(node, options) {
	return {
		...mergeOptionalWithDefaults(options),
		c: node.c || Object.create(null)
	};
}

//#endregion
//#region src/lib/consolidate.ts
/**
* Consolidate to DAWG
* @param root the root of the Trie tree
*/
function consolidate(root) {
	let count = 0;
	const signatures = /* @__PURE__ */ new Map();
	const cached = /* @__PURE__ */ new Map();
	const knownMap = /* @__PURE__ */ new Map();
	if (isCircular(root)) throw new Error("Trie is circular.");
	function signature$1(n) {
		return (n.f ? "*" : "") + (n.c ? JSON.stringify(Object.entries(n.c).map(([k, n$1]) => [k, cached.get(n$1)])) : "");
	}
	function findEow(n) {
		if (n.f && !n.c) return n;
		let r;
		// istanbul ignore else
		if (n.c) for (const c of Object.values(n.c)) {
			r = findEow(c);
			// istanbul ignore else
			if (r) break;
		}
		return r;
	}
	function compareMaps(a, b) {
		for (const e of a) if (b[e[0]] !== e[1]) return false;
		return a.length === b.size;
	}
	function deepCopy(n) {
		const k = knownMap.get(n);
		if (k) return k;
		const orig = n;
		if (n.c) {
			const children = Object.entries(n.c).map((c) => [c[0], deepCopy(c[1])]);
			if (!compareMaps(children, n.c)) n = {
				f: n.f,
				c: Object.fromEntries(children)
			};
		}
		const sig = signature$1(n);
		const ref = signatures.get(sig);
		if (ref) {
			knownMap.set(orig, ref);
			return ref;
		}
		Object.freeze(n);
		signatures.set(sig, n);
		cached.set(n, count++);
		knownMap.set(orig, n);
		return n;
	}
	function process(n) {
		if (cached.has(n)) return n;
		if (Object.isFrozen(n)) return knownMap.get(n) || deepCopy(n);
		if (n.c) {
			const children = Object.entries(n.c).sort((a, b) => a[0] < b[0] ? -1 : 1).map(([k, n$1]) => [k, process(n$1)]);
			n.c = Object.fromEntries(children);
		}
		const sig = signature$1(n);
		const ref = signatures.get(sig);
		if (ref) return ref;
		signatures.set(sig, n);
		cached.set(n, count++);
		return n;
	}
	const eow = findEow(root) || {
		f: FLAG_WORD,
		c: void 0
	};
	signatures.set(signature$1(eow), eow);
	cached.set(eow, count++);
	return trieNodeToRoot(process(root), root);
}

//#endregion
//#region src/lib/TrieNode/find.ts
const _defaultFindOptions = {
	matchCase: false,
	compoundMode: "compound",
	forbidPrefix: FORBID_PREFIX,
	compoundFix: COMPOUND_FIX,
	caseInsensitivePrefix: CASE_INSENSITIVE_PREFIX,
	legacyMinCompoundLength: 3
};
const knownCompoundModes = new Map([
	"none",
	"compound",
	"legacy"
].map((a) => [a, a]));
/**
*
* @param root Trie root node. root.c contains the compound root and forbidden root.
* @param word A pre normalized word use `normalizeWord` or `normalizeWordToLowercase`
* @param options
*/
function findWord(root, word, options) {
	return _findWord(root, word, createFindOptions(options));
}
/**
*
* @param root Trie root node. root.c contains the compound root and forbidden root.
* @param word A pre normalized word use `normalizeWord` or `normalizeWordToLowercase`
* @param options
*/
function findWordNode(root, word, options) {
	return _findWordNode(root, word, createFindOptions(options));
}
/**
*
* @param root Trie root node. root.c contains the compound root and forbidden root.
* @param word A pre normalized word use `normalizeWord` or `normalizeWordToLowercase`
* @param options
*/
function _findWord(root, word, options) {
	const { node: _,...result } = _findWordNode(root, word, options);
	return result;
}
/**
*
* @param root Trie root node. root.c contains the compound root and forbidden root.
* @param word A pre normalized word use `normalizeWord` or `normalizeWordToLowercase`
* @param options
*/
function _findWordNode(root, word, options) {
	const compoundMode = knownCompoundModes.get(options.compoundMode) || _defaultFindOptions.compoundMode;
	const compoundPrefix = options.compoundMode === "compound" ? root.compoundCharacter ?? options.compoundFix : "";
	const ignoreCasePrefix = options.matchCase ? "" : root.stripCaseAndAccentsPrefix ?? options.caseInsensitivePrefix;
	function __findCompound() {
		const f = findCompoundWord(root, word, compoundPrefix, ignoreCasePrefix);
		const result = { ...f };
		if (f.found !== false && f.compoundUsed) result.forbidden = isForbiddenWord(!f.caseMatched ? walk$1(root, options.caseInsensitivePrefix) : root, word, options.forbidPrefix);
		return result;
	}
	function __findExact() {
		const n = walk$1(root, word);
		return {
			found: isEndOfWordNode(n) && word,
			compoundUsed: false,
			forbidden: isForbiddenWord(root, word, options.forbidPrefix),
			node: n,
			caseMatched: true
		};
	}
	switch (compoundMode) {
		case "none": return options.matchCase ? __findExact() : __findCompound();
		case "compound": return __findCompound();
		case "legacy": return findLegacyCompound(root, word, options);
	}
}
function findLegacyCompound(root, word, options) {
	const roots = [root];
	if (!options.matchCase) roots.push(walk$1(root, options.caseInsensitivePrefix));
	return findLegacyCompoundNode(roots, word, options.legacyMinCompoundLength);
}
function findCompoundNode(root, word, compoundCharacter, ignoreCasePrefix) {
	const stack = [{
		n: root,
		compoundPrefix: ignoreCasePrefix,
		cr: void 0,
		caseMatched: true
	}];
	const compoundPrefix = compoundCharacter || ignoreCasePrefix;
	const possibleCompoundPrefix = ignoreCasePrefix && compoundCharacter ? ignoreCasePrefix + compoundCharacter : "";
	const nw = word.normalize();
	const w = [...nw];
	function determineRoot(s) {
		const prefix = s.compoundPrefix;
		let r = root;
		let i$1;
		for (i$1 = 0; i$1 < prefix.length && r; ++i$1) r = r.c?.[prefix[i$1]];
		const caseMatched$1 = s.caseMatched && prefix[0] !== ignoreCasePrefix;
		return {
			n: s.n,
			compoundPrefix: prefix === compoundPrefix ? possibleCompoundPrefix : "",
			cr: r,
			caseMatched: caseMatched$1
		};
	}
	let compoundUsed = false;
	let caseMatched = true;
	let i = 0;
	let node;
	while (true) {
		const s = stack[i];
		const h = w[i++];
		const c = (s.cr || s.n)?.c?.[h];
		if (c && i < word.length) {
			caseMatched = s.caseMatched;
			stack[i] = {
				n: c,
				compoundPrefix,
				cr: void 0,
				caseMatched
			};
		} else if (!c || !c.f) {
			node = node || c;
			while (--i > 0) {
				const s$1 = stack[i];
				if (!s$1.compoundPrefix || !s$1.n?.c) continue;
				if (compoundCharacter in s$1.n.c) break;
			}
			if (i >= 0 && stack[i].compoundPrefix) {
				compoundUsed = i > 0;
				const r = determineRoot(stack[i]);
				stack[i] = r;
				if (!r.cr) break;
				if (!i && !r.caseMatched && nw !== nw.toLowerCase()) break;
			} else break;
		} else {
			node = c;
			caseMatched = s.caseMatched;
			break;
		}
	}
	return {
		found: i && i === word.length && word || false,
		compoundUsed,
		node,
		forbidden: void 0,
		caseMatched
	};
}
function findCompoundWord(root, word, compoundCharacter, ignoreCasePrefix) {
	const { found, compoundUsed, node, caseMatched } = findCompoundNode(root, word, compoundCharacter, ignoreCasePrefix);
	if (!node || !node.f) return {
		found: false,
		compoundUsed,
		node,
		forbidden: void 0,
		caseMatched
	};
	return {
		found,
		compoundUsed,
		node,
		forbidden: void 0,
		caseMatched
	};
}
function findWordExact(root, word) {
	return isEndOfWordNode(walk$1(root, word));
}
function isEndOfWordNode(n) {
	return n?.f === FLAG_WORD;
}
function walk$1(root, word) {
	const w = [...word];
	let n = root;
	let i = 0;
	while (n && i < w.length) {
		const h = w[i++];
		n = n.c?.[h];
	}
	return n;
}
function findLegacyCompoundNode(roots, word, minCompoundLength) {
	const root = roots[0];
	const numRoots = roots.length;
	const stack = [{
		n: root,
		usedRoots: 1,
		subLength: 0,
		isCompound: false,
		cr: void 0,
		caseMatched: true
	}];
	const w = word;
	const wLen = w.length;
	let compoundUsed = false;
	let caseMatched = true;
	let i = 0;
	let node;
	while (true) {
		const s = stack[i];
		const h = w[i++];
		const c = (s.cr || s.n)?.c?.[h];
		if (c && i < wLen) stack[i] = {
			n: c,
			usedRoots: 0,
			subLength: s.subLength + 1,
			isCompound: s.isCompound,
			cr: void 0,
			caseMatched: s.caseMatched
		};
		else if (!c || !c.f || c.f && s.subLength < minCompoundLength - 1) {
			while (--i > 0) {
				const s$1 = stack[i];
				if (s$1.usedRoots < numRoots && s$1.n?.f && (s$1.subLength >= minCompoundLength || !s$1.subLength) && wLen - i >= minCompoundLength) break;
			}
			if (i > 0 || stack[i].usedRoots < numRoots) {
				compoundUsed = i > 0;
				const s$1 = stack[i];
				s$1.cr = roots[s$1.usedRoots++];
				s$1.subLength = 0;
				s$1.isCompound = compoundUsed;
				s$1.caseMatched = s$1.caseMatched && s$1.usedRoots <= 1;
			} else break;
		} else {
			node = c;
			caseMatched = s.caseMatched;
			break;
		}
	}
	function extractWord() {
		if (!word || i < word.length) return false;
		const letters = [];
		let subLen = 0;
		for (let j = 0; j < i; ++j) {
			const { subLength } = stack[j];
			if (subLength < subLen) letters.push("+");
			letters.push(word[j]);
			subLen = subLength;
		}
		return letters.join("");
	}
	return {
		found: extractWord(),
		compoundUsed,
		node,
		forbidden: void 0,
		caseMatched
	};
}
function isForbiddenWord(root, word, forbiddenPrefix) {
	return findWordExact(root?.c?.[forbiddenPrefix], word);
}
const createFindOptions = memorizeLastCall(_createFindOptions);
function _createFindOptions(options) {
	return mergeDefaults(options, _defaultFindOptions);
}

//#endregion
//#region src/lib/TrieNode/TrieNodeTrie.ts
var TrieNodeTrie = class TrieNodeTrie {
	_iTrieRoot;
	info;
	_size;
	hasForbiddenWords;
	hasCompoundWords;
	hasNonStrictWords;
	constructor(root) {
		this.root = root;
		this.info = mergeOptionalWithDefaults(root);
		this.hasForbiddenWords = !!root.c[root.forbiddenWordPrefix];
		this.hasCompoundWords = !!root.c[root.compoundCharacter];
		this.hasNonStrictWords = !!root.c[root.stripCaseAndAccentsPrefix];
	}
	wordToCharacters = (word) => [...word];
	get iTrieRoot() {
		return this._iTrieRoot || (this._iTrieRoot = trieRootToITrieRoot(this.root));
	}
	getRoot() {
		return this.iTrieRoot;
	}
	getNode(prefix) {
		return findNode$1(this.getRoot(), prefix);
	}
	words() {
		return iteratorTrieWords(this.root);
	}
	has(word) {
		return findWordExact(this.root, word);
	}
	isForbiddenWord(word) {
		return findWordExact(this.root.c[this.root.forbiddenWordPrefix], word);
	}
	get size() {
		return this._size ??= countNodes(this.root);
	}
	static createFromWords(words, options) {
		return new TrieNodeTrie(createTrieRootFromList(words, options));
	}
	static createFromWordsAndConsolidate(words, options) {
		return new TrieNodeTrie(consolidate(createTrieRootFromList(words, options)));
	}
};

//#endregion
//#region src/lib/convertToTrieRefNodes.ts
const MinReferenceCount = 3;
/**
* An iterator that will emit TrieRefNodes mostly in descending frequency
* @param root Root of the Trie -- a DAWG is preferred to keep the number of duplicates down.
*/
function convertToTrieRefNodes(root) {
	const eow = {
		f: FLAG_WORD,
		c: void 0
	};
	const tallies = new Map([[eow, 0]]);
	let count = 0;
	const cached = /* @__PURE__ */ new Map();
	const rollupTally = /* @__PURE__ */ new Map();
	function tally(n) {
		if (n.f && !n.c) {
			tallies.set(eow, (tallies.get(eow) || 0) + 1);
			return;
		}
		const t = tallies.get(n);
		if (t) {
			tallies.set(n, t + 1);
			return;
		}
		tallies.set(n, 1);
		for (const c of n.c && Object.values(n.c) || []) tally(c);
	}
	function rollup(n) {
		const c = rollupTally.get(n);
		if (c) return c;
		if (!n.c) {
			const sum$1 = tallies.get(eow) || 0;
			rollupTally.set(n, sum$1);
			return sum$1;
		}
		const sum = Object.values(n.c).reduce((acc, v) => acc + rollup(v), tallies.get(n) || 0);
		rollupTally.set(n, sum);
		return sum;
	}
	function* walkByTallies(tallies$1) {
		const nodes = genSequence(tallies$1).filter((a) => a[1] >= MinReferenceCount);
		for (const [n] of [...nodes].sort((a, b) => b[1] - a[1])) yield* walkByRollup(n);
	}
	function* walkByRollup(n) {
		if (cached.has(n)) return;
		if (n.f && !n.c) {
			cached.set(n, cached.get(eow));
			return;
		}
		const children = (n.c && Object.values(n.c) || []).sort((a, b) => (rollupTally.get(b) || 0) - (rollupTally.get(a) || 0));
		for (const c of children) yield* walkByRollup(c);
		cached.set(n, count++);
		yield convert(n);
	}
	function convert(n) {
		const { f, c } = n;
		const r = c ? Object.entries(c).sort((a, b) => a[0] < b[0] ? -1 : 1).map(([s, n$1]) => [s, cached.get(n$1)]) : void 0;
		return r ? f ? {
			f,
			r
		} : { r } : { f };
	}
	function* walk$3(root$1) {
		cached.set(eow, count++);
		yield convert(eow);
		yield* walkByTallies(tallies);
		yield* walkByRollup(root$1);
	}
	tally(root);
	rollup(root);
	return walk$3(root);
}

//#endregion
//#region src/lib/io/importExportV1.ts
const EOW$3 = "*";
const DATA$4 = EOW$3;
function toReferences(node) {
	return genSequence(convertToTrieRefNodes(node));
}
const regExpEscapeChars = /([[\]\\,:{}*])/g;
const regExTrailingComma = /,(\}|\n)/g;
function escapeChar(char) {
	return char.replaceAll(regExpEscapeChars, "\\$1");
}
function trieToExportString(node, base) {
	function* walk$3(node$1) {
		if (node$1.f) yield EOW$3;
		if (node$1.r) {
			const refs = [...node$1.r].sort((a, b) => a[0] < b[0] ? -1 : 1);
			for (const n of refs) {
				const [c, r] = n;
				const ref = r ? r.toString(base) : "";
				yield escapeChar(c) + ref + ",";
			}
		}
	}
	return genSequence(walk$3(node));
}
function generateHeader$3(base, comment) {
	return genSequence([
		...[
			"#!/usr/bin/env cspell-trie reader",
			"TrieXv1",
			"base=" + base
		],
		...comment ? comment.split("\n").map((a) => "# " + a) : [],
		...["# Data:"]
	]).map((a) => a + "\n");
}
/**
* Serialize a TrieNode.
* Note: This is destructive.  The node will no longer be usable.
* Even though it is possible to preserve the trie, dealing with very large tries can consume a lot of memory.
* Considering this is the last step before exporting, it was decided to let this be destructive.
*/
function serializeTrie$4(root, options = 16) {
	options = typeof options === "number" ? { base: options } : options;
	const { base = 16, comment = "" } = options;
	const radix = base > 36 ? 36 : base < 10 ? 10 : base;
	const rows = toReferences(root).map((node) => {
		return [...trieToExportString(node, radix), "\n"].join("").replaceAll(regExTrailingComma, "$1");
	});
	return generateHeader$3(radix, comment).concat(rows);
}
function* toIterableIterator$1(iter) {
	yield* iter;
}
function importTrie$5(linesX) {
	let radix = 16;
	const comment = /^\s*#/;
	const iter = toIterableIterator$1(linesX);
	function parseHeaderRows(headerRows) {
		const header = headerRows.slice(0, 2).join("\n");
		const headerReg$2 = /^TrieXv1\nbase=(\d+)$/;
		/* istanbul ignore if */
		if (!headerReg$2.test(header)) throw new Error("Unknown file format");
		radix = Number.parseInt(header.replace(headerReg$2, "$1"), 10);
	}
	function readHeader(iter$1) {
		const headerRows = [];
		while (true) {
			const next = iter$1.next();
			if (next.done) break;
			const line = next.value.trim();
			if (!line || comment.test(line)) continue;
			if (line === DATA$4) break;
			headerRows.push(line);
		}
		parseHeaderRows(headerRows);
	}
	const regNotEscapedCommas = /(^|[^\\]),/g;
	const regUnescapeCommas = /__COMMA__/g;
	const regUnescape = /[\\](.)/g;
	const flagsWord = { f: FLAG_WORD };
	function splitLine$1(line) {
		return line.replaceAll(regNotEscapedCommas, "$1__COMMA__").split(regUnescapeCommas).map((a) => a.replaceAll(regUnescape, "$1"));
	}
	function decodeLine$1(line, nodes) {
		const isWord = line[0] === EOW$3;
		line = isWord ? line.slice(1) : line;
		const flags = isWord ? flagsWord : {};
		const children = splitLine$1(line).filter((a) => !!a).map((a) => [a[0], Number.parseInt(a.slice(1) || "0", radix)]).map(([k, i]) => [k, nodes[i]]);
		return {
			...children.length ? { c: Object.fromEntries(children) } : {},
			...flags
		};
	}
	readHeader(iter);
	return trieNodeToRoot(genSequence([DATA$4]).concat(iter).map((a) => a.replace(/\r?\n/, "")).filter((a) => !!a).reduce((acc, line) => {
		const { lines, nodes } = acc;
		const root = decodeLine$1(line, nodes);
		nodes[lines] = root;
		return {
			lines: lines + 1,
			root,
			nodes
		};
	}, {
		lines: 0,
		nodes: [],
		root: {}
	}).root, { isCaseAware: false });
}

//#endregion
//#region src/lib/io/importExportV2.ts
const EOW$2 = "*";
const DATA$3 = "__DATA__";
function leaves(node) {
	function toRefNode(node$1, k) {
		const refNode = node$1;
		refNode.s = refNode.s ?? k;
		return refNode;
	}
	function* walk$3(node$1, k, p) {
		const ref = toRefNode(node$1, k);
		if (!ref.c) yield {
			n: ref,
			p
		};
		else for (const n of Object.entries(ref.c)) yield* walk$3(n[1], n[0], ref);
	}
	return genSequence(walk$3(node, ""));
}
function flattenToReferences(node) {
	function* walk$3() {
		let iterations = 100;
		let processed = 0;
		let index = 0;
		do {
			processed = 0;
			const signatureMap = /* @__PURE__ */ new Map();
			for (const leaf of leaves(node)) {
				const h = signature(leaf.n);
				let m = signatureMap.get(h);
				if (m === void 0) {
					yield leaf.n;
					m = index;
					signatureMap.set(h, m);
					index += 1;
				}
				/* istanbul ignore else */
				if (leaf.p && leaf.p.c) {
					leaf.p.r = leaf.p.r || [];
					leaf.p.r.push(m);
					delete leaf.p.c[leaf.n.s];
					if (!Object.entries(leaf.p.c).length) delete leaf.p.c;
				}
				processed += 1;
			}
			iterations -= 1;
		} while (processed && iterations && node.c);
		yield node;
	}
	return genSequence(walk$3());
}
function signature(node) {
	const flags = node.f ? EOW$2 : "";
	const refs = node.r ? node.r.sort((a, b) => a - b).join(",") : "";
	return node.s + flags + refs;
}
function toLine(node, base) {
	const flags = node.f ? EOW$2 : "";
	const refs = node.r ? node.r.sort((a, b) => a - b).map((r) => r.toString(base)).join(",") : "";
	return node.s + flags + refs;
}
function generateHeader$2(base, comment) {
	return genSequence([
		"#!/usr/bin/env cspell-trie reader",
		"TrieXv2",
		"base=" + base,
		...comment ? comment.split("\n").map((a) => "# " + a) : [],
		"# Data:",
		DATA$3
	]);
}
/**
* Serialize a TrieNode.
* Note: This is destructive.  The node will no longer be usable.
* Even though it is possible to preserve the trie, dealing with very large tries can consume a lot of memory.
* Considering this is the last step before exporting, it was decided to let this be destructive.
*/
function serializeTrie$3(root, options = 16) {
	options = typeof options === "number" ? { base: options } : options;
	const { base = 16, comment = "" } = options;
	const radix = base > 36 ? 36 : base < 10 ? 10 : base;
	const rows = flattenToReferences({
		...root,
		s: "^"
	}).map((n) => toLine(n, base));
	return generateHeader$2(radix, comment).concat(rows).map((a) => a + "\n");
}
function* toIterableIterator(iter) {
	yield* iter;
}
function importTrie$4(linesX) {
	let radix = 16;
	const comment = /^\s*#/;
	const iter = toIterableIterator(linesX);
	function parseHeaderRows(headerRows) {
		const header = headerRows.slice(0, 2).join("\n");
		const headerReg$2 = /^TrieXv2\nbase=(\d+)$/;
		/* istanbul ignore if */
		if (!headerReg$2.test(header)) throw new Error("Unknown file format");
		radix = Number.parseInt(header.replace(headerReg$2, "$1"), 10);
	}
	function readHeader(iter$1) {
		const headerRows = [];
		while (true) {
			const next = iter$1.next();
			if (next.done) break;
			const line = next.value.trim();
			if (!line || comment.test(line)) continue;
			if (line === DATA$3) break;
			headerRows.push(line);
		}
		parseHeaderRows(headerRows);
	}
	function parseLine(line, base) {
		const isWord = line[1] === EOW$2;
		const refOffset = isWord ? 2 : 1;
		const refs = line.slice(refOffset).split(",").filter((a) => !!a).map((r) => Number.parseInt(r, base));
		return {
			letter: line[0],
			isWord,
			refs
		};
	}
	const flagsWord = { f: FLAG_WORD };
	function decodeLine$1(line, nodes) {
		const { letter, isWord, refs } = parseLine(line, radix);
		const flags = isWord ? flagsWord : {};
		const children = refs.map((r) => nodes[r]).sort((a, b) => a.s < b.s ? -1 : 1).map((n) => [n.s, n]);
		return {
			s: letter,
			...children.length ? { c: Object.fromEntries(children) } : {},
			...flags
		};
	}
	readHeader(iter);
	return trieNodeToRoot(genSequence(iter).map((a) => a.replace(/\r?\n/, "")).filter((a) => !!a).reduce((acc, line) => {
		const { nodes } = acc;
		const root = decodeLine$1(line, nodes);
		nodes.push(root);
		return {
			root,
			nodes
		};
	}, {
		nodes: [],
		root: {
			s: "",
			c: Object.create(null)
		}
	}).root, { isCaseAware: false });
}

//#endregion
//#region src/lib/utils/bufferLines.ts
function* buffer(iter, bufferSize) {
	const buffer$1 = [];
	for (const s of iter) {
		buffer$1.push(s);
		if (buffer$1.length >= bufferSize) {
			yield buffer$1;
			buffer$1.length = 0;
		}
	}
	if (buffer$1.length) {
		yield buffer$1;
		buffer$1.length = 0;
	}
}
function* bufferLines(iter, bufferSize, eol) {
	if (eol) for (const s of buffer(iter, bufferSize)) yield s.join("") + eol;
	else for (const s of buffer(iter, bufferSize)) yield s.join("");
}

//#endregion
//#region src/lib/io/constants.ts
/** End of word */
const EOW = "$";
/** Move up the tree */
const BACK = "<";
/** End of Line (ignored) */
const EOL = "\n";
/** Line Feed (ignored) */
const LF = "\r";
/** Start of Absolute Reference */
const REF = "#";
/** Start indexed of Reference  */
const REF_REL = "@";
/** End of Reference */
const EOR = ";";
/** Escape the next character */
const ESCAPE = "\\";

//#endregion
//#region src/lib/io/importExportV4.ts
const REF_INDEX_BEGIN = "[";
const REF_INDEX_END = "]";
const INLINE_DATA_COMMENT_LINE = "/";
const specialCharacters$1 = stringToCharSet$2([
	EOW,
	BACK,
	EOL,
	REF,
	REF_REL,
	EOR,
	ESCAPE,
	LF,
	REF_INDEX_BEGIN,
	REF_INDEX_END,
	INLINE_DATA_COMMENT_LINE,
	..."0123456789",
	..."`~!@#$%^&*()_-+=[]{};:'\"<>,./?\\|"
].join(""));
const SPECIAL_CHARACTERS_MAP = [
	["\n", "\\n"],
	["\r", "\\r"],
	["\\", "\\\\"]
];
const specialCharacterMap$2 = stringToCharMap(SPECIAL_CHARACTERS_MAP);
const characterMap$1 = stringToCharMap(SPECIAL_CHARACTERS_MAP.map((a) => [a[1], a[0]]));
const specialPrefix$1 = stringToCharSet$2("~!");
const WORDS_PER_LINE$1 = 20;
const DATA$2 = "__DATA__";
function generateHeader$1(base, comment) {
	return `\
#!/usr/bin/env cspell-trie reader
TrieXv4
base=${base}
${comment.split("\n").map((a) => "# " + a.trimEnd()).join("\n")}
# Data:
${DATA$2}
`;
}
/**
* Serialize a TrieRoot.
*/
function serializeTrie$2(root, options = 16) {
	options = typeof options === "number" ? { base: options } : options;
	const { base = 10, comment = "" } = options;
	const radix = base > 36 ? 36 : base < 10 ? 10 : base;
	const cache = /* @__PURE__ */ new Map();
	const refMap = buildReferenceMap(root, base);
	const nodeToIndexMap = new Map(refMap.refCounts.map(([node], index) => [node, index]));
	let count = 0;
	const backBuffer = {
		last: "",
		count: 0,
		words: 0,
		eol: false
	};
	const wordChars = [];
	function ref(n, idx$1) {
		const r = idx$1 === void 0 || n < idx$1 ? REF + n.toString(radix) : REF_REL + idx$1.toString(radix);
		return radix === 10 ? r : r + ";";
	}
	function escape(s) {
		return s in specialCharacters$1 ? ESCAPE + (specialCharacterMap$2[s] || s) : s;
	}
	function* flush() {
		while (backBuffer.count) {
			const n = Math.min(9, backBuffer.count);
			yield n > 1 ? backBuffer.last + n : backBuffer.last;
			backBuffer.last = BACK;
			backBuffer.count -= n;
		}
		if (backBuffer.eol) {
			yield EOL;
			backBuffer.eol = false;
			backBuffer.words = 0;
		}
	}
	function* emit(s) {
		switch (s) {
			case EOW:
				yield* flush();
				backBuffer.last = EOW;
				backBuffer.count = 0;
				backBuffer.words++;
				break;
			case BACK:
				backBuffer.count++;
				break;
			case EOL:
				backBuffer.eol = true;
				break;
			default:
				if (backBuffer.words >= WORDS_PER_LINE$1) backBuffer.eol = true;
				yield* flush();
				if (s.startsWith(REF) || s.startsWith(REF_REL)) backBuffer.words++;
				yield s;
		}
	}
	const comment_begin = `${EOL}${INLINE_DATA_COMMENT_LINE}* `;
	const comment_end = ` *${INLINE_DATA_COMMENT_LINE}${EOL}`;
	function* walk$3(node, depth) {
		const nodeNumber = cache.get(node);
		const refIndex = nodeToIndexMap.get(node);
		if (nodeNumber !== void 0) {
			yield* emit(ref(nodeNumber, refIndex));
			return;
		}
		if (node.c) {
			if (depth > 0 && depth <= 2) yield* emit(comment_begin + wordChars.slice(0, depth).map(escape).join("") + comment_end);
			cache.set(node, count++);
			const c = Object.entries(node.c).sort((a, b) => a[0] < b[0] ? -1 : 1);
			for (const [s, n] of c) {
				wordChars[depth] = s;
				yield* emit(escape(s));
				yield* walk$3(n, depth + 1);
				yield* emit(BACK);
				if (depth === 0) yield* emit(EOL);
			}
		}
		if (node.f) yield* emit(EOW);
		if (depth === 2 || depth === 3 && wordChars[0] in specialPrefix$1) yield* emit(EOL);
	}
	function* serialize(node) {
		yield* walk$3(node, 0);
		yield* flush();
	}
	const lines = [...bufferLines(serialize(root), 1e3, "")];
	const reference = "[\n" + refMap.refCounts.map(([node]) => cache.get(node) || 0).map((n) => n.toString(radix)).join(",").replaceAll(/.{110,130}[,]/g, "$&\n") + "\n]\n";
	return pipe([generateHeader$1(radix, comment), reference], opAppend(lines));
}
function buildReferenceMap(root, base) {
	const refCount = /* @__PURE__ */ new Map();
	let nodeCount = 0;
	function walk$3(node) {
		const ref = refCount.get(node);
		if (ref) {
			ref.c++;
			return;
		}
		refCount.set(node, {
			c: 1,
			n: nodeCount++
		});
		if (!node.c) return;
		for (const child of Object.values(node.c)) walk$3(child);
	}
	walk$3(root);
	const refCountAndNode = [...pipe(refCount, opFilter(([_, ref]) => ref.c >= 2))].sort((a, b) => b[1].c - a[1].c || a[1].n - b[1].n);
	let adj = 0;
	const baseLogScale = 1 / Math.log(base);
	return { refCounts: refCountAndNode.filter(([_, ref], idx$1) => {
		const i = idx$1 - adj;
		const charsIdx = Math.ceil(Math.log(i) * baseLogScale);
		const charsNode = Math.ceil(Math.log(ref.n) * baseLogScale);
		const keep = ref.c * (charsNode - charsIdx) - charsIdx > 0;
		adj += keep ? 0 : 1;
		return keep;
	}).map(([n, ref]) => [n, ref.c]) };
}
function importTrie$3(linesX) {
	linesX = typeof linesX === "string" ? linesX.split(/^/m) : linesX;
	let radix = 10;
	const comment = /^\s*#/;
	const iter = tapIterable(pipe(linesX, opConcatMap((a) => a.split(/^/m))));
	function parseHeaderRows(headerRows) {
		const header = headerRows.slice(0, 2).join("\n");
		const headerReg$2 = /^TrieXv[34]\nbase=(\d+)$/;
		/* istanbul ignore if */
		if (!headerReg$2.test(header)) throw new Error("Unknown file format");
		radix = Number.parseInt(header.replace(headerReg$2, "$1"), 10);
	}
	function readHeader(iter$1) {
		const headerRows = [];
		for (const value of iter$1) {
			const line = value.trim();
			if (!line || comment.test(line)) continue;
			if (line === DATA$2) break;
			headerRows.push(line);
		}
		parseHeaderRows(headerRows);
	}
	readHeader(iter);
	return parseStream$1(radix, iter);
}
const numbersSet = stringToCharSet$2("0123456789");
function parseStream$1(radix, iter) {
	const eow = Object.freeze({ f: 1 });
	let refIndex = [];
	const root = trieNodeToRoot({}, {});
	function parseReference(acc, s) {
		const isIndexRef = s === REF_REL;
		let ref = "";
		function parser(acc$1, s$1) {
			if (s$1 === EOR || radix === 10 && !(s$1 in numbersSet)) {
				const { root: root$1, nodes: nodes$1, stack } = acc$1;
				const r = Number.parseInt(ref, radix);
				const top = stack[stack.length - 1];
				const p = stack[stack.length - 2].node;
				const n = isIndexRef ? refIndex[r] : r;
				p.c && (p.c[top.s] = nodes$1[n]);
				const rr = {
					root: root$1,
					nodes: nodes$1,
					stack,
					parser: void 0
				};
				return s$1 === EOR ? rr : parserMain(rr, s$1);
			}
			ref = ref + s$1;
			return acc$1;
		}
		const { nodes } = acc;
		nodes.pop();
		return {
			...acc,
			nodes,
			parser
		};
	}
	function parseEscapeCharacter(acc, _) {
		let prev = "";
		const parser = function(acc$1, s) {
			if (prev) {
				s = characterMap$1[prev + s] || s;
				return parseCharacter({
					...acc$1,
					parser: void 0
				}, s);
			}
			if (s === ESCAPE) {
				prev = s;
				return acc$1;
			}
			return parseCharacter({
				...acc$1,
				parser: void 0
			}, s);
		};
		return {
			...acc,
			parser
		};
	}
	function parseComment(acc, s) {
		const endOfComment = s;
		let isEscaped = false;
		function parser(acc$1, s$1) {
			if (isEscaped) {
				isEscaped = false;
				return acc$1;
			}
			if (s$1 === ESCAPE) {
				isEscaped = true;
				return acc$1;
			}
			if (s$1 === endOfComment) return {
				...acc$1,
				parser: void 0
			};
			return acc$1;
		}
		return {
			...acc,
			parser
		};
	}
	function parseCharacter(acc, s) {
		const parser = void 0;
		const { root: root$1, nodes, stack } = acc;
		const node = stack[stack.length - 1].node;
		const c = node.c ?? Object.create(null);
		const n = {
			f: void 0,
			c: void 0,
			n: nodes.length
		};
		c[s] = n;
		node.c = c;
		stack.push({
			node: n,
			s
		});
		nodes.push(n);
		return {
			root: root$1,
			nodes,
			stack,
			parser
		};
	}
	function parseEOW(acc, _) {
		const parser = parseBack;
		const { root: root$1, nodes, stack } = acc;
		const top = stack[stack.length - 1];
		const node = top.node;
		node.f = FLAG_WORD;
		if (!node.c) {
			top.node = eow;
			const p = stack[stack.length - 2].node;
			p.c && (p.c[top.s] = eow);
			nodes.pop();
		}
		stack.pop();
		return {
			root: root$1,
			nodes,
			stack,
			parser
		};
	}
	const charactersBack = stringToCharSet$2(BACK + "23456789");
	function parseBack(acc, s) {
		if (!(s in charactersBack)) return parserMain({
			...acc,
			parser: void 0
		}, s);
		let n = s === BACK ? 1 : Number.parseInt(s, 10) - 1;
		const { stack } = acc;
		while (n-- > 0) stack.pop();
		return {
			...acc,
			parser: parseBack
		};
	}
	function parseIgnore(acc, _) {
		return acc;
	}
	const parsers = createStringLookupMap([
		[EOW, parseEOW],
		[BACK, parseBack],
		[REF, parseReference],
		[REF_REL, parseReference],
		[ESCAPE, parseEscapeCharacter],
		[EOL, parseIgnore],
		[LF, parseIgnore],
		[INLINE_DATA_COMMENT_LINE, parseComment]
	]);
	function parserMain(acc, s) {
		return (acc.parser ?? parsers[s] ?? parseCharacter)(acc, s);
	}
	const charsetSpaces = stringToCharSet$2(" \r\n	");
	function parseReferenceIndex(acc, s) {
		let json = "";
		function parserStart(acc$1, s$1) {
			if (s$1 === REF_INDEX_BEGIN) {
				json = json + s$1;
				return {
					...acc$1,
					parser
				};
			}
			if (s$1 in charsetSpaces) return acc$1;
			return parserMain({
				...acc$1,
				parser: void 0
			}, s$1);
		}
		function parser(acc$1, s$1) {
			json = json + s$1;
			if (s$1 === REF_INDEX_END) {
				refIndex = json.replaceAll(/[\s[\]]/g, "").split(",").map((n) => Number.parseInt(n, radix));
				return {
					...acc$1,
					parser: void 0
				};
			}
			return acc$1;
		}
		return parserStart({
			...acc,
			parser: parserStart
		}, s);
	}
	reduce(pipe(iter, opConcatMap((a) => [...a])), parserMain, {
		nodes: [root],
		root,
		stack: [{
			node: root,
			s: ""
		}],
		parser: parseReferenceIndex
	});
	return root;
}
function stringToCharSet$2(values) {
	const set = Object.create(null);
	const len = values.length;
	for (let i = 0; i < len; ++i) set[values[i]] = true;
	return set;
}
function stringToCharMap(values) {
	return createStringLookupMap(values);
}
function createStringLookupMap(values) {
	const map = Object.create(null);
	const len = values.length;
	for (let i = 0; i < len; ++i) map[values[i][0]] = values[i][1];
	return map;
}
/**
* Allows an iterable to be shared by multiple consumers.
* Each consumer takes from the iterable.
* @param iterable - the iterable to share
*/
function tapIterable(iterable) {
	let lastValue;
	let iter;
	function getNext() {
		if (lastValue && lastValue.done) return { ...lastValue };
		iter = iter || iterable[Symbol.iterator]();
		lastValue = iter.next();
		return lastValue;
	}
	function* iterableFn() {
		let next;
		while (!(next = getNext()).done) yield next.value;
	}
	return { [Symbol.iterator]: iterableFn };
}

//#endregion
//#region src/lib/utils/assert.ts
function assert$1(condition, message = "Assert Failed") {
	if (condition) return;
	throw new Error(message);
}

//#endregion
//#region src/lib/TrieNode/TrieNodeBuilder.ts
const EOW$1 = Object.freeze({
	f: 1,
	k: true
});
const compare = new Intl.Collator().compare;
var TrieNodeBuilder = class {
	_cursor;
	root = {
		...defaultTrieInfo,
		c: Object.create(null)
	};
	shouldSort = false;
	wordToCharacters = (word) => [...word];
	setOptions(options) {
		const opts = mergeOptionalWithDefaults(options, this.root);
		Object.assign(this.root, opts);
		return opts;
	}
	build() {
		return new TrieNodeTrie(this.root);
	}
	getCursor() {
		this._cursor ??= this.createCursor();
		return this._cursor;
	}
	/**
	* In this case, it isn't necessary. The TrieNodeBuilder doesn't need to know the characters
	* @param _characters
	*/
	setCharacterSet(_characters) {
		this.shouldSort = true;
	}
	createCursor() {
		const nodes = [this.root, EOW$1];
		const eow = EOW$1;
		assert$1(Object.keys(this.root.c).length === 0, "The Trie MUST be empty for cursors to work.");
		const stack = [{
			n: this.root,
			c: ""
		}];
		let currNode = this.root;
		let depth = 0;
		const insertChar = (char) => {
			assertIsValidChar(char);
			if (currNode.k) {
				const s$1 = stack[depth];
				const { k: _, c: c$1,...copy } = currNode;
				currNode = s$1.n.c[s$1.c] = copy;
				if (c$1) currNode.c = Object.assign(Object.create(null), c$1);
				nodes.push(currNode);
			}
			const c = currNode.c || Object.create(null);
			currNode.c = c;
			const n = currNode;
			const next = c[char] = c[char] || {};
			nodes.push(next);
			++depth;
			const s = stack[depth];
			if (s) {
				s.n = n;
				s.c = char;
			} else stack.push({
				n,
				c: char
			});
			currNode = next;
		};
		const markEOW = () => {
			if (!currNode.c) {
				const s = stack[depth];
				s.n.c[s.c] = eow;
				if (nodes[nodes.length - 1] === currNode) nodes.pop();
				currNode = eow;
			} else currNode.f = 1;
		};
		const reference = (nodeId) => {
			const s = stack[depth];
			s.n.c[s.c] = nodes[nodeId];
			nodes.pop();
		};
		const backStep = (num) => {
			if (!num) return;
			assert$1(num <= depth && num > 0);
			depth -= num;
			currNode = stack[depth + 1].n;
		};
		return {
			insertChar,
			markEOW,
			reference,
			backStep
		};
	}
	sortChildren(node) {
		const entries = Object.entries(node.c).sort((a, b) => compare(a[0], b[0]));
		node.c = Object.fromEntries(entries);
		for (const c of Object.values(node.c)) if (c.c) this.sortChildren(c);
	}
	sortNodes() {
		if (this.shouldSort) this.sortChildren(this.root);
	}
};

//#endregion
//#region src/lib/io/importV3.ts
const specialCharacterMap$1 = new Map([
	["\n", "\\n"],
	["\r", "\\r"],
	["\\", "\\\\"]
]);
const characterMap = new Map([...specialCharacterMap$1].map((a) => [a[1], a[0]]));
const DATA$1 = "__DATA__";
function importTrieV3AsTrieRoot(srcLines) {
	return importTrieV3WithBuilder(new TrieNodeBuilder(), srcLines);
}
function importTrieV3WithBuilder(builder, srcLines) {
	const timer = getGlobalPerfTimer();
	const timerStart = timer.start("importTrieV3");
	const dataLines = typeof srcLines === "string" ? srcLines.split("\n") : Array.isArray(srcLines) ? srcLines : [...srcLines];
	let radix = 16;
	const comment = /^\s*#/;
	function parseHeaderRows(headerRows) {
		const header = headerRows.slice(0, 2).join("\n");
		const headerReg$2 = /^TrieXv3\nbase=(\d+)$/;
		/* istanbul ignore if */
		if (!headerReg$2.test(header)) throw new Error("Unknown file format");
		radix = Number.parseInt(header.replace(headerReg$2, "$1"), 10);
	}
	function findStartOfData(data) {
		for (let i = 0; i < data.length; ++i) if (data[i].includes(DATA$1)) return i;
		return -1;
	}
	function readHeader(data) {
		const headerRows = [];
		for (const hLine of data) {
			const line = hLine.trim();
			if (!line || comment.test(line)) continue;
			if (line === DATA$1) break;
			headerRows.push(line);
		}
		parseHeaderRows(headerRows);
	}
	const startOfData = findStartOfData(dataLines);
	if (startOfData < 0) throw new Error("Unknown file format");
	readHeader(dataLines.slice(0, startOfData));
	let node = {
		cursor: builder.getCursor(),
		parser: void 0
	};
	const parser = parseStream(radix);
	const timerParse = timer.start("importTrieV3.parse");
	for (let i = startOfData + 1; i < dataLines.length; ++i) {
		const line = dataLines[i];
		for (const c of line) node = parser(node, c);
	}
	timerParse();
	timerStart();
	return builder.build();
}
function parseStream(radix) {
	function parseReference(acc, _) {
		let ref = "";
		function parser(acc$1, s) {
			if (s === EOR) {
				const { cursor } = acc$1;
				const r = Number.parseInt(ref, radix);
				cursor.reference(r + 1);
				acc$1.parser = void 0;
				return acc$1;
			}
			ref = ref + s;
			return acc$1;
		}
		acc.parser = parser;
		return acc;
	}
	function parseEscapeCharacter(acc, _) {
		let prev = "";
		const parser = function(acc$1, s) {
			if (prev) {
				s = characterMap.get(prev + s) || s;
				acc$1.parser = void 0;
				return parseCharacter(acc$1, s);
			}
			if (s === ESCAPE) {
				prev = s;
				return acc$1;
			}
			acc$1.parser = void 0;
			return parseCharacter(acc$1, s);
		};
		acc.parser = parser;
		return acc;
	}
	function parseCharacter(acc, s) {
		acc.cursor.insertChar(s);
		acc.parser = void 0;
		return acc;
	}
	function parseEOW(acc, _) {
		acc.parser = parseBack;
		acc.cursor.markEOW();
		acc.cursor.backStep(1);
		return acc;
	}
	const charactersBack = stringToCharSet$1(BACK + "23456789");
	function parseBack(acc, s) {
		if (!(s in charactersBack)) {
			acc.parser = void 0;
			return parserMain(acc, s);
		}
		const n = s === BACK ? 1 : Number.parseInt(s, 10) - 1;
		acc.cursor.backStep(n);
		acc.parser = parseBack;
		return acc;
	}
	function parseIgnore(acc, _) {
		return acc;
	}
	const parsers = new Map([
		[EOW, parseEOW],
		[BACK, parseBack],
		[REF, parseReference],
		[ESCAPE, parseEscapeCharacter],
		[EOL, parseIgnore],
		[LF, parseIgnore]
	]);
	function parserMain(acc, s) {
		return (acc.parser ?? parsers.get(s) ?? parseCharacter)(acc, s);
	}
	return parserMain;
}
function stringToCharSet$1(values) {
	const set = Object.create(null);
	const len = values.length;
	for (let i = 0; i < len; ++i) set[values[i]] = true;
	return set;
}

//#endregion
//#region src/lib/io/importV3FastBlob.ts
function importTrieV3AsFastTrieBlob(srcLines) {
	return importTrieV3WithBuilder(new FastTrieBlobBuilder(), srcLines);
}

//#endregion
//#region src/lib/io/decode.ts
function decodeTrieData(raw) {
	return decodeStringFormat(typeof raw === "string" ? raw : raw.toString("utf8"));
}
function decodeStringFormat(data) {
	return importTrie$2(data);
}
const deserializers$1 = [
	(data) => new TrieNodeTrie(importTrie$5(data)),
	(data) => new TrieNodeTrie(importTrie$5(data)),
	(data) => new TrieNodeTrie(importTrie$4(data)),
	(data) => importTrieV3AsFastTrieBlob(data),
	(data) => new TrieNodeTrie(importTrie$3(data))
];
const headerReg$1 = /^\s*TrieXv(\d+)/m;
function importTrie$2(input) {
	const lines = Array.isArray(input) ? input : typeof input === "string" ? input.split("\n") : [...input];
	function parseHeaderRows(headerRows) {
		for (let i = 0; i < headerRows.length; ++i) {
			const match = headerRows[i].match(headerReg$1);
			if (match) return Number.parseInt(match[1], 10);
		}
		throw new Error("Unknown file format");
	}
	function readHeader(iter) {
		const headerRows = [];
		for (const entry of iter) {
			const line = entry.trim();
			headerRows.push(line);
			if (line === DATA$4 || line === DATA$3) break;
		}
		return headerRows;
	}
	const version$1 = parseHeaderRows(readHeader(lines));
	const method = deserializers$1[version$1];
	if (!method) throw new Error(`Unsupported version: ${version$1}`);
	return method(lines);
}

//#endregion
//#region src/lib/decodeTrie.ts
function decodeTrie(raw) {
	return new ITrieImpl(decodeTrieData(raw));
}

//#endregion
//#region src/lib/io/importExportV3.ts
const specialCharacters = stringToCharSet([
	EOW,
	BACK,
	EOL,
	REF,
	EOR,
	ESCAPE,
	LF,
	"0123456789",
	"`~!@#$%^&*()_-+=[]{};:'\"<>,./?\\|"
].join(""));
const specialCharacterMap = new Map([
	["\n", "\\n"],
	["\r", "\\r"],
	["\\", "\\\\"]
]);
const specialPrefix = stringToCharSet("~!");
const WORDS_PER_LINE = 20;
const DATA = "__DATA__";
function generateHeader(base, comment) {
	return [
		"#!/usr/bin/env cspell-trie reader",
		"TrieXv3",
		"base=" + base,
		...comment ? comment.split("\n").map((a) => "# " + a) : [],
		"# Data:",
		DATA
	].map((a) => a + "\n");
}
/**
* Serialize a TrieRoot.
*/
function serializeTrie$1(root, options = 16) {
	options = typeof options === "number" ? {
		base: options,
		addLineBreaksToImproveDiffs: false
	} : options;
	const { base = 16, comment = "", addLineBreaksToImproveDiffs: addBreaks = true } = options;
	const radix = base > 36 ? 36 : base < 10 ? 10 : base;
	const cache = /* @__PURE__ */ new Map();
	const cacheShouldRef = /* @__PURE__ */ new Map();
	let count = 0;
	const backBuffer = {
		last: "",
		count: 0,
		words: 0,
		eol: false
	};
	const optimizeSimpleReferences = options.optimizeSimpleReferences ?? false;
	const wordChars = [];
	function ref(n) {
		return "#" + n.toString(radix) + ";";
	}
	function escape(s) {
		return s in specialCharacters ? ESCAPE + (specialCharacterMap.get(s) || s) : s;
	}
	function* flush() {
		while (backBuffer.count) {
			const n = Math.min(9, backBuffer.count);
			yield n > 1 ? backBuffer.last + n : backBuffer.last;
			backBuffer.last = BACK;
			backBuffer.count -= n;
		}
		if (backBuffer.eol) {
			yield EOL;
			backBuffer.eol = false;
			backBuffer.words = 0;
		}
	}
	function* emit(s) {
		switch (s) {
			case EOW:
				yield* flush();
				backBuffer.last = EOW;
				backBuffer.count = 0;
				backBuffer.words++;
				break;
			case BACK:
				backBuffer.count++;
				break;
			case EOL:
				backBuffer.eol = true;
				break;
			default:
				if (backBuffer.words >= WORDS_PER_LINE) backBuffer.eol = true;
				yield* flush();
				if (s.startsWith(REF)) backBuffer.words++;
				yield s;
		}
	}
	function* walk$3(node, depth) {
		const r = cache.get(node);
		if (r !== void 0 && (!optimizeSimpleReferences || !shouldSimpleRef(node))) {
			yield* emit(ref(r));
			return;
		}
		if (node.c) {
			if (addBreaks && depth > 0 && depth <= 2) yield* emit(EOL);
			cache.set(node, count++);
			const c = Object.entries(node.c).sort((a, b) => a[0] < b[0] ? -1 : 1);
			for (const [s, n] of c) {
				wordChars[depth] = s;
				yield* emit(escape(s));
				yield* walk$3(n, depth + 1);
				yield* emit(BACK);
				if (depth === 0) yield* emit(EOL);
			}
		}
		if (node.f) yield* emit(EOW);
		if (addBreaks && (depth === 2 || depth === 3 && wordChars[0] in specialPrefix)) yield* emit(EOL);
	}
	function* serialize(node) {
		yield* walk$3(node, 0);
		yield* flush();
	}
	function _calcShouldSimpleRef(node) {
		if (!node.c) return false;
		const values = Object.values(node.c);
		if (values.length !== 1) return false;
		const n = values[0];
		return !!n.f && (!n.c || !Object.values(n.c).length);
	}
	function shouldSimpleRef(node) {
		const r = cacheShouldRef.get(node);
		if (r !== void 0) return r;
		const rr = _calcShouldSimpleRef(node);
		cacheShouldRef.set(node, rr);
		return rr;
	}
	return pipe(generateHeader(radix, comment), opAppend(bufferLines(serialize(root), 1200, "")));
}
function importTrie$1(srcLines) {
	return importTrieV3AsTrieRoot(srcLines).root;
}
function stringToCharSet(values) {
	const set = Object.create(null);
	const len = values.length;
	for (let i = 0; i < len; ++i) set[values[i]] = true;
	return set;
}

//#endregion
//#region src/lib/io/importExport.ts
const serializers = [
	serializeTrie$4,
	serializeTrie$4,
	serializeTrie$3,
	serializeTrie$1,
	serializeTrie$2
];
const deserializers = [
	importTrie$5,
	importTrie$5,
	importTrie$4,
	importTrie$1,
	importTrie$3
];
const DEFAULT_VERSION = 3;
/**
* Serialize a TrieNode.
* Note: This is destructive.  The node will no longer be usable.
* Even though it is possible to preserve the trie, dealing with very large tries can consume a lot of memory.
* Considering this is the last step before exporting, it was decided to let this be destructive.
*/
function serializeTrie(root, options = 16) {
	const version$1 = typeof options !== "number" && options.version ? options.version : DEFAULT_VERSION;
	const method = serializers[version$1];
	if (!method) throw new Error(`Unknown version: ${version$1}`);
	return method(root, options);
}
const headerReg = /^\s*TrieXv(\d+)/m;
function importTrie(input) {
	const lines = Array.isArray(input) ? input : typeof input === "string" ? input.split("\n") : [...input];
	function parseHeaderRows(headerRows) {
		for (let i = 0; i < headerRows.length; ++i) {
			const match = headerRows[i].match(headerReg);
			if (match) return Number.parseInt(match[1], 10);
		}
		throw new Error("Unknown file format");
	}
	function readHeader(iter) {
		const headerRows = [];
		for (const entry of iter) {
			const line = entry.trim();
			headerRows.push(line);
			if (line === DATA$4 || line === DATA$3) break;
		}
		return headerRows;
	}
	const version$1 = parseHeaderRows(readHeader(lines));
	const method = deserializers[version$1];
	if (!method) throw new Error(`Unsupported version: ${version$1}`);
	return method(lines);
}

//#endregion
//#region src/lib/models/locale/knownLocales.ts
const codes = [
	["af", "Afrikaans"],
	[
		"af-NA",
		"Afrikaans",
		"Namibia"
	],
	[
		"af-ZA",
		"Afrikaans",
		"South Africa"
	],
	["ak", "Akan"],
	[
		"ak-GH",
		"Akan",
		"Ghana"
	],
	["am", "Amharic"],
	[
		"am-ET",
		"Amharic",
		"Ethiopia"
	],
	["ar", "Arabic"],
	["ar-1", "Arabic"],
	[
		"ar-AE",
		"Arabic",
		"United Arab Emirates"
	],
	[
		"ar-BH",
		"Arabic",
		"Bahrain"
	],
	[
		"ar-DJ",
		"Arabic",
		"Djibouti"
	],
	[
		"ar-DZ",
		"Arabic",
		"Algeria"
	],
	[
		"ar-EG",
		"Arabic",
		"Egypt"
	],
	["ar-EH", "Arabic"],
	[
		"ar-ER",
		"Arabic",
		"Eritrea"
	],
	[
		"ar-IL",
		"Arabic",
		"Israel"
	],
	[
		"ar-IQ",
		"Arabic",
		"Iraq"
	],
	[
		"ar-JO",
		"Arabic",
		"Jordan"
	],
	[
		"ar-KM",
		"Arabic",
		"Comoros"
	],
	[
		"ar-KW",
		"Arabic",
		"Kuwait"
	],
	[
		"ar-LB",
		"Arabic",
		"Lebanon"
	],
	[
		"ar-LY",
		"Arabic",
		"Libya"
	],
	[
		"ar-MA",
		"Arabic",
		"Morocco"
	],
	[
		"ar-MR",
		"Arabic",
		"Mauritania"
	],
	[
		"ar-OM",
		"Arabic",
		"Oman"
	],
	["ar-PS", "Arabic"],
	[
		"ar-QA",
		"Arabic",
		"Qatar"
	],
	[
		"ar-SA",
		"Arabic",
		"Saudi Arabia"
	],
	[
		"ar-SD",
		"Arabic",
		"Sudan"
	],
	[
		"ar-SO",
		"Arabic",
		"Somalia"
	],
	["ar-SS", "Arabic"],
	[
		"ar-SY",
		"Arabic",
		"Syria"
	],
	[
		"ar-TD",
		"Arabic",
		"Chad"
	],
	[
		"ar-TN",
		"Arabic",
		"Tunisia"
	],
	[
		"ar-YE",
		"Arabic",
		"Yemen"
	],
	["as", "Assamese"],
	[
		"as-IN",
		"Assamese",
		"India"
	],
	["az", "Azerbaijani"],
	[
		"az-AZ",
		"Azerbaijani",
		"Azerbaijan"
	],
	["be", "Belarusian"],
	[
		"be-BY",
		"Belarusian",
		"Belarus"
	],
	["bg", "Bulgarian"],
	[
		"bg-BG",
		"Bulgarian",
		"Bulgaria"
	],
	["bm", "Bambara"],
	[
		"bm-ML",
		"Bambara",
		"Mali"
	],
	["bn", "Bengali"],
	[
		"bn-BD",
		"Bengali",
		"Bangladesh"
	],
	[
		"bn-IN",
		"Bengali",
		"India"
	],
	["bo", "Tibetan"],
	[
		"bo-CN",
		"Tibetan",
		"China"
	],
	[
		"bo-IN",
		"Tibetan",
		"India"
	],
	["br", "Breton"],
	[
		"br-FR",
		"Breton",
		"France"
	],
	["bs", "Bosnian"],
	[
		"bs-BA",
		"Bosnian",
		"Bosnia and Herzegovina"
	],
	["ca", "Catalan"],
	[
		"ca-AD",
		"Catalan",
		"Andorra"
	],
	[
		"ca-ES",
		"Catalan",
		"Spain"
	],
	[
		"ca-FR",
		"Catalan",
		"France"
	],
	[
		"ca-IT",
		"Catalan",
		"Italy"
	],
	["ce", "Chechen"],
	[
		"ce-RU",
		"Chechen",
		"Russia"
	],
	["cs", "Czech"],
	[
		"cs-CZ",
		"Czech",
		"Czech Republic"
	],
	["cu", "Old Slavonic"],
	[
		"cu-RU",
		"Old Slavonic",
		"Russia"
	],
	["cy", "Welsh"],
	[
		"cy-GB",
		"Welsh",
		"United Kingdom"
	],
	["da", "Danish"],
	[
		"da-DK",
		"Danish",
		"Denmark"
	],
	[
		"da-GL",
		"Danish",
		"Greenland"
	],
	["de", "German"],
	[
		"de-AT",
		"German",
		"Austria"
	],
	[
		"de-BE",
		"German",
		"Belgium"
	],
	[
		"de-CH",
		"German",
		"Switzerland"
	],
	[
		"de-DE",
		"German",
		"Germany"
	],
	[
		"de-IT",
		"German",
		"Italy"
	],
	[
		"de-LI",
		"German",
		"Liechtenstein"
	],
	[
		"de-LU",
		"German",
		"Luxembourg"
	],
	["dz", "Dzongkha"],
	[
		"dz-BT",
		"Dzongkha",
		"Bhutan"
	],
	["ee", "Ewe"],
	[
		"ee-GH",
		"Ewe",
		"Ghana"
	],
	[
		"ee-TG",
		"Ewe",
		"Togo"
	],
	[
		"el",
		"Greek",
		"Modern (1453-)"
	],
	[
		"el-CY",
		"Greek",
		"Cyprus"
	],
	[
		"el-GR",
		"Greek",
		"Greece"
	],
	["en", "English"],
	[
		"en-AG",
		"English",
		"Antigua and Barbuda"
	],
	[
		"en-AI",
		"English",
		"Anguilla"
	],
	[
		"en-AS",
		"English",
		"American Samoa"
	],
	[
		"en-AT",
		"English",
		"Austria"
	],
	[
		"en-AU",
		"English",
		"Australia"
	],
	[
		"en-BB",
		"English",
		"Barbados"
	],
	[
		"en-BE",
		"English",
		"Belgium"
	],
	[
		"en-BI",
		"English",
		"Burundi"
	],
	[
		"en-BM",
		"English",
		"Bermuda"
	],
	[
		"en-BS",
		"English",
		"Bahamas"
	],
	[
		"en-BW",
		"English",
		"Botswana"
	],
	[
		"en-BZ",
		"English",
		"Belize"
	],
	[
		"en-CA",
		"English",
		"Canada"
	],
	[
		"en-CC",
		"English",
		"Cocos (Keeling) Islands"
	],
	[
		"en-CH",
		"English",
		"Switzerland"
	],
	[
		"en-CK",
		"English",
		"Cook Islands"
	],
	[
		"en-CM",
		"English",
		"Cameroon"
	],
	[
		"en-CX",
		"English",
		"Christmas Island"
	],
	[
		"en-CY",
		"English",
		"Cyprus"
	],
	[
		"en-DE",
		"English",
		"Germany"
	],
	["en-DG", "English"],
	[
		"en-DK",
		"English",
		"Denmark"
	],
	[
		"en-DM",
		"English",
		"Dominica"
	],
	[
		"en-ER",
		"English",
		"Eritrea"
	],
	[
		"en-FI",
		"English",
		"Finland"
	],
	[
		"en-FJ",
		"English",
		"Fiji"
	],
	[
		"en-FK",
		"English",
		"Falkland Islands (Islas Malvinas)"
	],
	[
		"en-FM",
		"English",
		"Micronesia"
	],
	[
		"en-GB",
		"English",
		"United Kingdom"
	],
	[
		"en-GD",
		"English",
		"Grenada"
	],
	[
		"en-GG",
		"English",
		"Guernsey"
	],
	[
		"en-GH",
		"English",
		"Ghana"
	],
	[
		"en-GI",
		"English",
		"Gibraltar"
	],
	[
		"en-GM",
		"English",
		"Gambia"
	],
	[
		"en-GU",
		"English",
		"Guam"
	],
	[
		"en-GY",
		"English",
		"Guyana"
	],
	[
		"en-HK",
		"English",
		"Hong Kong"
	],
	[
		"en-IE",
		"English",
		"Ireland"
	],
	[
		"en-IL",
		"English",
		"Israel"
	],
	[
		"en-IM",
		"English",
		"Isle of Man"
	],
	[
		"en-IN",
		"English",
		"India"
	],
	[
		"en-IO",
		"English",
		"British Indian Ocean Territory"
	],
	[
		"en-JE",
		"English",
		"Jersey"
	],
	[
		"en-JM",
		"English",
		"Jamaica"
	],
	[
		"en-KE",
		"English",
		"Kenya"
	],
	[
		"en-KI",
		"English",
		"Kiribati"
	],
	[
		"en-KN",
		"English",
		"Saint Kitts and Nevis"
	],
	[
		"en-KY",
		"English",
		"Cayman Islands"
	],
	[
		"en-LC",
		"English",
		"Saint Lucia"
	],
	[
		"en-LR",
		"English",
		"Liberia"
	],
	[
		"en-LS",
		"English",
		"Lesotho"
	],
	[
		"en-MG",
		"English",
		"Madagascar"
	],
	[
		"en-MH",
		"English",
		"Marshall Islands"
	],
	[
		"en-MO",
		"English",
		"Macau"
	],
	[
		"en-MP",
		"English",
		"Northern Mariana Islands"
	],
	[
		"en-MS",
		"English",
		"Montserrat"
	],
	[
		"en-MT",
		"English",
		"Malta"
	],
	[
		"en-MU",
		"English",
		"Mauritius"
	],
	[
		"en-MW",
		"English",
		"Malawi"
	],
	[
		"en-MY",
		"English",
		"Malaysia"
	],
	[
		"en-NA",
		"English",
		"Namibia"
	],
	[
		"en-NF",
		"English",
		"Norfolk Island"
	],
	[
		"en-NG",
		"English",
		"Nigeria"
	],
	[
		"en-NL",
		"English",
		"Netherlands"
	],
	[
		"en-NR",
		"English",
		"Nauru"
	],
	[
		"en-NU",
		"English",
		"Niue"
	],
	[
		"en-NZ",
		"English",
		"New Zealand"
	],
	[
		"en-PG",
		"English",
		"Papua New Guinea"
	],
	[
		"en-PH",
		"English",
		"Philippines"
	],
	[
		"en-PK",
		"English",
		"Pakistan"
	],
	[
		"en-PN",
		"English",
		"Pitcairn Islands"
	],
	[
		"en-PR",
		"English",
		"Puerto Rico"
	],
	[
		"en-PW",
		"English",
		"Palau"
	],
	[
		"en-RW",
		"English",
		"Rwanda"
	],
	[
		"en-SB",
		"English",
		"Solomon Islands"
	],
	[
		"en-SC",
		"English",
		"Seychelles"
	],
	[
		"en-SD",
		"English",
		"Sudan"
	],
	[
		"en-SE",
		"English",
		"Sweden"
	],
	[
		"en-SG",
		"English",
		"Singapore"
	],
	[
		"en-SH",
		"English",
		"Saint Helena"
	],
	[
		"en-SI",
		"English",
		"Slovenia"
	],
	[
		"en-SL",
		"English",
		"Sierra Leone"
	],
	["en-SS", "English"],
	["en-SX", "English"],
	[
		"en-SZ",
		"English",
		"Swaziland"
	],
	[
		"en-TC",
		"English",
		"Turks and Caicos Islands"
	],
	[
		"en-TK",
		"English",
		"Tokelau"
	],
	[
		"en-TO",
		"English",
		"Tonga"
	],
	[
		"en-TT",
		"English",
		"Trinidad and Tobago"
	],
	[
		"en-TV",
		"English",
		"Tuvalu"
	],
	[
		"en-TZ",
		"English",
		"Tanzania"
	],
	[
		"en-UG",
		"English",
		"Uganda"
	],
	[
		"en-UM",
		"English",
		"Baker Island"
	],
	[
		"en-US",
		"English",
		"United States"
	],
	[
		"en-VC",
		"English",
		"Saint Vincent and the Grenadines"
	],
	[
		"en-VG",
		"English",
		"British Virgin Islands"
	],
	[
		"en-VI",
		"English",
		"U.S. Virgin Islands"
	],
	[
		"en-VU",
		"English",
		"Vanuatu"
	],
	[
		"en-WS",
		"English",
		"Samoa"
	],
	[
		"en-ZA",
		"English",
		"South Africa"
	],
	[
		"en-ZM",
		"English",
		"Zambia"
	],
	[
		"en-ZW",
		"English",
		"Zimbabwe"
	],
	["eo", "Esperanto"],
	["es", "Spanish"],
	[
		"es-AR",
		"Spanish",
		"Argentina"
	],
	[
		"es-BO",
		"Spanish",
		"Bolivia"
	],
	[
		"es-BR",
		"Spanish",
		"Brazil"
	],
	[
		"es-BZ",
		"Spanish",
		"Belize"
	],
	[
		"es-CL",
		"Spanish",
		"Chile"
	],
	[
		"es-CO",
		"Spanish",
		"Colombia"
	],
	[
		"es-CR",
		"Spanish",
		"Costa Rica"
	],
	[
		"es-CU",
		"Spanish",
		"Cuba"
	],
	[
		"es-DO",
		"Spanish",
		"Dominican Republic"
	],
	["es-EA", "Spanish"],
	[
		"es-EC",
		"Spanish",
		"Ecuador"
	],
	[
		"es-ES",
		"Spanish",
		"Spain"
	],
	[
		"es-GQ",
		"Spanish",
		"Equatorial Guinea"
	],
	[
		"es-GT",
		"Spanish",
		"Guatemala"
	],
	[
		"es-HN",
		"Spanish",
		"Honduras"
	],
	["es-IC", "Spanish"],
	[
		"es-MX",
		"Spanish",
		"Mexico"
	],
	[
		"es-NI",
		"Spanish",
		"Nicaragua"
	],
	[
		"es-PA",
		"Spanish",
		"Panama"
	],
	[
		"es-PE",
		"Spanish",
		"Peru"
	],
	[
		"es-PH",
		"Spanish",
		"Philippines"
	],
	[
		"es-PR",
		"Spanish",
		"Puerto Rico"
	],
	[
		"es-PY",
		"Spanish",
		"Paraguay"
	],
	[
		"es-SV",
		"Spanish",
		"El Salvador"
	],
	[
		"es-US",
		"Spanish",
		"United States"
	],
	[
		"es-UY",
		"Spanish",
		"Uruguay"
	],
	[
		"es-VE",
		"Spanish",
		"Venezuela"
	],
	["et", "Estonian"],
	[
		"et-EE",
		"Estonian",
		"Estonia"
	],
	["eu", "Basque"],
	[
		"eu-ES",
		"Basque",
		"Spain"
	],
	["fa", "Persian"],
	[
		"fa-AF",
		"Persian",
		"Afghanistan"
	],
	[
		"fa-IR",
		"Persian",
		"Iran"
	],
	["ff", "Fulah"],
	[
		"ff-CM",
		"Fulah",
		"Cameroon"
	],
	[
		"ff-GN",
		"Fulah",
		"Guinea"
	],
	[
		"ff-MR",
		"Fulah",
		"Mauritania"
	],
	[
		"ff-SN",
		"Fulah",
		"Senegal"
	],
	["fi", "Finnish"],
	[
		"fi-FI",
		"Finnish",
		"Finland"
	],
	["fo", "Faroese"],
	[
		"fo-DK",
		"Faroese",
		"Denmark"
	],
	[
		"fo-FO",
		"Faroese",
		"Faroe Islands"
	],
	["fr", "French"],
	[
		"fr-BE",
		"French",
		"Belgium"
	],
	[
		"fr-BF",
		"French",
		"Burkina Faso"
	],
	[
		"fr-BI",
		"French",
		"Burundi"
	],
	[
		"fr-BJ",
		"French",
		"Benin"
	],
	["fr-BL", "French"],
	[
		"fr-CA",
		"French",
		"Canada"
	],
	[
		"fr-CD",
		"French",
		"Congo"
	],
	[
		"fr-CF",
		"French",
		"Central African Republic"
	],
	[
		"fr-CG",
		"French",
		"Congo"
	],
	[
		"fr-CH",
		"French",
		"Switzerland"
	],
	["fr-CI", "French, Cote d'Ivoire (Ivory Coast)"],
	[
		"fr-CM",
		"French",
		"Cameroon"
	],
	[
		"fr-DJ",
		"French",
		"Djibouti"
	],
	[
		"fr-DZ",
		"French",
		"Algeria"
	],
	[
		"fr-FR",
		"French",
		"France"
	],
	[
		"fr-GA",
		"French",
		"Gabon"
	],
	[
		"fr-GF",
		"French",
		"French Guiana"
	],
	[
		"fr-GN",
		"French",
		"Guinea"
	],
	[
		"fr-GP",
		"French",
		"Saint Barthelemy"
	],
	[
		"fr-GQ",
		"French",
		"Equatorial Guinea"
	],
	[
		"fr-HT",
		"French",
		"Haiti"
	],
	[
		"fr-KM",
		"French",
		"Comoros"
	],
	[
		"fr-LU",
		"French",
		"Luxembourg"
	],
	[
		"fr-MA",
		"French",
		"Morocco"
	],
	[
		"fr-MC",
		"French",
		"Monaco"
	],
	["fr-MF", "French"],
	[
		"fr-MG",
		"French",
		"Madagascar"
	],
	[
		"fr-ML",
		"French",
		"Mali"
	],
	[
		"fr-MQ",
		"French",
		"Martinique"
	],
	[
		"fr-MR",
		"French",
		"Mauritania"
	],
	[
		"fr-MU",
		"French",
		"Mauritius"
	],
	[
		"fr-NC",
		"French",
		"New Caledonia"
	],
	[
		"fr-NE",
		"French",
		"Niger"
	],
	[
		"fr-PF",
		"French",
		"French Polynesia"
	],
	[
		"fr-PM",
		"French",
		"Saint Pierre and Miquelon"
	],
	[
		"fr-RE",
		"French",
		"Reunion"
	],
	[
		"fr-RW",
		"French",
		"Rwanda"
	],
	[
		"fr-SC",
		"French",
		"Seychelles"
	],
	[
		"fr-SN",
		"French",
		"Senegal"
	],
	[
		"fr-SY",
		"French",
		"Syria"
	],
	[
		"fr-TD",
		"French",
		"Chad"
	],
	[
		"fr-TG",
		"French",
		"Togo"
	],
	[
		"fr-TN",
		"French",
		"Tunisia"
	],
	[
		"fr-VU",
		"French",
		"Vanuatu"
	],
	[
		"fr-WF",
		"French",
		"Wallis and Futuna"
	],
	[
		"fr-YT",
		"French",
		"Mayotte"
	],
	["fy", "Western Frisian"],
	[
		"fy-NL",
		"Western Frisian",
		"Netherlands"
	],
	["ga", "Irish"],
	[
		"ga-IE",
		"Irish",
		"Ireland"
	],
	["gd", "Gaelic"],
	[
		"gd-GB",
		"Gaelic",
		"United Kingdom"
	],
	["gl", "Galician"],
	[
		"gl-ES",
		"Galician",
		"Spain"
	],
	["gu", "Gujarati"],
	[
		"gu-IN",
		"Gujarati",
		"India"
	],
	["gv", "Manx"],
	[
		"gv-IM",
		"Manx",
		"Isle of Man"
	],
	["ha", "Hausa"],
	[
		"ha-GH",
		"Hausa",
		"Ghana"
	],
	[
		"ha-NE",
		"Hausa",
		"Niger"
	],
	[
		"ha-NG",
		"Hausa",
		"Nigeria"
	],
	["he", "Hebrew"],
	[
		"he-IL",
		"Hebrew",
		"Israel"
	],
	["hi", "Hindi"],
	[
		"hi-IN",
		"Hindi",
		"India"
	],
	["hr", "Croatian"],
	[
		"hr-BA",
		"Croatian",
		"Bosnia and Herzegovina"
	],
	[
		"hr-HR",
		"Croatian",
		"Croatia"
	],
	["hu", "Hungarian"],
	[
		"hu-HU",
		"Hungarian",
		"Hungary"
	],
	["hy", "Armenian"],
	[
		"hy-AM",
		"Armenian",
		"Armenia"
	],
	["id", "Indonesian"],
	[
		"id-ID",
		"Indonesian",
		"Indonesia"
	],
	["ig", "Igbo"],
	[
		"ig-NG",
		"Igbo",
		"Nigeria"
	],
	["ii", "Sichuan Yi"],
	[
		"ii-CN",
		"Sichuan Yi",
		"China"
	],
	["is", "Icelandic"],
	[
		"is-IS",
		"Icelandic",
		"Iceland"
	],
	["it", "Italian"],
	[
		"it-CH",
		"Italian",
		"Switzerland"
	],
	[
		"it-IT",
		"Italian",
		"Italy"
	],
	[
		"it-SM",
		"Italian",
		"San Marino"
	],
	[
		"it-VA",
		"Italian",
		"Vatican City"
	],
	["ja", "Japanese"],
	[
		"ja-JP",
		"Japanese",
		"Japan"
	],
	["ka", "Georgian"],
	[
		"ka-GE",
		"Georgian",
		"Georgia"
	],
	["ki", "Kikuyu"],
	[
		"ki-KE",
		"Kikuyu",
		"Kenya"
	],
	["kk", "Kazakh"],
	[
		"kk-KZ",
		"Kazakh",
		"Kazakhstan"
	],
	["kl", "Kalaallisut"],
	[
		"kl-GL",
		"Kalaallisut",
		"Greenland"
	],
	["km", "Central Khmer"],
	[
		"km-KH",
		"Central Khmer",
		"Cambodia"
	],
	["kn", "Kannada"],
	[
		"kn-IN",
		"Kannada",
		"India"
	],
	["ko", "Korean"],
	[
		"ko-KP",
		"Korean",
		"Korea"
	],
	[
		"ko-KR",
		"Korean",
		"Korea"
	],
	["ks", "Kashmiri"],
	[
		"ks-IN",
		"Kashmiri",
		"India"
	],
	["kw", "Cornish"],
	[
		"kw-GB",
		"Cornish",
		"United Kingdom"
	],
	["ky", "Kirghiz"],
	[
		"ky-KG",
		"Kirghiz",
		"Kyrgyzstan"
	],
	["lb", "Luxembourgish"],
	[
		"lb-LU",
		"Luxembourgish",
		"Luxembourg"
	],
	["lg", "Ganda"],
	[
		"lg-UG",
		"Ganda",
		"Uganda"
	],
	["ln", "Lingala"],
	[
		"ln-AO",
		"Lingala",
		"Angola"
	],
	[
		"ln-CD",
		"Lingala",
		"Congo"
	],
	[
		"ln-CF",
		"Lingala",
		"Central African Republic"
	],
	[
		"ln-CG",
		"Lingala",
		"Congo"
	],
	["lo", "Lao"],
	[
		"lo-LA",
		"Lao",
		"Laos"
	],
	["lt", "Lithuanian"],
	[
		"lt-LT",
		"Lithuanian",
		"Lithuania"
	],
	["lu", "Luba-Katanga"],
	[
		"lu-CD",
		"Luba-Katanga",
		"Congo"
	],
	["lv", "Latvian"],
	[
		"lv-LV",
		"Latvian",
		"Latvia"
	],
	["mg", "Malagasy"],
	[
		"mg-MG",
		"Malagasy",
		"Madagascar"
	],
	["mk", "Macedonian"],
	[
		"mk-MK",
		"Macedonian",
		"Macedonia"
	],
	["ml", "Malayalam"],
	[
		"ml-IN",
		"Malayalam",
		"India"
	],
	["mn", "Mongolian"],
	[
		"mn-MN",
		"Mongolian",
		"Mongolia"
	],
	["mr", "Marathi"],
	[
		"mr-IN",
		"Marathi",
		"India"
	],
	["ms", "Malay"],
	[
		"ms-BN",
		"Malay",
		"Brunei"
	],
	[
		"ms-MY",
		"Malay",
		"Malaysia"
	],
	[
		"ms-SG",
		"Malay",
		"Singapore"
	],
	["mt", "Maltese"],
	[
		"mt-MT",
		"Maltese",
		"Malta"
	],
	["my", "Burmese"],
	[
		"my-MM",
		"Burmese",
		"Myanmar (Burma)"
	],
	["nb", "Bokmål Norwegian"],
	[
		"nb-NO",
		"Bokmål Norwegian",
		"Norway"
	],
	[
		"nb-SJ",
		"Bokmål Norwegian",
		"Svalbard"
	],
	["nd", "Ndebele, North"],
	[
		"nd-ZW",
		"Ndebele, North",
		"Zimbabwe"
	],
	["ne", "Nepali"],
	[
		"ne-IN",
		"Nepali",
		"India"
	],
	[
		"ne-NP",
		"Nepali",
		"Nepal"
	],
	["nl", "Dutch"],
	[
		"nl-AW",
		"Dutch",
		"Aruba"
	],
	[
		"nl-BE",
		"Dutch",
		"Belgium"
	],
	["nl-BQ", "Dutch"],
	["nl-CW", "Dutch"],
	[
		"nl-NL",
		"Dutch",
		"Netherlands"
	],
	[
		"nl-SR",
		"Dutch",
		"Suriname"
	],
	["nl-SX", "Dutch"],
	["nn", "Norwegian Nynorsk"],
	[
		"nn-NO",
		"Norwegian Nynorsk",
		"Norway"
	],
	["om", "Oromo"],
	[
		"om-ET",
		"Oromo",
		"Ethiopia"
	],
	[
		"om-KE",
		"Oromo",
		"Kenya"
	],
	["or", "Oriya"],
	[
		"or-IN",
		"Oriya",
		"India"
	],
	["os", "Ossetian"],
	[
		"os-GE",
		"Ossetian",
		"Georgia"
	],
	[
		"os-RU",
		"Ossetian",
		"Russia"
	],
	["pa", "Panjabi"],
	[
		"pa-IN",
		"Panjabi",
		"India"
	],
	[
		"pa-PK",
		"Panjabi",
		"Pakistan"
	],
	["pl", "Polish"],
	[
		"pl-PL",
		"Polish",
		"Poland"
	],
	["ps", "Pushto"],
	[
		"ps-AF",
		"Pushto",
		"Afghanistan"
	],
	["pt", "Portuguese"],
	[
		"pt-AO",
		"Portuguese",
		"Angola"
	],
	[
		"pt-BR",
		"Portuguese",
		"Brazil"
	],
	[
		"pt-CH",
		"Portuguese",
		"Switzerland"
	],
	[
		"pt-CV",
		"Portuguese",
		"Cape Verde"
	],
	[
		"pt-GQ",
		"Portuguese",
		"Equatorial Guinea"
	],
	[
		"pt-GW",
		"Portuguese",
		"Guinea-Bissau"
	],
	[
		"pt-LU",
		"Portuguese",
		"Luxembourg"
	],
	[
		"pt-MO",
		"Portuguese",
		"Macau"
	],
	[
		"pt-MZ",
		"Portuguese",
		"Mozambique"
	],
	[
		"pt-PT",
		"Portuguese",
		"Portugal"
	],
	[
		"pt-ST",
		"Portuguese",
		"Sao Tome and Principe"
	],
	[
		"pt-TL",
		"Portuguese",
		"Timor-Leste (East Timor)"
	],
	["qu", "Quechua"],
	[
		"qu-BO",
		"Quechua",
		"Bolivia"
	],
	[
		"qu-EC",
		"Quechua",
		"Ecuador"
	],
	[
		"qu-PE",
		"Quechua",
		"Peru"
	],
	["rm", "Romansh"],
	[
		"rm-CH",
		"Romansh",
		"Switzerland"
	],
	["rn", "Rundi"],
	[
		"rn-BI",
		"Rundi",
		"Burundi"
	],
	["ro", "Romanian"],
	[
		"ro-MD",
		"Romanian",
		"Moldova"
	],
	[
		"ro-RO",
		"Romanian",
		"Romania"
	],
	["ru", "Russian"],
	[
		"ru-BY",
		"Russian",
		"Belarus"
	],
	[
		"ru-KG",
		"Russian",
		"Kyrgyzstan"
	],
	[
		"ru-KZ",
		"Russian",
		"Kazakhstan"
	],
	[
		"ru-MD",
		"Russian",
		"Moldova"
	],
	[
		"ru-RU",
		"Russian",
		"Russia"
	],
	[
		"ru-UA",
		"Russian",
		"Ukraine"
	],
	["rw", "Kinyarwanda"],
	[
		"rw-RW",
		"Kinyarwanda",
		"Rwanda"
	],
	["se", "Northern Sami"],
	[
		"se-FI",
		"Northern Sami",
		"Finland"
	],
	[
		"se-NO",
		"Northern Sami",
		"Norway"
	],
	[
		"se-SE",
		"Northern Sami",
		"Sweden"
	],
	["sg", "Sango"],
	[
		"sg-CF",
		"Sango",
		"Central African Republic"
	],
	["si", "Sinhala"],
	[
		"si-LK",
		"Sinhala",
		"Sri Lanka"
	],
	["sk", "Slovak"],
	[
		"sk-SK",
		"Slovak",
		"Slovakia"
	],
	["sl", "Slovenian"],
	[
		"sl-SI",
		"Slovenian",
		"Slovenia"
	],
	["sn", "Shona"],
	[
		"sn-ZW",
		"Shona",
		"Zimbabwe"
	],
	["so", "Somali"],
	[
		"so-DJ",
		"Somali",
		"Djibouti"
	],
	[
		"so-ET",
		"Somali",
		"Ethiopia"
	],
	[
		"so-KE",
		"Somali",
		"Kenya"
	],
	[
		"so-SO",
		"Somali",
		"Somalia"
	],
	["sq", "Albanian"],
	[
		"sq-AL",
		"Albanian",
		"Albania"
	],
	[
		"sq-MK",
		"Albanian",
		"Macedonia"
	],
	["sq-XK", "Albanian"],
	["sr", "Serbian"],
	[
		"sr-BA",
		"Serbian",
		"Bosnia and Herzegovina"
	],
	[
		"sr-ME",
		"Serbian",
		"Montenegro"
	],
	[
		"sr-RS",
		"Serbian",
		"Serbia"
	],
	["sr-XK", "Serbian"],
	["sv", "Swedish"],
	[
		"sv-AX",
		"Swedish",
		"Aland"
	],
	[
		"sv-FI",
		"Swedish",
		"Finland"
	],
	[
		"sv-SE",
		"Swedish",
		"Sweden"
	],
	["sw", "Swahili"],
	[
		"sw-CD",
		"Swahili",
		"Congo"
	],
	[
		"sw-KE",
		"Swahili",
		"Kenya"
	],
	[
		"sw-TZ",
		"Swahili",
		"Tanzania"
	],
	[
		"sw-UG",
		"Swahili",
		"Uganda"
	],
	["ta", "Tamil"],
	[
		"ta-IN",
		"Tamil",
		"India"
	],
	[
		"ta-LK",
		"Tamil",
		"Sri Lanka"
	],
	[
		"ta-MY",
		"Tamil",
		"Malaysia"
	],
	[
		"ta-SG",
		"Tamil",
		"Singapore"
	],
	["te", "Telugu"],
	[
		"te-IN",
		"Telugu",
		"India"
	],
	["th", "Thai"],
	[
		"th-TH",
		"Thai",
		"Thailand"
	],
	["ti", "Tigrinya"],
	[
		"ti-ER",
		"Tigrinya",
		"Eritrea"
	],
	[
		"ti-ET",
		"Tigrinya",
		"Ethiopia"
	],
	["tk", "Turkmen"],
	[
		"tk-TM",
		"Turkmen",
		"Turkmenistan"
	],
	["to", "Tonga (Tonga Islands)"],
	[
		"to-TO",
		"Tonga (Tonga Islands)",
		"Tonga"
	],
	["tr", "Turkish"],
	[
		"tr-CY",
		"Turkish",
		"Cyprus"
	],
	[
		"tr-TR",
		"Turkish",
		"Turkey"
	],
	["ug", "Uighur"],
	[
		"ug-CN",
		"Uighur",
		"China"
	],
	["uk", "Ukrainian"],
	[
		"uk-UA",
		"Ukrainian",
		"Ukraine"
	],
	["ur", "Urdu"],
	[
		"ur-IN",
		"Urdu",
		"India"
	],
	[
		"ur-PK",
		"Urdu",
		"Pakistan"
	],
	["uz", "Uzbek"],
	[
		"uz-AF",
		"Uzbek",
		"Afghanistan"
	],
	[
		"uz-UZ",
		"Uzbek",
		"Uzbekistan"
	],
	["vi", "Vietnamese"],
	[
		"vi-VN",
		"Vietnamese",
		"Vietnam"
	],
	["vo", "Volapük"],
	["yi", "Yiddish"],
	["yi-1", "Yiddish"],
	["yo", "Yoruba"],
	[
		"yo-BJ",
		"Yoruba",
		"Benin"
	],
	[
		"yo-NG",
		"Yoruba",
		"Nigeria"
	],
	["zh", "Chinese"],
	[
		"zh-CN",
		"Chinese",
		"China"
	],
	[
		"zh-HK",
		"Chinese",
		"Hong Kong"
	],
	[
		"zh-MO",
		"Chinese",
		"Macau"
	],
	[
		"zh-SG",
		"Chinese",
		"Singapore"
	],
	[
		"zh-TW",
		"Chinese",
		"China"
	],
	["zu", "Zulu"],
	[
		"zu-ZA",
		"Zulu",
		"South Africa"
	]
];

//#endregion
//#region src/lib/models/locale/locale.ts
let codesByLocale;
var Locale = class {
	_raw;
	_locale;
	constructor(locale) {
		this._raw = locale;
		this._locale = normalizeLocale(locale);
	}
	get locale() {
		return this._locale;
	}
	localInfo() {
		return lookupLocaleInfo(this._locale);
	}
	isValid() {
		return isStandardLocale(this._locale);
	}
	toJSON() {
		return this.locale;
	}
	toString() {
		return this.locale;
	}
};
const regExTwoLetter = /^[a-z]{2}$/i;
const regExLocaleWithCountry = /^([a-z]{2})[_-]?([a-z]{2,3})$/i;
const regExValidLocale = /^([a-z]{2})(?:-([A-Z]{2,3}))?$/;
/**
* Attempt to normalize a locale.
* @param locale a locale string
*/
function normalizeLocale(locale) {
	locale = locale.trim();
	if (regExTwoLetter.test(locale)) return locale.toLowerCase();
	const m = locale.match(regExLocaleWithCountry);
	if (!m) return locale;
	return `${m[1].toLowerCase()}-${m[2].toUpperCase()}`;
}
function isStandardLocale(locale) {
	return regExValidLocale.test(locale);
}
function lookupLocaleInfo(locale) {
	codesByLocale = codesByLocale || buildLocaleLookup();
	return codesByLocale.get(locale);
}
function buildLocaleLookup() {
	const info = codes.map(([locale, language, country]) => ({
		locale,
		language,
		country
	}));
	return new Map(info.map((i) => [i.locale, i]));
}
function createLocale(locale) {
	return new Locale(locale);
}
function parseLocale(locales) {
	locales = typeof locales === "string" ? locales.split(",") : locales;
	return locales.map(createLocale);
}

//#endregion
//#region src/lib/mappers/mapCosts.ts
const defaultEditCosts = {
	accentCosts: 1,
	baseCost: 100,
	capsCosts: 1,
	firstLetterPenalty: 4,
	nonAlphabetCosts: 110
};
const defaultHunspellCosts = {
	...defaultEditCosts,
	ioConvertCost: 30,
	keyboardCost: 99,
	mapCost: 25,
	replaceCosts: 75,
	tryCharCost: 100
};
function mapHunspellCosts(costs = {}) {
	return {
		...defaultHunspellCosts,
		...cleanCopy(costs)
	};
}
function mapEditCosts(costs = {}) {
	return {
		...defaultEditCosts,
		...cleanCopy(costs)
	};
}

//#endregion
//#region src/lib/mappers/joinLetters.ts
/**
* Bring letters / strings together.
* - `['a', 'b'] => 'ab'`
* - `['a', 'bc'] => 'a(bc)'`
* @param letters - letters to join
*/
function joinLetters(letters) {
	return [...letters].map((a) => a.length > 1 || !a.length ? `(${a})` : a).join("");
}

//#endregion
//#region src/lib/mappers/mapToSuggestionCostDef.ts
function parseAlphabet(cs, locale, editCost) {
	const { cost, penalty } = cs;
	const alphabet = joinLetters([...pipe([...pipe(expandCharacterSet(cs.characters), opMap((c) => caseForms(c, locale).sort()))], opFlatten(), opMap((letter) => accentForms(letter)), opFlatten(), opUnique())].sort());
	return [
		clean$1({
			map: alphabet,
			replace: cost,
			insDel: cost,
			swap: cost,
			penalty
		}),
		parseAlphabetCaps(cs.characters, locale, editCost),
		...calcCostsForAccentedLetters(alphabet, locale, editCost)
	];
}
function parseAlphabetCaps(alphabet, locale, editCost) {
	return {
		map: [...pipe(expandCharacterSet(alphabet), opMap((c) => caseForms(c, locale).sort()))].map((a) => joinLetters(a)).join("|"),
		replace: editCost.capsCosts
	};
}
function calcFirstCharacterReplaceDefs(alphabets, editCost) {
	return alphabets.map((cs) => calcFirstCharacterReplace(cs, editCost));
}
function calcFirstCharacterReplace(cs, editCost) {
	const mapOfFirstLetters = [...pipe(expandCharacterSet(cs.characters), opUnique(), opMap((letter) => `(^${letter})`))].sort().join("") + "(^)";
	const penalty = editCost.firstLetterPenalty;
	return {
		map: mapOfFirstLetters,
		replace: cs.cost - penalty,
		penalty: penalty * 2
	};
}
function parseAccents(cs, _editCost) {
	const { cost, penalty } = cs;
	const accents = joinLetters([...pipe(expandCharacterSet(cs.characters), opMap((char) => stripNonAccents(char)))]);
	if (!accents) return void 0;
	return clean$1({
		map: accents,
		replace: cost,
		insDel: cost,
		penalty
	});
}
function calcCostsForAccentedLetters(simpleMap, locale, costs) {
	const charactersWithAccents = [...pipe(splitMap(simpleMap), opMap((char) => caseForms(char, locale)), opFlatten(), opMap((char) => [...accentForms(char)]), opFilter((forms$1) => forms$1.length > 1))];
	const replaceAccentMap = [...pipe(charactersWithAccents, opMap((forms$1) => new Set([...forms$1, ...forms$1.map((char) => stripAccents(char))])), opMap((forms$1) => [...forms$1].sort()), opFilter((forms$1) => forms$1.length > 1), opMap(joinLetters), opUnique())].join("|");
	const cost = costs.accentCosts;
	const costToReplaceAccent = !replaceAccentMap ? [] : [{
		map: replaceAccentMap,
		replace: cost
	}];
	const normalizeMap$1 = charactersWithAccents.map((a) => a.sort()).map(joinLetters).join("|");
	const costToNormalizeAccent = !normalizeMap$1 ? [] : [{
		map: normalizeMap$1,
		replace: 0
	}];
	return [...costToReplaceAccent, ...costToNormalizeAccent];
}
/**
* Splits a simple map string into its parts.
* - `abc` => `a`, `b`, `c`
* - `a(bc)` => `a`, `bc`
* @param map - string of characters
*/
function* splitMap(map) {
	let seq = "";
	let mode = 0;
	for (const char of map) {
		if (mode && char === ")") {
			yield seq;
			mode = 0;
			continue;
		}
		if (mode) {
			seq += char;
			continue;
		}
		if (char === "(") {
			mode = 1;
			seq = "";
			continue;
		}
		yield char;
	}
}

//#endregion
//#region src/lib/mappers/mapHunspellInformation.ts
function hunspellInformationToSuggestionCostDef(hunInfo, locales) {
	const costs = calcCosts(hunInfo.costs, locales);
	const operations = [
		affKey,
		affKeyCaps,
		affMap,
		affMapAccents,
		affMapCaps,
		affNoTry,
		affRepConv,
		affTry,
		affTryAccents,
		affTryFirstCharacterReplace
	];
	function parseAff(aff, costs$1) {
		const regSupportedAff = /^(?:MAP|KEY|TRY|NO-TRY|ICONV|OCONV|REP)\s/;
		const rejectAff = /^(?:MAP|KEY|TRY|ICONV|OCONV|REP)\s+\d+$/;
		return [...pipe(aff.split("\n").map((a) => a.replace(/#.*/, "")).map((a) => a.trim()).filter((a) => regSupportedAff.test(a)).filter((a) => !rejectAff.test(a)), opMap((line) => pipe(operations, opMap((fn) => fn(line, costs$1)), opMap(asArrayOf), opFlatten())), opFlatten(), opFilter(isDefined$1))];
	}
	return parseAff(hunInfo.aff, costs);
}
function calcCosts(costs = {}, locale) {
	const useLocale = locale?.length ? locale.map((loc) => loc.locale) : void 0;
	return {
		...mapHunspellCosts(costs),
		locale: useLocale
	};
}
const regExpMap = /^(?:MAP)\s+(\S+)$/;
function affMap(line, costs) {
	const m = line.match(regExpMap);
	if (!m) return void 0;
	const map = m[1];
	const cost = costs.mapCost;
	return {
		map,
		replace: cost,
		swap: cost
	};
}
const regExpTry = /^(?:TRY)\s+(\S+)$/;
function affTry(line, costs) {
	const m = line.match(regExpTry);
	if (!m) return void 0;
	const cost = costs.tryCharCost;
	const characters = m[1];
	return parseAlphabet({
		characters,
		cost
	}, costs.locale, costs);
}
function affTryFirstCharacterReplace(line, costs) {
	const m = line.match(regExpTry);
	if (!m) return void 0;
	const characters = m[1];
	const cost = costs.tryCharCost;
	return calcFirstCharacterReplace({
		characters,
		cost
	}, costs);
}
const regExpNoTry = /^NO-TRY\s+(\S+)$/;
function affNoTry(line, costs) {
	const m = line.match(regExpNoTry);
	if (!m) return void 0;
	return {
		map: m[1],
		insDel: Math.max(costs.nonAlphabetCosts - costs.tryCharCost, 0),
		penalty: costs.nonAlphabetCosts + costs.tryCharCost
	};
}
const regExpRepConv = /^(?:REP|(?:I|O)CONV)\s+(\S+)\s+(\S+)$/;
function affRepConv(line, costs) {
	const m = line.match(regExpRepConv);
	if (!m) return void 0;
	const cost = line.startsWith("REP") ? costs.replaceCosts : costs.ioConvertCost;
	const from = m[1];
	let into = m[2];
	into = into.replace(/^0$/, "");
	if (from.startsWith("^") && !into.startsWith("^")) into = "^" + into;
	if (from.endsWith("$") && !into.endsWith("$")) into = into + "$";
	return {
		map: joinLetters([from, into]),
		replace: cost
	};
}
const regExpKey = /^(?:KEY)\s+(\S+)$/;
function affKey(line, costs) {
	const m = line.match(regExpKey);
	if (!m) return void 0;
	const kbd = m[1];
	const pairs = [...splitMap(kbd)].map(reducer((p, v) => ({
		a: p.b,
		b: v
	}), {
		a: "|",
		b: "|"
	})).filter((ab) => ab.a !== "|" && ab.b !== "|").map(({ a, b }) => joinLetters([a, b]));
	const pairsUpper = pairs.map((p) => p.toLocaleUpperCase(costs.locale));
	const map = unique([...pairs, ...pairsUpper]).join("|");
	const cost = costs.keyboardCost;
	return {
		map,
		replace: cost,
		swap: cost
	};
}
function affKeyCaps(line, costs) {
	const m = line.match(regExpKey);
	if (!m) return void 0;
	return parseCaps(m[1], costs);
}
function affMapCaps(line, costs) {
	const m = line.match(regExpMap);
	if (!m) return void 0;
	return parseCaps(m[1], costs);
}
function affTryAccents(line, costs) {
	const m = line.match(regExpTry);
	if (!m) return void 0;
	return calcCostsForAccentedLetters(m[1], costs.locale, costs);
}
function affMapAccents(line, costs) {
	const m = line.match(regExpMap);
	if (!m) return void 0;
	return calcCostsForAccentedLetters(m[1], costs.locale, costs);
}
function parseCaps(value, costs) {
	const locale = costs.locale;
	const map = unique([...splitMap(value)].filter((a) => a !== "|").map((s) => caseForms(s, locale)).filter((forms$1) => forms$1.length > 1).map(joinLetters)).join("|");
	const cost = costs.capsCosts;
	if (!map) return void 0;
	return {
		map,
		replace: cost
	};
}
function reducer(fn, initialVal) {
	let acc = initialVal;
	return (val, i) => acc = fn(acc, val, i);
}
function asArrayOf(v) {
	return Array.isArray(v) ? v : [v];
}

//#endregion
//#region src/lib/mappers/mapDictionaryInfo.ts
function mapDictionaryInformation(dictInfo) {
	const _locale = dictInfo.locale;
	const locale = _locale ? parseLocale(_locale).filter((loc) => loc.isValid()) : void 0;
	const locales = locale?.map((loc) => loc.locale);
	const costs = mapEditCosts(dictInfo.costs);
	const defsEC = dictInfo.suggestionEditCosts || [];
	const defsHI = dictInfo.hunspellInformation ? hunspellInformationToSuggestionCostDef(dictInfo.hunspellInformation, locale) : [];
	return [
		...defsEC,
		...processAlphabet(dictInfo.alphabet, locales, costs),
		...processAccents(dictInfo.accents, costs),
		...defsHI
	];
}
function processAlphabet(alphabet, locale, editCost) {
	const csAlphabet = toCharSets(alphabet, "a-zA-Z", editCost.baseCost);
	return [...pipeSync(csAlphabet, opMap((cs) => parseAlphabet(cs, locale, editCost)), opFlatten()), ...calcFirstCharacterReplaceDefs(csAlphabet, editCost)];
}
function toCharSets(cs, defaultValue, cost, penalty) {
	cs = cs ?? defaultValue;
	if (!cs) return [];
	if (typeof cs === "string") cs = [{
		characters: cs,
		cost
	}];
	if (penalty !== void 0) cs.forEach((cs$1) => cs$1.penalty = penalty);
	return cs;
}
function processAccents(accents, editCost) {
	return toCharSets(accents, "̀-́", editCost.accentCosts).map((cs) => parseAccents(cs, editCost)).filter(isDefined$1);
}
function mapDictionaryInformationToAdjustment(dictInfo) {
	if (!dictInfo.adjustments) return [];
	return dictInfo.adjustments.map(mapAdjustment);
}
function mapAdjustment(adj) {
	const { id, regexp, penalty } = adj;
	return {
		id,
		regexp: new RegExp(regexp),
		penalty
	};
}

//#endregion
//#region src/lib/mappers/mapDictionaryInfoToWeightMap.ts
const defaultDefs = [{
	map: "1234567890-.",
	insDel: 1,
	penalty: 200
}];
const defaultAdjustments = [
	{
		id: "compound-case-change",
		regexp: /\p{Ll}∙\p{Lu}/gu,
		penalty: 1e3
	},
	{
		id: "short-compounds-1",
		regexp: /^[^∙]{0,2}(?=∙)|∙[^∙]{0,2}(?=∙|$)/gm,
		penalty: 100
	},
	{
		id: "short-compounds-3",
		regexp: /^[^∙]{3}(?=∙)|∙[^∙]{3}(?=∙|$)/gm,
		penalty: 50
	}
];
function mapDictionaryInformationToWeightMap(dictInfo) {
	const defs = [...mapDictionaryInformation(dictInfo), ...defaultDefs];
	const adjustments = mapDictionaryInformationToAdjustment(dictInfo);
	const map = createWeightMap(...defs);
	addAdjustment(map, ...defaultAdjustments, ...adjustments);
	return map;
}

//#endregion
//#region src/lib/suggestions/suggest.ts
const baseCost = opCosts.baseCost;
const postSwapCost = opCosts.swapCost - baseCost;
const insertSpaceCost = -1;
const mapSubCost = opCosts.visuallySimilar;
const maxCostScale = opCosts.wordLengthCostFactor;
const discourageInsertCost = baseCost;
const setOfSeparators = new Set([JOIN_SEPARATOR, WORD_SEPARATOR]);
function suggest(root, word, options = {}) {
	const opts = createSuggestionOptions(options);
	const collector = suggestionCollector(word, clean(opts));
	collector.collect(genSuggestions(root, word, {
		...opts,
		...collector.genSuggestionOptions
	}));
	return collector.suggestions;
}
function* genSuggestions(root, word, options = {}) {
	const roots = Array.isArray(root) ? root : [root];
	for (const r of roots) yield* genCompoundableSuggestions(r, word, options);
}
function* genCompoundableSuggestions(root, word, options = {}) {
	const { compoundMethod = CompoundWordsMethod.NONE, changeLimit, ignoreCase } = createSuggestionOptions(options);
	const history = [];
	const historyTags = /* @__PURE__ */ new Map();
	const bc = baseCost;
	const psc = postSwapCost;
	const matrix = [[]];
	const stack = [];
	const x = " " + word;
	const mx = x.length - 1;
	const specialInsCosts = Object.assign(Object.create(null), {
		[WORD_SEPARATOR]: insertSpaceCost,
		[JOIN_SEPARATOR]: insertSpaceCost
	});
	const specialSubCosts = Object.assign(Object.create(null), { "-": discourageInsertCost });
	let stopNow = false;
	let costLimit = bc * Math.min(word.length * maxCostScale, changeLimit);
	function updateCostLimit(maxCost) {
		switch (typeof maxCost) {
			case "number":
				costLimit = maxCost;
				break;
			case "symbol":
				stopNow = true;
				break;
		}
	}
	const a = 0;
	let b = 0;
	for (let i = 0, c = 0; i <= mx && c <= costLimit; ++i) {
		c = i * baseCost;
		matrix[0][i] = c;
		b = i;
	}
	stack[0] = {
		a,
		b
	};
	const iWalk = hintedWalker(root, ignoreCase, word, compoundMethod, options.compoundSeparator);
	let goDeeper = true;
	for (let r = iWalk.next({ goDeeper }); !stopNow && !r.done; r = iWalk.next({ goDeeper })) {
		const { text, node, depth } = r.value;
		let { a: a$1, b: b$1 } = stack[depth];
		/** Current character from word */
		const w = text.slice(-1);
		/** Current character visual letter group */
		const wG = visualLetterMaskMap[w] || 0;
		if (setOfSeparators.has(w)) {
			const mxRange = matrix[depth].slice(a$1, b$1 + 1);
			const mxMin = Math.min(...mxRange);
			const tag = [a$1, ...mxRange.map((c$1) => c$1 - mxMin)].join(",");
			const ht = historyTags.get(tag);
			if (ht && ht.m <= mxMin) {
				goDeeper = false;
				const { i: i$1, w: w$1, m } = ht;
				if (i$1 >= history.length) continue;
				if (history[i$1].word.slice(0, w$1.length) !== w$1) continue;
				const dc = mxMin - m;
				for (let p = i$1; p < history.length; ++p) {
					const { word: word$1, cost: hCost } = history[p];
					if (word$1.slice(0, w$1.length) !== w$1) break;
					const cost$1 = hCost + dc;
					if (cost$1 <= costLimit) updateCostLimit(yield {
						word: text + word$1.slice(w$1.length),
						cost: cost$1
					});
				}
				continue;
			} else historyTags.set(tag, {
				w: text,
				i: history.length,
				m: mxMin
			});
		}
		/** current depth */
		const d = depth + 1;
		const lastSugLetter = d > 1 ? text[d - 2] : "";
		/** standard cost */
		const c = bc - d + (specialSubCosts[w] || 0);
		/** insert cost */
		const ci = c + (specialInsCosts[w] || 0);
		matrix[d] = matrix[d] || [];
		matrix[d][a$1] = matrix[d - 1][a$1] + ci + d - a$1;
		let lastLetter = x[a$1];
		let min = matrix[d][a$1];
		let i;
		for (i = a$1 + 1; i <= b$1; ++i) {
			const curLetter = x[i];
			/** current group */
			const cG = visualLetterMaskMap[curLetter] || 0;
			const subCost = w === curLetter ? 0 : wG & cG ? mapSubCost : curLetter === lastSugLetter ? w === lastLetter ? psc : c : c;
			const e = Math.min(matrix[d - 1][i - 1] + subCost, matrix[d - 1][i] + ci, matrix[d][i - 1] + c);
			min = Math.min(min, e);
			matrix[d][i] = e;
			lastLetter = curLetter;
		}
		const { b: bb } = stack[d - 1];
		while (b$1 < mx) {
			b$1 += 1;
			i = b$1;
			const curLetter = x[i];
			const cG = visualLetterMaskMap[curLetter] || 0;
			const subCost = w === curLetter ? 0 : wG & cG ? mapSubCost : curLetter === lastSugLetter ? w === lastLetter ? psc : c : c;
			const j = Math.min(bb, i - 1);
			const e = Math.min(matrix[d - 1][j] + subCost, matrix[d][i - 1] + c);
			min = Math.min(min, e);
			matrix[d][i] = e;
			lastLetter = curLetter;
			if (e > costLimit) break;
		}
		for (; b$1 > a$1 && matrix[d][b$1] > costLimit; b$1 -= 1);
		for (; a$1 < b$1 && matrix[d][a$1] > costLimit; a$1 += 1);
		b$1 = Math.min(b$1 + 1, mx);
		stack[d] = {
			a: a$1,
			b: b$1
		};
		const cost = matrix[d][b$1];
		if (node.f && isWordTerminationNode(node) && cost <= costLimit) {
			const r$1 = {
				word: text,
				cost
			};
			history.push(r$1);
			updateCostLimit(yield r$1);
		} else updateCostLimit(yield void 0);
		goDeeper = min <= costLimit;
	}
}

//#endregion
//#region src/lib/trie.ts
const defaultLegacyMinCompoundLength = 3;
var Trie = class Trie {
	_options;
	_findOptionsDefaults;
	_findOptionsExact;
	isLegacy;
	hasForbidden;
	constructor(root, count) {
		this.root = root;
		this.count = count;
		this._options = mergeOptionalWithDefaults(root);
		this.isLegacy = this.calcIsLegacy();
		this.hasForbidden = !!root.c[root.forbiddenWordPrefix];
		this._findOptionsDefaults = {
			caseInsensitivePrefix: this._options.stripCaseAndAccentsPrefix,
			compoundFix: this._options.compoundCharacter,
			forbidPrefix: this._options.forbiddenWordPrefix
		};
		this._findOptionsExact = this.createFindOptions({ compoundMode: "none" });
	}
	/**
	* Number of words in the Trie
	*/
	size() {
		this.count = this.count ?? countWords(this.root);
		return this.count;
	}
	isSizeKnown() {
		return this.count !== void 0;
	}
	get options() {
		return this._options;
	}
	/**
	* @param text - text to find in the Trie
	* @param minCompoundLength - deprecated - allows words to be glued together
	*/
	find(text, minCompoundLength = false) {
		const minLength = !minCompoundLength ? void 0 : minCompoundLength === true ? defaultLegacyMinCompoundLength : minCompoundLength;
		const options = this.createFindOptions({
			compoundMode: minLength ? "legacy" : "compound",
			legacyMinCompoundLength: minLength
		});
		return findWordNode(this.root, text, options).node;
	}
	has(word, minLegacyCompoundLength) {
		if (this.hasWord(word, false)) return true;
		if (minLegacyCompoundLength) return !!this.findWord(word, { useLegacyWordCompounds: minLegacyCompoundLength }).found;
		return false;
	}
	/**
	* Determine if a word is in the dictionary.
	* @param word - the exact word to search for - must be normalized.
	* @param caseSensitive - false means also searching a dictionary where the words were normalized to lower case and accents removed.
	* @returns true if the word was found and is not forbidden.
	*/
	hasWord(word, caseSensitive) {
		const f = this.findWord(word, { caseSensitive });
		return !!f.found && !f.forbidden;
	}
	findWord(word, options) {
		if (options?.useLegacyWordCompounds) {
			const len = options.useLegacyWordCompounds !== true ? options.useLegacyWordCompounds : defaultLegacyMinCompoundLength;
			const findOptions$1 = this.createFindOptions({
				legacyMinCompoundLength: len,
				matchCase: options.caseSensitive
			});
			return findLegacyCompound(this.root, word, findOptions$1);
		}
		const findOptions = this.createFindOptionsMatchCase(options?.caseSensitive);
		return findWord(this.root, word, findOptions);
	}
	/**
	* Determine if a word is in the forbidden word list.
	* @param word the word to lookup.
	*/
	isForbiddenWord(word) {
		return this.hasForbidden && isForbiddenWord(this.root, word, this.options.forbiddenWordPrefix);
	}
	/**
	* Provides an ordered sequence of words with the prefix of text.
	*/
	completeWord(text) {
		const n = this.find(text);
		const compoundChar = this.options.compoundCharacter;
		const subNodes = pipe(iteratorTrieWords(n || {}), opFilter((w) => w[w.length - 1] !== compoundChar), opMap((suffix) => text + suffix));
		return pipe(n && isWordTerminationNode(n) ? [text] : [], opAppend(subNodes));
	}
	/**
	* Suggest spellings for `text`.  The results are sorted by edit distance with changes near the beginning of a word having a greater impact.
	* @param text - the text to search for
	* @param maxNumSuggestions - the maximum number of suggestions to return.
	* @param compoundMethod - Use to control splitting words.
	* @param numChanges - the maximum number of changes allowed to text. This is an approximate value, since some changes cost less than others.
	*                      the lower the value, the faster results are returned. Values less than 4 are best.
	*/
	suggest(text, options) {
		return this.suggestWithCost(text, options).map((a) => a.word);
	}
	/**
	* Suggest spellings for `text`.  The results are sorted by edit distance with changes near the beginning of a word having a greater impact.
	* The results include the word and adjusted edit cost.  This is useful for merging results from multiple tries.
	*/
	suggestWithCost(text, options) {
		const sep = options.compoundSeparator;
		const adjWord = sep ? replaceAllFactory(sep, "") : (a) => a;
		const optFilter = options.filter;
		const filter = optFilter ? (word, cost) => {
			const w = adjWord(word);
			return !this.isForbiddenWord(w) && optFilter(w, cost);
		} : (word) => !this.isForbiddenWord(adjWord(word));
		const opts = {
			...options,
			filter
		};
		return suggest(this.root, text, opts);
	}
	/**
	* genSuggestions will generate suggestions and send them to `collector`. `collector` is responsible for returning the max acceptable cost.
	* Costs are measured in weighted changes. A cost of 100 is the same as 1 edit. Some edits are considered cheaper.
	* Returning a MaxCost < 0 will effectively cause the search for suggestions to stop.
	*/
	genSuggestions(collector, compoundMethod) {
		const filter = (word) => !this.isForbiddenWord(word);
		const options = clean({
			compoundMethod,
			...collector.genSuggestionOptions
		});
		const suggestions = genSuggestions(this.root, collector.word, options);
		collector.collect(suggestions, void 0, filter);
	}
	/**
	* Returns an iterator that can be used to get all words in the trie. For some dictionaries, this can result in millions of words.
	*/
	words() {
		return iteratorTrieWords(this.root);
	}
	/**
	* Allows iteration over the entire tree.
	* On the returned Iterator, calling .next(goDeeper: boolean), allows for controlling the depth.
	*/
	iterate() {
		return walker(this.root);
	}
	insert(word) {
		insert(word, this.root);
		return this;
	}
	calcIsLegacy() {
		const c = this.root.c;
		return !(c && c[this._options.compoundCharacter] || c[this._options.stripCaseAndAccentsPrefix] || c[this._options.forbiddenWordPrefix]);
	}
	static create(words, options) {
		const root = createTrieRootFromList(words, options);
		orderTrie(root);
		return new Trie(root, void 0);
	}
	createFindOptions(options = {}) {
		return createFindOptions({
			...this._findOptionsDefaults,
			...options
		});
	}
	lastCreateFindOptionsMatchCaseMap = /* @__PURE__ */ new Map();
	createFindOptionsMatchCase(matchCase) {
		const f = this.lastCreateFindOptionsMatchCaseMap.get(matchCase);
		if (f !== void 0) return f;
		const findOptions = this.createFindOptions({ matchCase });
		this.lastCreateFindOptionsMatchCaseMap.set(matchCase, findOptions);
		return findOptions;
	}
};

//#endregion
//#region src/lib/utils/secondChanceCache.ts
var SecondChanceCache = class {
	map0 = /* @__PURE__ */ new Map();
	map1 = /* @__PURE__ */ new Map();
	constructor(maxL0Size) {
		this.maxL0Size = maxL0Size;
	}
	has(key) {
		if (this.map0.has(key)) return true;
		if (this.map1.has(key)) {
			this.set(key, this.get1(key));
			return true;
		}
		return false;
	}
	get(key) {
		return this.map0.get(key) ?? this.get1(key);
	}
	set(key, value) {
		if (this.map0.size >= this.maxL0Size && !this.map0.has(key)) {
			this.map1 = this.map0;
			this.map0 = /* @__PURE__ */ new Map();
		}
		this.map0.set(key, value);
		return this;
	}
	get size() {
		return this.map0.size + this.map1.size;
	}
	get size0() {
		return this.map0.size;
	}
	get size1() {
		return this.map1.size;
	}
	clear() {
		this.map0.clear();
		this.map1.clear();
		return this;
	}
	get1(key) {
		if (this.map1.has(key)) {
			const v = this.map1.get(key);
			this.map1.delete(key);
			this.map0.set(key, v);
			return v;
		}
	}
	toArray() {
		return [...this.map1, ...this.map0];
	}
};

//#endregion
//#region src/lib/TrieBuilder.ts
/**
* Builds an optimized Trie from a Iterable<string>. It attempts to reduce the size of the trie
* by finding common endings.
* @param words Iterable set of words -- no processing is done on the words, they are inserted as is.
* @param trieOptions options for the Trie
*/
function buildTrie(words, trieOptions) {
	return new TrieBuilder(words, trieOptions).build();
}
/**
* Builds a Trie from a Iterable<string>. NO attempt a reducing the size of the Trie is done.
* @param words Iterable set of words -- no processing is done on the words, they are inserted as is.
* @param trieOptions options for the Trie
*/
function buildTrieFast(words, trieOptions) {
	return new Trie(createTrieRootFromList(words, trieOptions), void 0);
}
const MAX_NUM_SIGS = 1e5;
const MAX_TRANSFORMS = 1e6;
const MAX_CACHE_SIZE = 1e6;
var TrieBuilder = class {
	count = 0;
	signatures = new SecondChanceCache(MAX_NUM_SIGS);
	cached = new SecondChanceCache(MAX_CACHE_SIZE);
	transforms = new SecondChanceCache(MAX_TRANSFORMS);
	_eow;
	/** position 0 of lastPath is always the root */
	lastPath = [{
		s: "",
		n: {
			id: 0,
			f: void 0,
			c: void 0
		}
	}];
	tails = /* @__PURE__ */ new Map();
	trieOptions;
	numWords = 0;
	_debug_lastWordsInserted = [];
	_debug_mode = false;
	constructor(words, trieOptions) {
		this._eow = this.createNodeFrozen(1);
		this.tails.set("", this._eow);
		this._canBeCached(this._eow);
		this.signatures.set(this.signature(this._eow), this._eow);
		this.cached.set(this._eow, this._eow.id ?? ++this.count);
		this.trieOptions = Object.freeze(mergeOptionalWithDefaults(trieOptions));
		if (words) this.insert(words);
	}
	get _root() {
		return trieNodeToRoot(this.lastPath[0].n, this.trieOptions);
	}
	signature(n) {
		const isWord = n.f ? "*" : "";
		const entries = n.c ? Object.entries(n.c) : void 0;
		const c = entries ? entries.map(([k, n$1]) => [k, this.cached.get(n$1)]) : void 0;
		return isWord + (c ? JSON.stringify(c) : "");
	}
	_canBeCached(n) {
		if (!n.c) return true;
		for (const v of Object.values(n.c)) if (!this.cached.has(v)) return false;
		return true;
	}
	tryCacheFrozen(n) {
		assertFrozen(n);
		if (this.cached.has(n)) return n;
		this.cached.set(n, n.id ?? ++this.count);
		return n;
	}
	freeze(n) {
		if (Object.isFrozen(n)) return n;
		// istanbul ignore else
		if (n.c) {
			const c = Object.entries(n.c).sort((a, b) => a[0] < b[0] ? -1 : 1).map(([k, n$1]) => [k, this.freeze(n$1)]);
			n.c = Object.fromEntries(c);
			Object.freeze(n.c);
		}
		return Object.freeze(n);
	}
	tryToCache(n) {
		if (!this._canBeCached(n)) return n;
		const sig = this.signature(n);
		const ref = this.signatures.get(sig);
		if (ref !== void 0) return this.tryCacheFrozen(ref);
		this.signatures.set(sig, this.freeze(n));
		return n;
	}
	storeTransform(src, s, result) {
		if (!Object.isFrozen(result) || !Object.isFrozen(src)) return;
		this.logDebug("storeTransform", () => ({
			s,
			src: this.debNodeInfo(src),
			result: this.debNodeInfo(result)
		}));
		const t = this.transforms.get(src) ?? /* @__PURE__ */ new Map();
		t.set(s, result);
		this.transforms.set(src, t);
	}
	addChild(node, head, child) {
		if (node.c?.[head] !== child) {
			let c = node.c || Object.create(null);
			if (Object.isFrozen(c)) c = Object.assign(Object.create(null), c);
			c[head] = child;
			if (Object.isFrozen(node)) node = this.createNode(node.f, c);
			else node.c = c;
		}
		return Object.isFrozen(child) ? this.tryToCache(node) : node;
	}
	buildTail(s) {
		const ss = s.join("");
		const v = this.tails.get(ss);
		if (v) return v;
		const head = s[0];
		const tail = s.slice(1);
		const t = this.tails.get(tail.join(""));
		const c = t || this.buildTail(tail);
		const n = this.addChild(this.createNode(), head, c);
		if (!t) return n;
		const cachedNode = this.tryCacheFrozen(this.freeze(n));
		this.tails.set(ss, cachedNode);
		return cachedNode;
	}
	_insert(node, s, d) {
		this.logDebug("_insert", () => ({
			n: this.debNodeInfo(node),
			s,
			d,
			w: this.lastPath.map((a) => a.s).join("")
		}));
		const orig = node;
		if (Object.isFrozen(node)) {
			const n = this.transforms.get(node)?.get(s.join(""));
			if (n) return this.tryCacheFrozen(n);
		}
		if (!s.length) if (!node.c) return this._eow;
		else {
			node = this.copyIfFrozen(node);
			node.f = this._eow.f;
			return node;
		}
		const head = s[0];
		const tail = s.slice(1);
		const cNode = node.c?.[head];
		const child = cNode ? this._insert(cNode, tail, d + 1) : this.buildTail(tail);
		node = this.addChild(node, head, child);
		this.storeTransform(orig, s.join(""), node);
		this.lastPath[d] = {
			s: head,
			n: child
		};
		return node;
	}
	insertWord(word) {
		this.logDebug("insertWord", word);
		this._debug_lastWordsInserted[this.numWords & 15] = word;
		this.numWords++;
		const chars = [...word];
		let d = 1;
		for (const s of chars) {
			if (this.lastPath[d]?.s !== s) break;
			d++;
		}
		if (chars.length < d) d = chars.length;
		this.lastPath.length = d;
		d -= 1;
		const { n } = this.lastPath[d];
		const tail = chars.slice(d);
		this.lastPath[d].n = this._insert(n, tail, d + 1);
		while (d > 0) {
			const { s, n: n$1 } = this.lastPath[d];
			d -= 1;
			const parent = this.lastPath[d];
			const pn = parent.n;
			parent.n = this.addChild(pn, s, n$1);
			if (pn === parent.n) break;
			const tail$1 = chars.slice(d);
			this.storeTransform(pn, tail$1.join(""), parent.n);
		}
	}
	insert(words) {
		for (const w of words) w && this.insertWord(w);
	}
	/**
	* Resets the builder
	*/
	reset() {
		this.lastPath = [{
			s: "",
			n: {
				id: 0,
				f: void 0,
				c: void 0
			}
		}];
		this.cached.clear();
		this.signatures.clear();
		this.signatures.set(this.signature(this._eow), this._eow);
		this.count = 0;
		this.cached.set(this._eow, this._eow.id ?? ++this.count);
	}
	build(consolidateSuffixes = false) {
		const root = this._root;
		this.reset();
		const check = checkCircular(this._root);
		if (check.isCircular) {
			const { word, pos } = check.ref;
			console.error("Circular Reference %o", {
				word,
				pos
			});
			throw new Error("Trie: Circular Reference");
		}
		return new Trie(consolidateSuffixes ? consolidate(root) : root);
	}
	debugStack(stack) {
		return stack.map((n) => this.debNodeInfo(n));
	}
	debNodeInfo(node) {
		return {
			id: node.id ?? "?",
			cid: this.cached.get(node) ?? "?",
			f: node.f || 0,
			c: node.c ? Object.fromEntries(Object.entries(node.c).map(([k, n]) => [k, {
				id: n.id,
				r: this.cached.get(n)
			}])) : void 0,
			L: Object.isFrozen(node)
		};
	}
	logDebug(methodName, contentOrFunction) {
		this.runDebug(() => {
			const content = typeof contentOrFunction === "function" ? contentOrFunction() : contentOrFunction;
			console.warn("%s: %o", methodName, content);
		});
	}
	runDebug(method) {
		if (this._debug_mode) method();
	}
	copyIfFrozen(n) {
		if (!Object.isFrozen(n)) return n;
		const c = n.c ? Object.assign(Object.create(null), n.c) : void 0;
		return this.createNode(n.f, c);
	}
	createNodeFrozen(f, c) {
		return this.freeze(this.createNode(f, c));
	}
	createNode(f, c) {
		return {
			id: ++this.count,
			f,
			c
		};
	}
};
function assertFrozen(n) {
	if (!("id" in n)) console.warn("%o", n);
	if (!Object.isFrozen(n) || !("id" in n)) throw new Error("Must be TrieNodeExFrozen");
}

//#endregion
//#region src/lib/utils/normalizeWord.ts
/**
* Normalize word unicode.
* @param text - text to normalize
* @returns returns a word normalized to `NFC`
*/
const normalizeWord = (text) => text.normalize();
/**
* converts text to lower case and removes any accents.
* @param text - text to convert
* @returns lowercase word without accents
* @deprecated true
*/
const normalizeWordToLowercase = (text) => text.toLowerCase().normalize("NFD").replaceAll(/\p{M}/gu, "");
/**
* generate case insensitive forms of a word
* @param text - text to convert
* @returns the forms of the word.
*/
const normalizeWordForCaseInsensitive = (text) => {
	const t = text.toLowerCase();
	return [t, t.normalize("NFD").replaceAll(/\p{M}/gu, "")];
};

//#endregion
//#region src/lib/SimpleDictionaryParser.ts
const _defaultOptions = {
	commentCharacter: LINE_COMMENT,
	optionalCompoundCharacter: OPTIONAL_COMPOUND_FIX,
	compoundCharacter: COMPOUND_FIX,
	forbiddenPrefix: FORBID_PREFIX,
	caseInsensitivePrefix: CASE_INSENSITIVE_PREFIX,
	keepExactPrefix: IDENTITY_PREFIX,
	stripCaseAndAccents: true,
	stripCaseAndAccentsKeepDuplicate: false,
	stripCaseAndAccentsOnForbidden: false,
	split: false,
	splitKeepBoth: false,
	splitSeparator: /[\s,;]/g,
	keepOptionalCompoundCharacter: false
};
const defaultParseDictionaryOptions = Object.freeze(_defaultOptions);
const cSpellToolDirective = "cspell-dictionary:";
/**
* Normalizes a dictionary words based upon prefix / suffixes.
* Case insensitive versions are also generated.
* @param options - defines prefixes used when parsing lines.
* @returns words that have been normalized.
*/
function createDictionaryLineParserMapper(options) {
	const _options = options || _defaultOptions;
	const { commentCharacter = _defaultOptions.commentCharacter, optionalCompoundCharacter: optionalCompound = _defaultOptions.optionalCompoundCharacter, compoundCharacter: compound = _defaultOptions.compoundCharacter, caseInsensitivePrefix: ignoreCase = _defaultOptions.caseInsensitivePrefix, forbiddenPrefix: forbidden = _defaultOptions.forbiddenPrefix, keepExactPrefix: keepCase = _defaultOptions.keepExactPrefix, splitSeparator = _defaultOptions.splitSeparator, splitKeepBoth = _defaultOptions.splitKeepBoth, stripCaseAndAccentsKeepDuplicate = _defaultOptions.stripCaseAndAccentsKeepDuplicate, stripCaseAndAccentsOnForbidden = _defaultOptions.stripCaseAndAccentsOnForbidden, keepOptionalCompoundCharacter = _defaultOptions.keepOptionalCompoundCharacter } = _options;
	let { stripCaseAndAccents = _defaultOptions.stripCaseAndAccents, split = _defaultOptions.split } = _options;
	function isString(line) {
		return typeof line === "string";
	}
	function trim(line) {
		return line.trim();
	}
	function removeComments(line) {
		const idx$1 = line.indexOf(commentCharacter);
		if (idx$1 < 0) return line;
		const idxDirective = line.indexOf(cSpellToolDirective, idx$1);
		if (idxDirective >= 0) {
			const flags = line.slice(idxDirective).split(/[\s,;]/g).map((s) => s.trim()).filter((a) => !!a);
			for (const flag of flags) switch (flag) {
				case "split":
					split = true;
					break;
				case "no-split":
					split = false;
					break;
				case "no-generate-alternatives":
					stripCaseAndAccents = false;
					break;
				case "generate-alternatives":
					stripCaseAndAccents = true;
					break;
			}
		}
		return line.slice(0, idx$1).trim();
	}
	function filterEmptyLines(line) {
		return !!line;
	}
	function* mapOptionalPrefix(line) {
		if (line[0] === optionalCompound) {
			const t = line.slice(1);
			yield t;
			yield compound + t;
		} else yield line;
	}
	function* mapOptionalSuffix(line) {
		if (line.slice(-1) === optionalCompound) {
			const t = line.slice(0, -1);
			yield t;
			yield t + compound;
		} else yield line;
	}
	const doNotNormalizePrefix = Object.create(null);
	[
		ignoreCase,
		keepCase,
		"\""
	].forEach((prefix) => doNotNormalizePrefix[prefix] = true);
	if (!stripCaseAndAccentsOnForbidden) doNotNormalizePrefix[forbidden] = true;
	function removeDoublePrefix(w) {
		return w.startsWith(ignoreCase + ignoreCase) ? w.slice(1) : w;
	}
	function stripKeepCasePrefixAndQuotes(word) {
		word = word.replaceAll(/"(.*?)"/g, "$1");
		return word[0] === keepCase ? word.slice(1) : word;
	}
	function _normalize(word) {
		return normalizeWord(stripKeepCasePrefixAndQuotes(word));
	}
	function* mapNormalize(word) {
		const nWord = _normalize(word);
		const forms$1 = /* @__PURE__ */ new Set();
		forms$1.add(nWord);
		if (stripCaseAndAccents && !(word[0] in doNotNormalizePrefix)) for (const n of normalizeWordForCaseInsensitive(nWord)) (stripCaseAndAccentsKeepDuplicate || n !== nWord) && forms$1.add(ignoreCase + n);
		yield* forms$1;
	}
	function* splitWords(lines) {
		for (const line of lines) {
			if (split) {
				yield* splitLine(line.includes("\"") ? line.replaceAll(/".*?"/g, (quoted) => " " + quoted.replaceAll(/(\s)/g, "\\$1") + " ") : line, splitSeparator).map((escaped) => escaped.replaceAll("\\", ""));
				if (!splitKeepBoth) continue;
			}
			yield line;
		}
	}
	function* splitLines(paragraphs) {
		for (const paragraph of paragraphs) yield* paragraph.split("\n");
	}
	const mapCompounds = keepOptionalCompoundCharacter ? [] : [opConcatMap(mapOptionalPrefix), opConcatMap(mapOptionalSuffix)];
	return opCombine(opFilter(isString), splitLines, opMap(removeComments), splitWords, opMap(trim), opFilter(filterEmptyLines), ...mapCompounds, opConcatMap(mapNormalize), opMap(removeDoublePrefix));
}
/**
* Normalizes a dictionary words based upon prefix / suffixes.
* Case insensitive versions are also generated.
* @param lines - one word per line
* @param _options - defines prefixes used when parsing lines.
* @returns words that have been normalized.
*/
function parseDictionaryLines(lines, options) {
	return createDictionaryLineParserMapper(options)(typeof lines === "string" ? [lines] : lines);
}
function parseLinesToDictionaryLegacy(lines, options) {
	const _options = mergeOptions(_defaultOptions, options);
	const dictLines = parseDictionaryLines(lines, _options);
	return buildTrieFast([...new Set(dictLines)].sort(), {
		compoundCharacter: _options.compoundCharacter,
		forbiddenWordPrefix: _options.forbiddenPrefix,
		stripCaseAndAccentsPrefix: _options.caseInsensitivePrefix
	});
}
function parseDictionaryLegacy(text, options) {
	return parseLinesToDictionaryLegacy(typeof text === "string" ? text.split("\n") : text, options);
}
function parseLinesToDictionary(lines, options) {
	const _options = mergeOptions(_defaultOptions, options);
	const dictLines = parseDictionaryLines(lines, _options);
	return buildITrieFromWords([...new Set(dictLines)].sort(), {
		compoundCharacter: _options.compoundCharacter,
		forbiddenWordPrefix: _options.forbiddenPrefix,
		stripCaseAndAccentsPrefix: _options.caseInsensitivePrefix
	});
}
function parseDictionary(text, options) {
	return parseLinesToDictionary(typeof text === "string" ? text.split("\n") : text, options);
}
function mergeOptions(base, ...partials) {
	const opt = { ...base };
	for (const p of partials) {
		if (!p) continue;
		Object.assign(opt, p);
	}
	return opt;
}
const RegExpToEncode = /\\([\s,;])/g;
const RegExpDecode = /<<(%[\da-f]{2})>>/gi;
function encodeLine(line) {
	return line.replaceAll(RegExpToEncode, (_, v) => "<<" + encodeURIComponent(v) + ">>");
}
function decodeLine(line) {
	return line.replaceAll(RegExpDecode, (_, v) => "\\" + decodeURIComponent(v));
}
function splitLine(line, regExp) {
	return encodeLine(line).split(regExp).map((line$1) => decodeLine(line$1));
}

//#endregion
export { CASE_INSENSITIVE_PREFIX, COMPOUND_FIX, CompoundWordsMethod, FLAG_WORD, FORBID_PREFIX, JOIN_SEPARATOR, OPTIONAL_COMPOUND_FIX, Trie, TrieBuilder, WORD_SEPARATOR, buildITrieFromWords, buildTrie, buildTrieFast, consolidate, countNodes, countWords, createDictionaryLineParserMapper as createDictionaryLineParser, createTrieRoot, createTrieRootFromList, createWeightedMap, decodeTrie, defaultTrieInfo, defaultTrieInfo as defaultTrieOptions, editDistance, editDistanceWeighted, expandCharacterSet, findNode, has, hintedWalker, impersonateCollector, importTrie, insert, isCircular, isDefined, isWordTerminationNode, iterateTrie, iteratorTrieWords, mapDictionaryInformationToWeightMap, mergeDefaults, mergeOptionalWithDefaults, normalizeWord, normalizeWordForCaseInsensitive, normalizeWordToLowercase, orderTrie, parseDictionary, parseDictionaryLegacy, parseDictionaryLines, serializeTrie, suggestionCollector, trieNodeToRoot, walk, walker };
//# sourceMappingURL=index.js.map